{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10 comp",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9d61xlCwX0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation, Conv2D, BatchNormalization, Add, ZeroPadding2D, Input, Dense, MaxPooling2D\n",
        "\n",
        "class ResBlock:\n",
        "    def __init__(self, out_channels, in_channels=None, strides=1):\n",
        "        self.out_channels = out_channels\n",
        "        self.in_channels = in_channels if in_channels else out_channels\n",
        "        self.strides = strides\n",
        "        \n",
        "        \n",
        "    def __call__(self, x):    \n",
        "        res = BatchNormalization()(x)\n",
        "        res_1 = Activation('relu')(res)\n",
        "        \n",
        "        res = Conv2D(filters=self.out_channels//4, kernel_size=(1,1), strides=1, use_bias=False)(res_1)\n",
        "        res = BatchNormalization()(res)\n",
        "        res = Activation('relu')(res)\n",
        "        # res = ZeroPadding2D(padding=(1,1))(res)\n",
        "        res = Conv2D(filters=self.out_channels//4, kernel_size=(3,3), strides=self.strides, padding='same')(res)\n",
        "        res = BatchNormalization()(res)\n",
        "        res = Activation('relu')(res)\n",
        "        res = Conv2D(filters=self.out_channels, kernel_size=(1,1), strides=1, use_bias=False)(res)\n",
        "        \n",
        "        if self.strides != 1 or self.in_channels != self.out_channels:\n",
        "            identity = Conv2D(filters=self.out_channels, kernel_size=(1,1), strides=self.strides, use_bias=False)(res_1)\n",
        "        else:\n",
        "            identity = x\n",
        "        output = Add()([identity, res])\n",
        "        return output\n",
        "    \n",
        "    \n",
        "class MaxPool2D:\n",
        "    def __init__(self, pool_size=(3,3), strides=2, padding=(1,1)):\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        \n",
        "        \n",
        "    def __call__(self, x):\n",
        "        # padd = ZeroPadding2D(padding=self.padding)(x)\n",
        "        # pool = MaxPooling2D(pool_size=self.pool_size, strides=self.strides, padding='same')(padd)\n",
        "        pool = MaxPooling2D(pool_size=self.pool_size, strides=self.strides, padding='same')(x)\n",
        "        return pool\n",
        "\n",
        "class AvgPool2D:\n",
        "    def __init__(self, pool_size=(3,3), strides=2, padding=(1,1)):\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        \n",
        "        \n",
        "    def __call__(self, x):\n",
        "        padd = ZeroPadding2D(padding=self.padding)(x)\n",
        "        pool = AveragePooling2D(pool_size=self.pool_size, strides=self.strides, padding='valid')(padd)\n",
        "        return pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5N_nCBPAOAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# from basic_module import ResBlock, MaxPool2D\n",
        "from tensorflow.keras.layers import Activation, Conv2D, BatchNormalization, Add, ZeroPadding2D, MaxPooling2D, UpSampling2D, Multiply, AveragePooling2D, Flatten, Dropout\n",
        "    \n",
        "\n",
        "class AttentionModule_stage1:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        \n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "\n",
        "        out_mpool1 = MaxPool2D()(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_mpool1)\n",
        "        out_skip1_connection = ResBlock(channels)(out_softmax1)\n",
        "\n",
        "        out_mpool2 = MaxPool2D()(out_softmax1)\n",
        "        out_softmax2 = ResBlock(channels)(out_mpool2)\n",
        "        out_softmax2 = ResBlock(channels)(out_softmax2)\n",
        "\n",
        "        out_interp2 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax2), out_softmax1])\n",
        "        out = Add()([out_interp2, out_skip1_connection])\n",
        "\n",
        "        out_softmax3 = ResBlock(channels)(out)\n",
        "        out_interp1 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax3), out_trunk])\n",
        "\n",
        "        out_softmax4 = BatchNormalization()(out_interp1)\n",
        "        out_softmax4 = Activation('relu')(out_softmax4)\n",
        "        out_softmax4 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax4)\n",
        "        out_softmax4 = BatchNormalization()(out_softmax4)\n",
        "        out_softmax4 = Activation('relu')(out_softmax4)\n",
        "        out_softmax4 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax4)\n",
        "        out_softmax4 = Activation('sigmoid')(out_softmax4)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax4), out_softmax4])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    \n",
        "    \n",
        "class AttentionModule_stage2:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        \n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "\n",
        "        out_mpool1 = MaxPool2D()(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_mpool1)\n",
        "        out_softmax1 = ResBlock(channels)(out_softmax1)\n",
        "\n",
        "        out_interp1 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax1), out_trunk])\n",
        "        \n",
        "        out_softmax2 = BatchNormalization()(out_interp1)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = BatchNormalization()(out_softmax2)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = Activation('sigmoid')(out_softmax2)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax2), out_softmax2])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    \n",
        "    \n",
        "class AttentionModule_stage3:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "                \n",
        "        out_softmax1 = ResBlock(channels)(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_softmax1)\n",
        "        \n",
        "        out_softmax2 = BatchNormalization()(out_softmax1)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = BatchNormalization()(out_softmax2)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = Activation('sigmoid')(out_softmax2)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax2), out_softmax2])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZLCaXmIAVvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualAttentionModel_32input:\n",
        "\n",
        "    def __init__(self, classes=10):\n",
        "        self.classes = classes\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        classes = self.classes\n",
        "                \n",
        "        # x = ZeroPadding2D(padding=(3,3))(x)\n",
        "        x = Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', use_bias=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        \n",
        "        # x = MaxPool2D()(x)\n",
        "        x = ResBlock(64, in_channels=32)(x)\n",
        "        x = AttentionModule_stage1(64)(x)\n",
        "        x = ResBlock(128, in_channels=64, strides=2)(x)\n",
        "        x = AttentionModule_stage2(128)(x)\n",
        "        x = ResBlock(256, in_channels=128, strides=2)(x)\n",
        "        x = AttentionModule_stage3(256)(x)\n",
        "        x = ResBlock(512, in_channels=256)(x)\n",
        "#        x = ResBlock(1024)(x)\n",
        "        x = ResBlock(512)(x)\n",
        "        \n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = AvgPool2D(pool_size=(8,8), strides=1, padding=(0,0))(x)\n",
        "        \n",
        "        x = Conv2D(classes, kernel_size=(1,1))(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGh1bL11Ai1P",
        "colab_type": "code",
        "outputId": "c1fbdfd7-40ee-4bbd-9a06-5e0fbead708e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "model_in = Input((32, 32, 3))\n",
        "\n",
        "model_out = ResidualAttentionModel_32input()(model_in)\n",
        "model_out = Dropout(0.2)(model_out)\n",
        "model_out = Activation('softmax')(model_out)\n",
        "\n",
        "model = Model(inputs=model_in, outputs=model_out)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_365 (Conv2D)             (None, 32, 32, 32)   864         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_350 (BatchN (None, 32, 32, 32)   128         conv2d_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_366 (Activation)     (None, 32, 32, 32)   0           batch_normalization_350[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_351 (BatchN (None, 32, 32, 32)   128         activation_366[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_367 (Activation)     (None, 32, 32, 32)   0           batch_normalization_351[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_366 (Conv2D)             (None, 32, 32, 16)   512         activation_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_352 (BatchN (None, 32, 32, 16)   64          conv2d_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_368 (Activation)     (None, 32, 32, 16)   0           batch_normalization_352[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_367 (Conv2D)             (None, 32, 32, 16)   2320        activation_368[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_353 (BatchN (None, 32, 32, 16)   64          conv2d_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_369 (Activation)     (None, 32, 32, 16)   0           batch_normalization_353[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_369 (Conv2D)             (None, 32, 32, 64)   2048        activation_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_368 (Conv2D)             (None, 32, 32, 64)   1024        activation_369[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_134 (Add)                   (None, 32, 32, 64)   0           conv2d_369[0][0]                 \n",
            "                                                                 conv2d_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_354 (BatchN (None, 32, 32, 64)   256         add_134[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_370 (Activation)     (None, 32, 32, 64)   0           batch_normalization_354[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_370 (Conv2D)             (None, 32, 32, 16)   1024        activation_370[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_355 (BatchN (None, 32, 32, 16)   64          conv2d_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_371 (Activation)     (None, 32, 32, 16)   0           batch_normalization_355[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_371 (Conv2D)             (None, 32, 32, 16)   2320        activation_371[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_356 (BatchN (None, 32, 32, 16)   64          conv2d_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_372 (Activation)     (None, 32, 32, 16)   0           batch_normalization_356[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_372 (Conv2D)             (None, 32, 32, 64)   1024        activation_372[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_135 (Add)                   (None, 32, 32, 64)   0           add_134[0][0]                    \n",
            "                                                                 conv2d_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 16, 16, 64)   0           add_135[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_363 (BatchN (None, 16, 16, 64)   256         max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_379 (Activation)     (None, 16, 16, 64)   0           batch_normalization_363[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_379 (Conv2D)             (None, 16, 16, 16)   1024        activation_379[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_364 (BatchN (None, 16, 16, 16)   64          conv2d_379[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_380 (Activation)     (None, 16, 16, 16)   0           batch_normalization_364[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_380 (Conv2D)             (None, 16, 16, 16)   2320        activation_380[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_365 (BatchN (None, 16, 16, 16)   64          conv2d_380[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_381 (Activation)     (None, 16, 16, 16)   0           batch_normalization_365[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_381 (Conv2D)             (None, 16, 16, 64)   1024        activation_381[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_138 (Add)                   (None, 16, 16, 64)   0           max_pooling2d_12[0][0]           \n",
            "                                                                 conv2d_381[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 8, 8, 64)     0           add_138[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_369 (BatchN (None, 8, 8, 64)     256         max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_385 (Activation)     (None, 8, 8, 64)     0           batch_normalization_369[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_385 (Conv2D)             (None, 8, 8, 16)     1024        activation_385[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_370 (BatchN (None, 8, 8, 16)     64          conv2d_385[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_386 (Activation)     (None, 8, 8, 16)     0           batch_normalization_370[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_386 (Conv2D)             (None, 8, 8, 16)     2320        activation_386[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_371 (BatchN (None, 8, 8, 16)     64          conv2d_386[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_387 (Activation)     (None, 8, 8, 16)     0           batch_normalization_371[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_387 (Conv2D)             (None, 8, 8, 64)     1024        activation_387[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_140 (Add)                   (None, 8, 8, 64)     0           max_pooling2d_13[0][0]           \n",
            "                                                                 conv2d_387[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_372 (BatchN (None, 8, 8, 64)     256         add_140[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_388 (Activation)     (None, 8, 8, 64)     0           batch_normalization_372[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_388 (Conv2D)             (None, 8, 8, 16)     1024        activation_388[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_366 (BatchN (None, 16, 16, 64)   256         add_138[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_373 (BatchN (None, 8, 8, 16)     64          conv2d_388[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_382 (Activation)     (None, 16, 16, 64)   0           batch_normalization_366[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_389 (Activation)     (None, 8, 8, 16)     0           batch_normalization_373[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_382 (Conv2D)             (None, 16, 16, 16)   1024        activation_382[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_357 (BatchN (None, 32, 32, 64)   256         add_135[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_389 (Conv2D)             (None, 8, 8, 16)     2320        activation_389[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_367 (BatchN (None, 16, 16, 16)   64          conv2d_382[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_373 (Activation)     (None, 32, 32, 64)   0           batch_normalization_357[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_374 (BatchN (None, 8, 8, 16)     64          conv2d_389[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_383 (Activation)     (None, 16, 16, 16)   0           batch_normalization_367[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_373 (Conv2D)             (None, 32, 32, 16)   1024        activation_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_390 (Activation)     (None, 8, 8, 16)     0           batch_normalization_374[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_383 (Conv2D)             (None, 16, 16, 16)   2320        activation_383[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_358 (BatchN (None, 32, 32, 16)   64          conv2d_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_390 (Conv2D)             (None, 8, 8, 64)     1024        activation_390[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_368 (BatchN (None, 16, 16, 16)   64          conv2d_383[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_374 (Activation)     (None, 32, 32, 16)   0           batch_normalization_358[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_141 (Add)                   (None, 8, 8, 64)     0           add_140[0][0]                    \n",
            "                                                                 conv2d_390[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_384 (Activation)     (None, 16, 16, 16)   0           batch_normalization_368[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_374 (Conv2D)             (None, 32, 32, 16)   2320        activation_374[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_12 (UpSampling2D) (None, 16, 16, 64)   0           add_141[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_384 (Conv2D)             (None, 16, 16, 64)   1024        activation_384[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_359 (BatchN (None, 32, 32, 16)   64          conv2d_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_142 (Add)                   (None, 16, 16, 64)   0           up_sampling2d_12[0][0]           \n",
            "                                                                 add_138[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_139 (Add)                   (None, 16, 16, 64)   0           add_138[0][0]                    \n",
            "                                                                 conv2d_384[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_375 (Activation)     (None, 32, 32, 16)   0           batch_normalization_359[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_143 (Add)                   (None, 16, 16, 64)   0           add_142[0][0]                    \n",
            "                                                                 add_139[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_375 (Conv2D)             (None, 32, 32, 64)   1024        activation_375[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_375 (BatchN (None, 16, 16, 64)   256         add_143[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_136 (Add)                   (None, 32, 32, 64)   0           add_135[0][0]                    \n",
            "                                                                 conv2d_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_391 (Activation)     (None, 16, 16, 64)   0           batch_normalization_375[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_360 (BatchN (None, 32, 32, 64)   256         add_136[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_391 (Conv2D)             (None, 16, 16, 16)   1024        activation_391[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_376 (Activation)     (None, 32, 32, 64)   0           batch_normalization_360[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_376 (BatchN (None, 16, 16, 16)   64          conv2d_391[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_376 (Conv2D)             (None, 32, 32, 16)   1024        activation_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_392 (Activation)     (None, 16, 16, 16)   0           batch_normalization_376[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_361 (BatchN (None, 32, 32, 16)   64          conv2d_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_392 (Conv2D)             (None, 16, 16, 16)   2320        activation_392[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_377 (Activation)     (None, 32, 32, 16)   0           batch_normalization_361[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_377 (BatchN (None, 16, 16, 16)   64          conv2d_392[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_377 (Conv2D)             (None, 32, 32, 16)   2320        activation_377[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_393 (Activation)     (None, 16, 16, 16)   0           batch_normalization_377[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_362 (BatchN (None, 32, 32, 16)   64          conv2d_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_393 (Conv2D)             (None, 16, 16, 64)   1024        activation_393[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_378 (Activation)     (None, 32, 32, 16)   0           batch_normalization_362[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_144 (Add)                   (None, 16, 16, 64)   0           add_143[0][0]                    \n",
            "                                                                 conv2d_393[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_378 (Conv2D)             (None, 32, 32, 64)   1024        activation_378[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_13 (UpSampling2D) (None, 32, 32, 64)   0           add_144[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_137 (Add)                   (None, 32, 32, 64)   0           add_136[0][0]                    \n",
            "                                                                 conv2d_378[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_145 (Add)                   (None, 32, 32, 64)   0           up_sampling2d_13[0][0]           \n",
            "                                                                 add_137[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_378 (BatchN (None, 32, 32, 64)   256         add_145[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_394 (Activation)     (None, 32, 32, 64)   0           batch_normalization_378[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_394 (Conv2D)             (None, 32, 32, 64)   4096        activation_394[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_379 (BatchN (None, 32, 32, 64)   256         conv2d_394[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_395 (Activation)     (None, 32, 32, 64)   0           batch_normalization_379[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_395 (Conv2D)             (None, 32, 32, 64)   4096        activation_395[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_396 (Activation)     (None, 32, 32, 64)   0           conv2d_395[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_12 (TensorFlo [(4,)]               0           activation_396[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_12 (TensorFlow [(None, 32, 32, 64)] 0           tf_op_layer_Shape_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_146 (Add)                   (None, 32, 32, 64)   0           tf_op_layer_Fill_12[0][0]        \n",
            "                                                                 activation_396[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_12 (Multiply)          (None, 32, 32, 64)   0           add_146[0][0]                    \n",
            "                                                                 add_137[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_380 (BatchN (None, 32, 32, 64)   256         multiply_12[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_397 (Activation)     (None, 32, 32, 64)   0           batch_normalization_380[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_396 (Conv2D)             (None, 32, 32, 16)   1024        activation_397[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_381 (BatchN (None, 32, 32, 16)   64          conv2d_396[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_398 (Activation)     (None, 32, 32, 16)   0           batch_normalization_381[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_397 (Conv2D)             (None, 32, 32, 16)   2320        activation_398[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_382 (BatchN (None, 32, 32, 16)   64          conv2d_397[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_399 (Activation)     (None, 32, 32, 16)   0           batch_normalization_382[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_398 (Conv2D)             (None, 32, 32, 64)   1024        activation_399[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_147 (Add)                   (None, 32, 32, 64)   0           multiply_12[0][0]                \n",
            "                                                                 conv2d_398[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_383 (BatchN (None, 32, 32, 64)   256         add_147[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_400 (Activation)     (None, 32, 32, 64)   0           batch_normalization_383[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_399 (Conv2D)             (None, 32, 32, 32)   2048        activation_400[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_384 (BatchN (None, 32, 32, 32)   128         conv2d_399[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_401 (Activation)     (None, 32, 32, 32)   0           batch_normalization_384[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_400 (Conv2D)             (None, 16, 16, 32)   9248        activation_401[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_385 (BatchN (None, 16, 16, 32)   128         conv2d_400[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_402 (Activation)     (None, 16, 16, 32)   0           batch_normalization_385[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_402 (Conv2D)             (None, 16, 16, 128)  8192        activation_400[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_401 (Conv2D)             (None, 16, 16, 128)  4096        activation_402[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_148 (Add)                   (None, 16, 16, 128)  0           conv2d_402[0][0]                 \n",
            "                                                                 conv2d_401[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_386 (BatchN (None, 16, 16, 128)  512         add_148[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_403 (Activation)     (None, 16, 16, 128)  0           batch_normalization_386[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_403 (Conv2D)             (None, 16, 16, 32)   4096        activation_403[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_387 (BatchN (None, 16, 16, 32)   128         conv2d_403[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_404 (Activation)     (None, 16, 16, 32)   0           batch_normalization_387[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_404 (Conv2D)             (None, 16, 16, 32)   9248        activation_404[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_388 (BatchN (None, 16, 16, 32)   128         conv2d_404[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_405 (Activation)     (None, 16, 16, 32)   0           batch_normalization_388[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_405 (Conv2D)             (None, 16, 16, 128)  4096        activation_405[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_149 (Add)                   (None, 16, 16, 128)  0           add_148[0][0]                    \n",
            "                                                                 conv2d_405[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 8, 8, 128)    0           add_149[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_395 (BatchN (None, 8, 8, 128)    512         max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_412 (Activation)     (None, 8, 8, 128)    0           batch_normalization_395[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_389 (BatchN (None, 16, 16, 128)  512         add_149[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_412 (Conv2D)             (None, 8, 8, 32)     4096        activation_412[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_406 (Activation)     (None, 16, 16, 128)  0           batch_normalization_389[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_396 (BatchN (None, 8, 8, 32)     128         conv2d_412[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_406 (Conv2D)             (None, 16, 16, 32)   4096        activation_406[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_413 (Activation)     (None, 8, 8, 32)     0           batch_normalization_396[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_390 (BatchN (None, 16, 16, 32)   128         conv2d_406[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_413 (Conv2D)             (None, 8, 8, 32)     9248        activation_413[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_407 (Activation)     (None, 16, 16, 32)   0           batch_normalization_390[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_397 (BatchN (None, 8, 8, 32)     128         conv2d_413[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_407 (Conv2D)             (None, 16, 16, 32)   9248        activation_407[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_414 (Activation)     (None, 8, 8, 32)     0           batch_normalization_397[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_391 (BatchN (None, 16, 16, 32)   128         conv2d_407[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_414 (Conv2D)             (None, 8, 8, 128)    4096        activation_414[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_408 (Activation)     (None, 16, 16, 32)   0           batch_normalization_391[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_152 (Add)                   (None, 8, 8, 128)    0           max_pooling2d_14[0][0]           \n",
            "                                                                 conv2d_414[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_408 (Conv2D)             (None, 16, 16, 128)  4096        activation_408[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_398 (BatchN (None, 8, 8, 128)    512         add_152[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_150 (Add)                   (None, 16, 16, 128)  0           add_149[0][0]                    \n",
            "                                                                 conv2d_408[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_415 (Activation)     (None, 8, 8, 128)    0           batch_normalization_398[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_392 (BatchN (None, 16, 16, 128)  512         add_150[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_415 (Conv2D)             (None, 8, 8, 32)     4096        activation_415[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_409 (Activation)     (None, 16, 16, 128)  0           batch_normalization_392[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_399 (BatchN (None, 8, 8, 32)     128         conv2d_415[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_409 (Conv2D)             (None, 16, 16, 32)   4096        activation_409[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_416 (Activation)     (None, 8, 8, 32)     0           batch_normalization_399[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_393 (BatchN (None, 16, 16, 32)   128         conv2d_409[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_416 (Conv2D)             (None, 8, 8, 32)     9248        activation_416[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_410 (Activation)     (None, 16, 16, 32)   0           batch_normalization_393[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_400 (BatchN (None, 8, 8, 32)     128         conv2d_416[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_410 (Conv2D)             (None, 16, 16, 32)   9248        activation_410[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_417 (Activation)     (None, 8, 8, 32)     0           batch_normalization_400[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_394 (BatchN (None, 16, 16, 32)   128         conv2d_410[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_417 (Conv2D)             (None, 8, 8, 128)    4096        activation_417[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_411 (Activation)     (None, 16, 16, 32)   0           batch_normalization_394[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_153 (Add)                   (None, 8, 8, 128)    0           add_152[0][0]                    \n",
            "                                                                 conv2d_417[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_411 (Conv2D)             (None, 16, 16, 128)  4096        activation_411[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_14 (UpSampling2D) (None, 16, 16, 128)  0           add_153[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_151 (Add)                   (None, 16, 16, 128)  0           add_150[0][0]                    \n",
            "                                                                 conv2d_411[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_154 (Add)                   (None, 16, 16, 128)  0           up_sampling2d_14[0][0]           \n",
            "                                                                 add_151[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_401 (BatchN (None, 16, 16, 128)  512         add_154[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_418 (Activation)     (None, 16, 16, 128)  0           batch_normalization_401[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_418 (Conv2D)             (None, 16, 16, 128)  16384       activation_418[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_402 (BatchN (None, 16, 16, 128)  512         conv2d_418[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_419 (Activation)     (None, 16, 16, 128)  0           batch_normalization_402[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_419 (Conv2D)             (None, 16, 16, 128)  16384       activation_419[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_420 (Activation)     (None, 16, 16, 128)  0           conv2d_419[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_13 (TensorFlo [(4,)]               0           activation_420[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_13 (TensorFlow [(None, 16, 16, 128) 0           tf_op_layer_Shape_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_155 (Add)                   (None, 16, 16, 128)  0           tf_op_layer_Fill_13[0][0]        \n",
            "                                                                 activation_420[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_13 (Multiply)          (None, 16, 16, 128)  0           add_155[0][0]                    \n",
            "                                                                 add_151[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_403 (BatchN (None, 16, 16, 128)  512         multiply_13[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_421 (Activation)     (None, 16, 16, 128)  0           batch_normalization_403[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_420 (Conv2D)             (None, 16, 16, 32)   4096        activation_421[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_404 (BatchN (None, 16, 16, 32)   128         conv2d_420[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_422 (Activation)     (None, 16, 16, 32)   0           batch_normalization_404[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_421 (Conv2D)             (None, 16, 16, 32)   9248        activation_422[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_405 (BatchN (None, 16, 16, 32)   128         conv2d_421[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_423 (Activation)     (None, 16, 16, 32)   0           batch_normalization_405[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_422 (Conv2D)             (None, 16, 16, 128)  4096        activation_423[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_156 (Add)                   (None, 16, 16, 128)  0           multiply_13[0][0]                \n",
            "                                                                 conv2d_422[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_406 (BatchN (None, 16, 16, 128)  512         add_156[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_424 (Activation)     (None, 16, 16, 128)  0           batch_normalization_406[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_423 (Conv2D)             (None, 16, 16, 64)   8192        activation_424[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_407 (BatchN (None, 16, 16, 64)   256         conv2d_423[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_425 (Activation)     (None, 16, 16, 64)   0           batch_normalization_407[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_424 (Conv2D)             (None, 8, 8, 64)     36928       activation_425[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_408 (BatchN (None, 8, 8, 64)     256         conv2d_424[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_426 (Activation)     (None, 8, 8, 64)     0           batch_normalization_408[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_426 (Conv2D)             (None, 8, 8, 256)    32768       activation_424[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_425 (Conv2D)             (None, 8, 8, 256)    16384       activation_426[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_157 (Add)                   (None, 8, 8, 256)    0           conv2d_426[0][0]                 \n",
            "                                                                 conv2d_425[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_409 (BatchN (None, 8, 8, 256)    1024        add_157[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_427 (Activation)     (None, 8, 8, 256)    0           batch_normalization_409[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_427 (Conv2D)             (None, 8, 8, 64)     16384       activation_427[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_410 (BatchN (None, 8, 8, 64)     256         conv2d_427[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_428 (Activation)     (None, 8, 8, 64)     0           batch_normalization_410[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_428 (Conv2D)             (None, 8, 8, 64)     36928       activation_428[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_411 (BatchN (None, 8, 8, 64)     256         conv2d_428[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_429 (Activation)     (None, 8, 8, 64)     0           batch_normalization_411[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_429 (Conv2D)             (None, 8, 8, 256)    16384       activation_429[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_158 (Add)                   (None, 8, 8, 256)    0           add_157[0][0]                    \n",
            "                                                                 conv2d_429[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_418 (BatchN (None, 8, 8, 256)    1024        add_158[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_436 (Activation)     (None, 8, 8, 256)    0           batch_normalization_418[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_436 (Conv2D)             (None, 8, 8, 64)     16384       activation_436[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_419 (BatchN (None, 8, 8, 64)     256         conv2d_436[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_437 (Activation)     (None, 8, 8, 64)     0           batch_normalization_419[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_437 (Conv2D)             (None, 8, 8, 64)     36928       activation_437[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_420 (BatchN (None, 8, 8, 64)     256         conv2d_437[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_438 (Activation)     (None, 8, 8, 64)     0           batch_normalization_420[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_438 (Conv2D)             (None, 8, 8, 256)    16384       activation_438[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_161 (Add)                   (None, 8, 8, 256)    0           add_158[0][0]                    \n",
            "                                                                 conv2d_438[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_421 (BatchN (None, 8, 8, 256)    1024        add_161[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_412 (BatchN (None, 8, 8, 256)    1024        add_158[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_439 (Activation)     (None, 8, 8, 256)    0           batch_normalization_421[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_430 (Activation)     (None, 8, 8, 256)    0           batch_normalization_412[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_439 (Conv2D)             (None, 8, 8, 64)     16384       activation_439[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_430 (Conv2D)             (None, 8, 8, 64)     16384       activation_430[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_422 (BatchN (None, 8, 8, 64)     256         conv2d_439[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_413 (BatchN (None, 8, 8, 64)     256         conv2d_430[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_440 (Activation)     (None, 8, 8, 64)     0           batch_normalization_422[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_431 (Activation)     (None, 8, 8, 64)     0           batch_normalization_413[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_440 (Conv2D)             (None, 8, 8, 64)     36928       activation_440[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_431 (Conv2D)             (None, 8, 8, 64)     36928       activation_431[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_423 (BatchN (None, 8, 8, 64)     256         conv2d_440[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_414 (BatchN (None, 8, 8, 64)     256         conv2d_431[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_441 (Activation)     (None, 8, 8, 64)     0           batch_normalization_423[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_432 (Activation)     (None, 8, 8, 64)     0           batch_normalization_414[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_441 (Conv2D)             (None, 8, 8, 256)    16384       activation_441[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_432 (Conv2D)             (None, 8, 8, 256)    16384       activation_432[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_162 (Add)                   (None, 8, 8, 256)    0           add_161[0][0]                    \n",
            "                                                                 conv2d_441[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_159 (Add)                   (None, 8, 8, 256)    0           add_158[0][0]                    \n",
            "                                                                 conv2d_432[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_424 (BatchN (None, 8, 8, 256)    1024        add_162[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_415 (BatchN (None, 8, 8, 256)    1024        add_159[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_442 (Activation)     (None, 8, 8, 256)    0           batch_normalization_424[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_433 (Activation)     (None, 8, 8, 256)    0           batch_normalization_415[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_442 (Conv2D)             (None, 8, 8, 256)    65536       activation_442[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_433 (Conv2D)             (None, 8, 8, 64)     16384       activation_433[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_425 (BatchN (None, 8, 8, 256)    1024        conv2d_442[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_416 (BatchN (None, 8, 8, 64)     256         conv2d_433[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_443 (Activation)     (None, 8, 8, 256)    0           batch_normalization_425[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_434 (Activation)     (None, 8, 8, 64)     0           batch_normalization_416[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_443 (Conv2D)             (None, 8, 8, 256)    65536       activation_443[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_434 (Conv2D)             (None, 8, 8, 64)     36928       activation_434[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_444 (Activation)     (None, 8, 8, 256)    0           conv2d_443[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_417 (BatchN (None, 8, 8, 64)     256         conv2d_434[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_14 (TensorFlo [(4,)]               0           activation_444[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_435 (Activation)     (None, 8, 8, 64)     0           batch_normalization_417[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_14 (TensorFlow [(None, 8, 8, 256)]  0           tf_op_layer_Shape_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_435 (Conv2D)             (None, 8, 8, 256)    16384       activation_435[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_163 (Add)                   (None, 8, 8, 256)    0           tf_op_layer_Fill_14[0][0]        \n",
            "                                                                 activation_444[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_160 (Add)                   (None, 8, 8, 256)    0           add_159[0][0]                    \n",
            "                                                                 conv2d_435[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "multiply_14 (Multiply)          (None, 8, 8, 256)    0           add_163[0][0]                    \n",
            "                                                                 add_160[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_426 (BatchN (None, 8, 8, 256)    1024        multiply_14[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_445 (Activation)     (None, 8, 8, 256)    0           batch_normalization_426[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_444 (Conv2D)             (None, 8, 8, 64)     16384       activation_445[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_427 (BatchN (None, 8, 8, 64)     256         conv2d_444[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_446 (Activation)     (None, 8, 8, 64)     0           batch_normalization_427[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_445 (Conv2D)             (None, 8, 8, 64)     36928       activation_446[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_428 (BatchN (None, 8, 8, 64)     256         conv2d_445[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_447 (Activation)     (None, 8, 8, 64)     0           batch_normalization_428[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_446 (Conv2D)             (None, 8, 8, 256)    16384       activation_447[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_164 (Add)                   (None, 8, 8, 256)    0           multiply_14[0][0]                \n",
            "                                                                 conv2d_446[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_429 (BatchN (None, 8, 8, 256)    1024        add_164[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_448 (Activation)     (None, 8, 8, 256)    0           batch_normalization_429[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_447 (Conv2D)             (None, 8, 8, 128)    32768       activation_448[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_430 (BatchN (None, 8, 8, 128)    512         conv2d_447[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_449 (Activation)     (None, 8, 8, 128)    0           batch_normalization_430[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_448 (Conv2D)             (None, 8, 8, 128)    147584      activation_449[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_431 (BatchN (None, 8, 8, 128)    512         conv2d_448[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_450 (Activation)     (None, 8, 8, 128)    0           batch_normalization_431[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_450 (Conv2D)             (None, 8, 8, 512)    131072      activation_448[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_449 (Conv2D)             (None, 8, 8, 512)    65536       activation_450[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_165 (Add)                   (None, 8, 8, 512)    0           conv2d_450[0][0]                 \n",
            "                                                                 conv2d_449[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_432 (BatchN (None, 8, 8, 512)    2048        add_165[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_451 (Activation)     (None, 8, 8, 512)    0           batch_normalization_432[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_451 (Conv2D)             (None, 8, 8, 128)    65536       activation_451[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_433 (BatchN (None, 8, 8, 128)    512         conv2d_451[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_452 (Activation)     (None, 8, 8, 128)    0           batch_normalization_433[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_452 (Conv2D)             (None, 8, 8, 128)    147584      activation_452[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_434 (BatchN (None, 8, 8, 128)    512         conv2d_452[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_453 (Activation)     (None, 8, 8, 128)    0           batch_normalization_434[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_453 (Conv2D)             (None, 8, 8, 512)    65536       activation_453[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_166 (Add)                   (None, 8, 8, 512)    0           add_165[0][0]                    \n",
            "                                                                 conv2d_453[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_435 (BatchN (None, 8, 8, 512)    2048        add_166[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_454 (Activation)     (None, 8, 8, 512)    0           batch_normalization_435[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, 8, 8, 512)    0           activation_454[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 1, 1, 512)    0           zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_454 (Conv2D)             (None, 1, 1, 10)     5130        average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 10)           0           conv2d_454[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 10)           0           flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_455 (Activation)     (None, 10)           0           dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,549,482\n",
            "Trainable params: 1,534,506\n",
            "Non-trainable params: 14,976\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NPhLU_zApIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "num_classes = 10 \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def get_cutout(rand_s=0.02, fac_s=20, rand_r=0.3):\n",
        "    def cutout(img):\n",
        "        if np.random.rand() > 0.5:\n",
        "            return img\n",
        "        while True:\n",
        "            s = np.random.uniform(rand_s, fac_s*rand_s) * 32 * 32\n",
        "            r = np.random.uniform(rand_r, 1/rand_r)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, 32)\n",
        "            top = np.random.randint(0, 32)\n",
        "            if left + w <= 32 and top + h <= 32:\n",
        "                break\n",
        "        img[top:top + h, left:left + w, :] = np.random.uniform(0, 1)\n",
        "        return img\n",
        "    return cutout\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    preprocessing_function=get_cutout()\n",
        "    )\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwrTwZA8Ar0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "def cosine_decay_warmup(global_step, lr_base, num_steps, warmup_steps=0):\n",
        "\n",
        "    lr = 0.5 * lr_base * (1 + np.cos( np.pi *\n",
        "        (global_step - warmup_steps) / float(num_steps - warmup_steps)))\n",
        "    if warmup_steps > 0:\n",
        "        slope = lr_base / warmup_steps\n",
        "        lr = np.where(global_step < warmup_steps, slope * global_step, lr)\n",
        "    return np.where(global_step > num_steps, 0.0, lr)\n",
        "\n",
        "\n",
        "class Scheduler(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, lr_base, num_epochs,\n",
        "                 warmup_epochs=0, num_sample = 50000, batch_size=64):\n",
        "\n",
        "        super(Scheduler, self).__init__()\n",
        "        self.lr_base = lr_base\n",
        "        self.num_steps = int(num_epochs * num_sample / batch_size)\n",
        "        self.global_step = 0\n",
        "        self.warmup_steps = int(warmup_epochs * num_sample / batch_size)\n",
        "\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.global_step = self.global_step + 1\n",
        "\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        lr = cosine_decay_warmup(global_step=self.global_step,\n",
        "                                 lr_base=self.lr_base,\n",
        "                                 num_steps=self.num_steps,\n",
        "                                 warmup_steps=self.warmup_steps,\n",
        "                                 )\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "num_sample = 50000\n",
        "epochs = 200\n",
        "batch_size = 128\n",
        "warmup_epochs = 10\n",
        "lr_base = 0.001\n",
        "\n",
        "\n",
        "warm_up_lr = Scheduler(lr_base=lr_base,\n",
        "                       num_epochs=epochs,\n",
        "                       warmup_epochs=warmup_epochs, \n",
        "                       num_sample = num_sample, \n",
        "                       batch_size=batch_size,\n",
        "                       )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JcfYyXaA91L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = '/content'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTASBF4kA-2U",
        "colab_type": "code",
        "outputId": "6be4c934-88b3-4a17-c450-eeb92a50a4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                              epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                              validation_data=(x_test, y_test), callbacks=[warm_up_lr, cp_callback])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1434 - accuracy: 0.2117\n",
            "Epoch 00001: saving model to /content\n",
            "390/390 [==============================] - 44s 113ms/step - loss: 2.1434 - accuracy: 0.2117 - val_loss: 2.7420 - val_accuracy: 0.1305\n",
            "Epoch 2/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.7772 - accuracy: 0.3573\n",
            "Epoch 00002: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 1.7772 - accuracy: 0.3573 - val_loss: 1.5988 - val_accuracy: 0.4106\n",
            "Epoch 3/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.6302 - accuracy: 0.4095\n",
            "Epoch 00003: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 1.6302 - accuracy: 0.4095 - val_loss: 1.5695 - val_accuracy: 0.4499\n",
            "Epoch 4/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.5280 - accuracy: 0.4496\n",
            "Epoch 00004: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 1.5280 - accuracy: 0.4496 - val_loss: 1.5696 - val_accuracy: 0.4443\n",
            "Epoch 5/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.4418 - accuracy: 0.4813\n",
            "Epoch 00005: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 1.4418 - accuracy: 0.4813 - val_loss: 1.2378 - val_accuracy: 0.5592\n",
            "Epoch 6/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.3417 - accuracy: 0.5186\n",
            "Epoch 00006: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 1.3417 - accuracy: 0.5186 - val_loss: 1.8356 - val_accuracy: 0.4448\n",
            "Epoch 7/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.2539 - accuracy: 0.5480\n",
            "Epoch 00007: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 1.2539 - accuracy: 0.5480 - val_loss: 1.4026 - val_accuracy: 0.5287\n",
            "Epoch 8/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.1735 - accuracy: 0.5761\n",
            "Epoch 00008: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 1.1735 - accuracy: 0.5761 - val_loss: 1.1414 - val_accuracy: 0.6059\n",
            "Epoch 9/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0869 - accuracy: 0.6048\n",
            "Epoch 00009: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 1.0869 - accuracy: 0.6048 - val_loss: 1.3141 - val_accuracy: 0.6025\n",
            "Epoch 10/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0288 - accuracy: 0.6262\n",
            "Epoch 00010: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 1.0288 - accuracy: 0.6262 - val_loss: 1.0816 - val_accuracy: 0.6568\n",
            "Epoch 11/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9715 - accuracy: 0.6458\n",
            "Epoch 00011: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.9715 - accuracy: 0.6458 - val_loss: 1.2789 - val_accuracy: 0.6234\n",
            "Epoch 12/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9164 - accuracy: 0.6664\n",
            "Epoch 00012: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.9164 - accuracy: 0.6664 - val_loss: 0.7363 - val_accuracy: 0.7542\n",
            "Epoch 13/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8783 - accuracy: 0.6786\n",
            "Epoch 00013: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.8783 - accuracy: 0.6786 - val_loss: 0.9503 - val_accuracy: 0.6883\n",
            "Epoch 14/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8413 - accuracy: 0.6889\n",
            "Epoch 00014: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.8413 - accuracy: 0.6889 - val_loss: 1.0310 - val_accuracy: 0.6849\n",
            "Epoch 15/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8021 - accuracy: 0.7009\n",
            "Epoch 00015: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.8021 - accuracy: 0.7009 - val_loss: 0.7291 - val_accuracy: 0.7605\n",
            "Epoch 16/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7827 - accuracy: 0.7078\n",
            "Epoch 00016: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.7827 - accuracy: 0.7078 - val_loss: 0.7287 - val_accuracy: 0.7579\n",
            "Epoch 17/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7510 - accuracy: 0.7185\n",
            "Epoch 00017: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.7510 - accuracy: 0.7185 - val_loss: 1.0706 - val_accuracy: 0.6571\n",
            "Epoch 18/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7350 - accuracy: 0.7225\n",
            "Epoch 00018: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.7350 - accuracy: 0.7225 - val_loss: 0.5566 - val_accuracy: 0.8115\n",
            "Epoch 19/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7162 - accuracy: 0.7256\n",
            "Epoch 00019: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.7162 - accuracy: 0.7256 - val_loss: 0.7388 - val_accuracy: 0.7798\n",
            "Epoch 20/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.7349\n",
            "Epoch 00020: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.6946 - accuracy: 0.7349 - val_loss: 0.4821 - val_accuracy: 0.8350\n",
            "Epoch 21/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.7410\n",
            "Epoch 00021: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.6785 - accuracy: 0.7410 - val_loss: 0.5538 - val_accuracy: 0.8213\n",
            "Epoch 22/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.7439\n",
            "Epoch 00022: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.6627 - accuracy: 0.7439 - val_loss: 0.6840 - val_accuracy: 0.7904\n",
            "Epoch 23/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6442 - accuracy: 0.7509\n",
            "Epoch 00023: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.6442 - accuracy: 0.7509 - val_loss: 0.7962 - val_accuracy: 0.7623\n",
            "Epoch 24/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6277 - accuracy: 0.7530\n",
            "Epoch 00024: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.6277 - accuracy: 0.7530 - val_loss: 0.5450 - val_accuracy: 0.8222\n",
            "Epoch 25/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6197 - accuracy: 0.7593\n",
            "Epoch 00025: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.6197 - accuracy: 0.7593 - val_loss: 0.5264 - val_accuracy: 0.8291\n",
            "Epoch 26/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.7641\n",
            "Epoch 00026: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.6006 - accuracy: 0.7641 - val_loss: 0.4569 - val_accuracy: 0.8487\n",
            "Epoch 27/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5870 - accuracy: 0.7696\n",
            "Epoch 00027: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.5870 - accuracy: 0.7696 - val_loss: 0.4451 - val_accuracy: 0.8467\n",
            "Epoch 28/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5848 - accuracy: 0.7708\n",
            "Epoch 00028: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.5848 - accuracy: 0.7708 - val_loss: 0.6038 - val_accuracy: 0.8062\n",
            "Epoch 29/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.7748\n",
            "Epoch 00029: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.5628 - accuracy: 0.7748 - val_loss: 0.4937 - val_accuracy: 0.8448\n",
            "Epoch 30/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.7767\n",
            "Epoch 00030: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.5569 - accuracy: 0.7767 - val_loss: 0.5063 - val_accuracy: 0.8477\n",
            "Epoch 31/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.7824\n",
            "Epoch 00031: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.5439 - accuracy: 0.7824 - val_loss: 0.6638 - val_accuracy: 0.8099\n",
            "Epoch 32/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5387 - accuracy: 0.7847\n",
            "Epoch 00032: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.5387 - accuracy: 0.7847 - val_loss: 0.4913 - val_accuracy: 0.8494\n",
            "Epoch 33/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5236 - accuracy: 0.7876\n",
            "Epoch 00033: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.5236 - accuracy: 0.7876 - val_loss: 0.4876 - val_accuracy: 0.8498\n",
            "Epoch 34/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.7864\n",
            "Epoch 00034: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.5220 - accuracy: 0.7864 - val_loss: 0.4825 - val_accuracy: 0.8524\n",
            "Epoch 35/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.7925\n",
            "Epoch 00035: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.5103 - accuracy: 0.7925 - val_loss: 0.4100 - val_accuracy: 0.8731\n",
            "Epoch 36/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.7943\n",
            "Epoch 00036: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.5023 - accuracy: 0.7943 - val_loss: 0.3950 - val_accuracy: 0.8701\n",
            "Epoch 37/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4873 - accuracy: 0.8012\n",
            "Epoch 00037: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.4873 - accuracy: 0.8012 - val_loss: 0.4367 - val_accuracy: 0.8661\n",
            "Epoch 38/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4817 - accuracy: 0.8005\n",
            "Epoch 00038: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4817 - accuracy: 0.8005 - val_loss: 0.5928 - val_accuracy: 0.8375\n",
            "Epoch 39/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4728 - accuracy: 0.8042\n",
            "Epoch 00039: saving model to /content\n",
            "390/390 [==============================] - 44s 112ms/step - loss: 0.4728 - accuracy: 0.8042 - val_loss: 0.4608 - val_accuracy: 0.8609\n",
            "Epoch 40/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4661 - accuracy: 0.8043\n",
            "Epoch 00040: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4661 - accuracy: 0.8043 - val_loss: 0.3966 - val_accuracy: 0.8829\n",
            "Epoch 41/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.8060\n",
            "Epoch 00041: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4647 - accuracy: 0.8060 - val_loss: 0.4537 - val_accuracy: 0.8669\n",
            "Epoch 42/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.8137\n",
            "Epoch 00042: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4474 - accuracy: 0.8137 - val_loss: 0.3817 - val_accuracy: 0.8820\n",
            "Epoch 43/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.8114\n",
            "Epoch 00043: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4475 - accuracy: 0.8114 - val_loss: 0.4308 - val_accuracy: 0.8699\n",
            "Epoch 44/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4399 - accuracy: 0.8136\n",
            "Epoch 00044: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4399 - accuracy: 0.8136 - val_loss: 0.4440 - val_accuracy: 0.8696\n",
            "Epoch 45/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.8174\n",
            "Epoch 00045: saving model to /content\n",
            "390/390 [==============================] - 44s 112ms/step - loss: 0.4299 - accuracy: 0.8174 - val_loss: 0.3515 - val_accuracy: 0.8932\n",
            "Epoch 46/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.8202\n",
            "Epoch 00046: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4247 - accuracy: 0.8202 - val_loss: 0.3510 - val_accuracy: 0.8940\n",
            "Epoch 47/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4205 - accuracy: 0.8190\n",
            "Epoch 00047: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4205 - accuracy: 0.8190 - val_loss: 0.4712 - val_accuracy: 0.8739\n",
            "Epoch 48/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.8202\n",
            "Epoch 00048: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4191 - accuracy: 0.8202 - val_loss: 0.3692 - val_accuracy: 0.8938\n",
            "Epoch 49/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8250\n",
            "Epoch 00049: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.4028 - accuracy: 0.8250 - val_loss: 0.3997 - val_accuracy: 0.8889\n",
            "Epoch 50/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.8267\n",
            "Epoch 00050: saving model to /content\n",
            "390/390 [==============================] - 44s 112ms/step - loss: 0.3972 - accuracy: 0.8267 - val_loss: 0.4125 - val_accuracy: 0.8868\n",
            "Epoch 51/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3976 - accuracy: 0.8264\n",
            "Epoch 00051: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3976 - accuracy: 0.8264 - val_loss: 0.5917 - val_accuracy: 0.8607\n",
            "Epoch 52/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8263\n",
            "Epoch 00052: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.3931 - accuracy: 0.8263 - val_loss: 0.3688 - val_accuracy: 0.8935\n",
            "Epoch 53/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8294\n",
            "Epoch 00053: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3894 - accuracy: 0.8294 - val_loss: 0.4697 - val_accuracy: 0.8714\n",
            "Epoch 54/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.8316\n",
            "Epoch 00054: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.3812 - accuracy: 0.8316 - val_loss: 0.4271 - val_accuracy: 0.8829\n",
            "Epoch 55/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.8322\n",
            "Epoch 00055: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3757 - accuracy: 0.8322 - val_loss: 0.4238 - val_accuracy: 0.8799\n",
            "Epoch 56/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8340\n",
            "Epoch 00056: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.3715 - accuracy: 0.8340 - val_loss: 0.4291 - val_accuracy: 0.8904\n",
            "Epoch 57/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3617 - accuracy: 0.8370\n",
            "Epoch 00057: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.3617 - accuracy: 0.8370 - val_loss: 0.3661 - val_accuracy: 0.9025\n",
            "Epoch 58/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.8372\n",
            "Epoch 00058: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.3598 - accuracy: 0.8372 - val_loss: 0.3678 - val_accuracy: 0.9028\n",
            "Epoch 59/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.8407\n",
            "Epoch 00059: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.3526 - accuracy: 0.8407 - val_loss: 0.3574 - val_accuracy: 0.9052\n",
            "Epoch 60/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.8406\n",
            "Epoch 00060: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3531 - accuracy: 0.8406 - val_loss: 0.4441 - val_accuracy: 0.8843\n",
            "Epoch 61/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.8449\n",
            "Epoch 00061: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3395 - accuracy: 0.8449 - val_loss: 0.3919 - val_accuracy: 0.9000\n",
            "Epoch 62/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.8433\n",
            "Epoch 00062: saving model to /content\n",
            "390/390 [==============================] - 43s 109ms/step - loss: 0.3446 - accuracy: 0.8433 - val_loss: 0.3641 - val_accuracy: 0.9019\n",
            "Epoch 63/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8434\n",
            "Epoch 00063: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3400 - accuracy: 0.8434 - val_loss: 0.3463 - val_accuracy: 0.9046\n",
            "Epoch 64/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.8467\n",
            "Epoch 00064: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3324 - accuracy: 0.8467 - val_loss: 0.3382 - val_accuracy: 0.9116\n",
            "Epoch 65/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.8515\n",
            "Epoch 00065: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.3242 - accuracy: 0.8515 - val_loss: 0.3333 - val_accuracy: 0.9154\n",
            "Epoch 66/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3244 - accuracy: 0.8494\n",
            "Epoch 00066: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3244 - accuracy: 0.8494 - val_loss: 0.3929 - val_accuracy: 0.9076\n",
            "Epoch 67/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.8501\n",
            "Epoch 00067: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3230 - accuracy: 0.8501 - val_loss: 0.3456 - val_accuracy: 0.9132\n",
            "Epoch 68/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.8492\n",
            "Epoch 00068: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.3202 - accuracy: 0.8492 - val_loss: 0.3498 - val_accuracy: 0.9088\n",
            "Epoch 69/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3140 - accuracy: 0.8521\n",
            "Epoch 00069: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3140 - accuracy: 0.8521 - val_loss: 0.3417 - val_accuracy: 0.9175\n",
            "Epoch 70/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.8553\n",
            "Epoch 00070: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3064 - accuracy: 0.8553 - val_loss: 0.3535 - val_accuracy: 0.9105\n",
            "Epoch 71/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8553\n",
            "Epoch 00071: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3046 - accuracy: 0.8553 - val_loss: 0.3371 - val_accuracy: 0.9177\n",
            "Epoch 72/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8542\n",
            "Epoch 00072: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3024 - accuracy: 0.8542 - val_loss: 0.3475 - val_accuracy: 0.9196\n",
            "Epoch 73/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8549\n",
            "Epoch 00073: saving model to /content\n",
            "390/390 [==============================] - 43s 109ms/step - loss: 0.3037 - accuracy: 0.8549 - val_loss: 0.3452 - val_accuracy: 0.9166\n",
            "Epoch 74/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.8573\n",
            "Epoch 00074: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.3023 - accuracy: 0.8573 - val_loss: 0.3507 - val_accuracy: 0.9160\n",
            "Epoch 75/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.8581\n",
            "Epoch 00075: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2948 - accuracy: 0.8581 - val_loss: 0.3475 - val_accuracy: 0.9189\n",
            "Epoch 76/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.8557\n",
            "Epoch 00076: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2974 - accuracy: 0.8557 - val_loss: 0.3584 - val_accuracy: 0.9156\n",
            "Epoch 77/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.8585\n",
            "Epoch 00077: saving model to /content\n",
            "390/390 [==============================] - 43s 111ms/step - loss: 0.2941 - accuracy: 0.8585 - val_loss: 0.3197 - val_accuracy: 0.9223\n",
            "Epoch 78/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8590\n",
            "Epoch 00078: saving model to /content\n",
            "390/390 [==============================] - 43s 109ms/step - loss: 0.2890 - accuracy: 0.8590 - val_loss: 0.3254 - val_accuracy: 0.9217\n",
            "Epoch 79/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.8607\n",
            "Epoch 00079: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2867 - accuracy: 0.8607 - val_loss: 0.3275 - val_accuracy: 0.9227\n",
            "Epoch 80/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.8612\n",
            "Epoch 00080: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2849 - accuracy: 0.8612 - val_loss: 0.3351 - val_accuracy: 0.9201\n",
            "Epoch 81/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.8626\n",
            "Epoch 00081: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2800 - accuracy: 0.8626 - val_loss: 0.3177 - val_accuracy: 0.9241\n",
            "Epoch 82/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.8608\n",
            "Epoch 00082: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2849 - accuracy: 0.8608 - val_loss: 0.3336 - val_accuracy: 0.9181\n",
            "Epoch 83/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.8632\n",
            "Epoch 00083: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2817 - accuracy: 0.8632 - val_loss: 0.3297 - val_accuracy: 0.9229\n",
            "Epoch 84/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.8633\n",
            "Epoch 00084: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2772 - accuracy: 0.8633 - val_loss: 0.3212 - val_accuracy: 0.9229\n",
            "Epoch 85/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.8648\n",
            "Epoch 00085: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2750 - accuracy: 0.8648 - val_loss: 0.3187 - val_accuracy: 0.9247\n",
            "Epoch 86/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.8633\n",
            "Epoch 00086: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2763 - accuracy: 0.8633 - val_loss: 0.3260 - val_accuracy: 0.9237\n",
            "Epoch 87/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.8656\n",
            "Epoch 00087: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2696 - accuracy: 0.8656 - val_loss: 0.3156 - val_accuracy: 0.9251\n",
            "Epoch 88/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8632\n",
            "Epoch 00088: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2700 - accuracy: 0.8632 - val_loss: 0.3167 - val_accuracy: 0.9243\n",
            "Epoch 89/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.8638\n",
            "Epoch 00089: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2710 - accuracy: 0.8638 - val_loss: 0.3226 - val_accuracy: 0.9240\n",
            "Epoch 90/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.8626\n",
            "Epoch 00090: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2757 - accuracy: 0.8626 - val_loss: 0.3244 - val_accuracy: 0.9263\n",
            "Epoch 91/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.8636\n",
            "Epoch 00091: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2734 - accuracy: 0.8636 - val_loss: 0.3212 - val_accuracy: 0.9257\n",
            "Epoch 92/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.8649\n",
            "Epoch 00092: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2677 - accuracy: 0.8649 - val_loss: 0.3238 - val_accuracy: 0.9250\n",
            "Epoch 93/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.8662\n",
            "Epoch 00093: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2647 - accuracy: 0.8662 - val_loss: 0.3268 - val_accuracy: 0.9242\n",
            "Epoch 94/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.8667\n",
            "Epoch 00094: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2676 - accuracy: 0.8667 - val_loss: 0.3220 - val_accuracy: 0.9258\n",
            "Epoch 95/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.8654\n",
            "Epoch 00095: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2676 - accuracy: 0.8654 - val_loss: 0.3197 - val_accuracy: 0.9266\n",
            "Epoch 96/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8658\n",
            "Epoch 00096: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2700 - accuracy: 0.8658 - val_loss: 0.3205 - val_accuracy: 0.9257\n",
            "Epoch 97/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.8659\n",
            "Epoch 00097: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2665 - accuracy: 0.8659 - val_loss: 0.3203 - val_accuracy: 0.9257\n",
            "Epoch 98/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2718 - accuracy: 0.8648\n",
            "Epoch 00098: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2718 - accuracy: 0.8648 - val_loss: 0.3208 - val_accuracy: 0.9254\n",
            "Epoch 99/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.8638\n",
            "Epoch 00099: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2722 - accuracy: 0.8638 - val_loss: 0.3206 - val_accuracy: 0.9255\n",
            "Epoch 100/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.8640\n",
            "Epoch 00100: saving model to /content\n",
            "390/390 [==============================] - 43s 110ms/step - loss: 0.2662 - accuracy: 0.8640 - val_loss: 0.3210 - val_accuracy: 0.9260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgvZHQahBOu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('residual_attention_model_100epoch_reduced1.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JbSRKBFkOPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "f = h5py.File('his_reduced_100_1.hdf5', 'w')\n",
        "f['acc'] = history.history['accuracy']\n",
        "f['val_acc'] = history.history['val_accuracy']\n",
        "f['loss'] = history.history['loss']\n",
        "f['val_loss'] = history.history['val_loss']\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejj26zXhBQNf",
        "colab_type": "code",
        "outputId": "7819842f-eaa3-4add-9642-6046076231e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = tf.keras.models.load_model('residual_attention_model_100epoch.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vnAA6fwBRZZ",
        "colab_type": "code",
        "outputId": "cf9bde8a-b78e-4b58-e1f4-9243a6e60fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model.evaluate(x=x_train, y=y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0149 - accuracy: 0.9947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.014934700913727283, 0.994700014591217]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWTVeg15BSsW",
        "colab_type": "code",
        "outputId": "f1688965-e004-40dd-98e0-5235ce058521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3210 - accuracy: 0.9260\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32104867696762085, 0.9259999990463257]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t30no7elva_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualAttentionModel_32input_red:\n",
        "\n",
        "    def __init__(self, classes=10):\n",
        "        self.classes = classes\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        classes = self.classes\n",
        "                \n",
        "        # x = ZeroPadding2D(padding=(3,3))(x)\n",
        "        x = Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', use_bias=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        \n",
        "        # x = MaxPool2D()(x)\n",
        "        x = ResBlock(32, in_channels=32)(x)\n",
        "        x = AttentionModule_stage1(32)(x)\n",
        "        x = ResBlock(64, in_channels=32, strides=2)(x)\n",
        "        x = AttentionModule_stage2(64)(x)\n",
        "        x = ResBlock(128, in_channels=64, strides=2)(x)\n",
        "        x = AttentionModule_stage3(128)(x)\n",
        "        x = ResBlock(256, in_channels=128)(x)\n",
        "#        x = ResBlock(1024)(x)\n",
        "        x = ResBlock(256)(x)\n",
        "        \n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = AvgPool2D(pool_size=(8,8), strides=1, padding=(0,0))(x)\n",
        "        \n",
        "        x = Conv2D(classes, kernel_size=(1,1))(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxsWmQ-rlzwQ",
        "colab_type": "code",
        "outputId": "11b8527f-7a0a-462d-883a-561f3a6242c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "model_in = Input((32, 32, 3))\n",
        "\n",
        "model_out = ResidualAttentionModel_32input_red()(model_in)\n",
        "model_out = Dropout(0.2)(model_out)\n",
        "model_out = Activation('softmax')(model_out)\n",
        "\n",
        "red_model = Model(inputs=model_in, outputs=model_out)\n",
        "\n",
        "red_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "red_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 32, 32, 32)   864         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 32, 32, 32)   128         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 32, 32, 32)   0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 32, 32, 32)   128         activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 32, 32, 32)   0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 32, 32, 8)    256         activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 32, 32, 8)    32          conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 32, 32, 8)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 32, 32, 8)    584         activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 32, 32, 8)    32          conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 32, 32, 8)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 32, 32, 32)   256         activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 32, 32, 32)   0           activation_52[0][0]              \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 32, 32, 32)   128         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 32, 32, 32)   0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 32, 32, 8)    256         activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 32, 32, 8)    32          conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 32, 32, 8)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 32, 32, 8)    584         activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 32, 32, 8)    32          conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 32, 32, 8)    0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 32, 32, 32)   256         activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 32, 32, 32)   0           add_18[0][0]                     \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 32)   0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 16, 16, 32)   128         max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 16, 16, 32)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 16, 16, 8)    256         activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 16, 16, 8)    32          conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 16, 16, 8)    0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 16, 16, 8)    584         activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 16, 16, 8)    32          conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 16, 16, 8)    0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 16, 16, 32)   256         activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 32)   0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 32)     0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 8, 8, 32)     128         max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 32)     0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 8)      256         activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 8, 8, 8)      32          conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 8)      0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 8)      584         activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 8)      32          conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 8)      0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 32)     256         activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 32)     0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 32)     128         add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 32)     0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 8)      256         activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 16, 16, 32)   128         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 8, 8, 8)      32          conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 16, 16, 32)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 8)      0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 16, 16, 8)    256         activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 32, 32, 32)   128         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 8)      584         activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 16, 16, 8)    32          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 32, 32, 32)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 8, 8, 8)      32          conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 16, 16, 8)    0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 32, 32, 8)    256         activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 8)      0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 16, 16, 8)    584         activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 32, 32, 8)    32          conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 32)     256         activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 16, 16, 8)    32          conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 32, 32, 8)    0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 32)     0           add_24[0][0]                     \n",
            "                                                                 conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 16, 16, 8)    0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 32, 32, 8)    584         activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 32)   0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 16, 16, 32)   256         activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 32, 32, 8)    32          conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 16, 16, 32)   0           up_sampling2d_1[0][0]            \n",
            "                                                                 add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 16, 16, 32)   0           add_22[0][0]                     \n",
            "                                                                 conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 32, 32, 8)    0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 16, 16, 32)   0           add_26[0][0]                     \n",
            "                                                                 add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 32, 32, 32)   256         activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 16, 16, 32)   128         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 32)   0           add_19[0][0]                     \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 16, 16, 32)   0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 32, 32, 32)   128         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 16, 16, 8)    256         activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 32, 32, 32)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 16, 16, 8)    32          conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 32, 32, 8)    256         activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 16, 16, 8)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 32, 32, 8)    32          conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 16, 16, 8)    584         activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32, 32, 8)    0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 16, 16, 8)    32          conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 32, 32, 8)    584         activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 16, 16, 8)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 32, 32, 8)    32          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 16, 16, 32)   256         activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32, 32, 8)    0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 16, 16, 32)   0           add_27[0][0]                     \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 32, 32, 32)   256         activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 32)   0           add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 32, 32, 32)   0           add_20[0][0]                     \n",
            "                                                                 conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 32, 32, 32)   0           up_sampling2d_2[0][0]            \n",
            "                                                                 add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 32, 32, 32)   128         add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 32, 32, 32)   0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 32, 32, 32)   1024        activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 32, 32, 32)   128         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 32, 32, 32)   0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 32, 32, 32)   1024        activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 32, 32, 32)   0           conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_2 (TensorFlow [(4,)]               0           activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_2 (TensorFlowO [(None, 32, 32, 32)] 0           tf_op_layer_Shape_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 32, 32, 32)   0           tf_op_layer_Fill_2[0][0]         \n",
            "                                                                 activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 32, 32, 32)   0           add_30[0][0]                     \n",
            "                                                                 add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 32, 32, 32)   128         multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 32, 32, 32)   0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 32, 32, 8)    256         activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 32, 32, 8)    32          conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 32, 32, 8)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 32, 32, 8)    584         activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 32, 32, 8)    32          conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 32, 32, 8)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 32, 32, 32)   256         activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 32, 32, 32)   0           multiply_2[0][0]                 \n",
            "                                                                 conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 32, 32, 32)   128         add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 32, 32, 32)   0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 32, 32, 16)   512         activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 32, 32, 16)   64          conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 32, 32, 16)   0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 16, 16, 16)   2320        activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 16, 16, 16)   64          conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 16, 16, 16)   0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 16, 16, 64)   2048        activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 16, 16, 64)   1024        activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 16, 16, 64)   0           conv2d_88[0][0]                  \n",
            "                                                                 conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 16, 16, 64)   256         add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 16, 16, 64)   0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 16, 16, 16)   1024        activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 16, 16, 16)   64          conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 16, 16, 16)   0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 16, 16, 16)   2320        activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 16, 16, 16)   64          conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 16, 16, 16)   0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 16, 16, 64)   1024        activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 16, 16, 64)   0           add_32[0][0]                     \n",
            "                                                                 conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 8, 8, 64)     256         max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 8, 8, 64)     0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 16, 16, 64)   256         add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 8, 8, 16)     1024        activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 16, 16, 64)   0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 8, 8, 16)     64          conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 16, 16, 16)   1024        activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 8, 8, 16)     0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 16, 16, 16)   64          conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 8, 8, 16)     2320        activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 16, 16, 16)   0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 8, 8, 16)     64          conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 16, 16, 16)   2320        activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 8, 8, 16)     0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 16, 16, 16)   64          conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 8, 8, 64)     1024        activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 16, 16, 16)   0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 8, 8, 64)     0           max_pooling2d_3[0][0]            \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 16, 16, 64)   1024        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 8, 8, 64)     256         add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 16, 16, 64)   0           add_33[0][0]                     \n",
            "                                                                 conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 8, 8, 64)     0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 16, 16, 64)   256         add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 8, 8, 16)     1024        activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 16, 16, 64)   0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 8, 8, 16)     64          conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 16, 16, 16)   1024        activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 8, 8, 16)     0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 16, 16, 16)   64          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 8, 8, 16)     2320        activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 16, 16, 16)   0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 8, 8, 16)     64          conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 16, 16, 16)   2320        activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 8, 8, 16)     0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 16, 16, 16)   64          conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 8, 8, 64)     1024        activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 16, 16, 16)   0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 8, 8, 64)     0           add_36[0][0]                     \n",
            "                                                                 conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 16, 16, 64)   1024        activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 16, 16, 64)   0           add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 16, 16, 64)   0           add_34[0][0]                     \n",
            "                                                                 conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 16, 16, 64)   0           up_sampling2d_3[0][0]            \n",
            "                                                                 add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 16, 16, 64)   256         add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 16, 16, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 16, 64)   4096        activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 16, 16, 64)   256         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 16, 16, 64)   0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 16, 64)   4096        activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 16, 16, 64)   0           conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_3 (TensorFlow [(4,)]               0           activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_3 (TensorFlowO [(None, 16, 16, 64)] 0           tf_op_layer_Shape_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 16, 16, 64)   0           tf_op_layer_Fill_3[0][0]         \n",
            "                                                                 activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 16, 16, 64)   0           add_39[0][0]                     \n",
            "                                                                 add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 16, 16, 64)   256         multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 16, 16, 64)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 16, 16, 16)   1024        activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 16, 16, 16)   64          conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 16, 16, 16)   0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 16, 16, 16)   2320        activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 16, 16, 16)   64          conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 16, 16, 16)   0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 16, 16, 64)   1024        activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 16, 16, 64)   0           multiply_3[0][0]                 \n",
            "                                                                 conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 16, 16, 64)   256         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 16, 16, 64)   0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 16, 16, 32)   2048        activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 16, 16, 32)   128         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 16, 16, 32)   0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 8, 8, 32)     9248        activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 8, 8, 32)     128         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 8, 8, 32)     0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 8, 8, 128)    8192        activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 8, 8, 128)    4096        activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 8, 8, 128)    0           conv2d_112[0][0]                 \n",
            "                                                                 conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 8, 8, 128)    512         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 8, 8, 128)    0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 8, 8, 32)     4096        activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 8, 8, 32)     128         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 8, 8, 32)     0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 8, 8, 32)     9248        activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 8, 8, 32)     128         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 8, 8, 32)     0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 8, 8, 128)    4096        activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 8, 8, 128)    0           add_41[0][0]                     \n",
            "                                                                 conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 8, 8, 128)    512         add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 8, 8, 128)    0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 8, 8, 32)     4096        activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 8, 8, 32)     128         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 8, 8, 32)     0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 8, 8, 32)     9248        activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 8, 8, 32)     128         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 8, 8, 32)     0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 8, 8, 128)    4096        activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 8, 8, 128)    0           add_42[0][0]                     \n",
            "                                                                 conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 8, 8, 128)    512         add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 8, 8, 128)    512         add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 8, 8, 128)    0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 8, 8, 128)    0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 8, 8, 32)     4096        activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 8, 8, 32)     4096        activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 8, 8, 32)     128         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 8, 8, 32)     128         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 8, 8, 32)     0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 8, 8, 32)     0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 8, 8, 32)     9248        activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 8, 8, 32)     9248        activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 8, 8, 32)     128         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 8, 8, 32)     128         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 8, 8, 32)     0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 8, 8, 32)     0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 8, 8, 128)    4096        activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 8, 8, 128)    4096        activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 8, 8, 128)    0           add_45[0][0]                     \n",
            "                                                                 conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 8, 8, 128)    0           add_42[0][0]                     \n",
            "                                                                 conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 8, 8, 128)    512         add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 8, 8, 128)    512         add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 8, 8, 128)    0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 8, 8, 128)    0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 8, 8, 128)    16384       activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 8, 8, 32)     4096        activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 8, 8, 128)    512         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 8, 8, 32)     128         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 8, 8, 128)    0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 8, 8, 32)     0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 8, 8, 128)    16384       activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 8, 8, 32)     9248        activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 8, 8, 128)    0           conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 8, 8, 32)     128         conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_4 (TensorFlow [(4,)]               0           activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 8, 8, 32)     0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_4 (TensorFlowO [(None, 8, 8, 128)]  0           tf_op_layer_Shape_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 8, 8, 128)    4096        activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 8, 8, 128)    0           tf_op_layer_Fill_4[0][0]         \n",
            "                                                                 activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 8, 8, 128)    0           add_43[0][0]                     \n",
            "                                                                 conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "multiply_4 (Multiply)           (None, 8, 8, 128)    0           add_47[0][0]                     \n",
            "                                                                 add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 8, 8, 128)    512         multiply_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 8, 8, 128)    0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 8, 8, 32)     4096        activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 8, 8, 32)     128         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 8, 8, 32)     0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 8, 8, 32)     9248        activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 8, 8, 32)     128         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 8, 32)     0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 8, 8, 128)    4096        activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 8, 8, 128)    0           multiply_4[0][0]                 \n",
            "                                                                 conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 8, 8, 128)    512         add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 8, 8, 128)    0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 8, 8, 64)     8192        activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 8, 8, 64)     256         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 8, 8, 64)     0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 8, 8, 64)     36928       activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 8, 8, 64)     256         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 8, 8, 64)     0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 8, 8, 256)    32768       activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 8, 8, 256)    16384       activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 8, 8, 256)    0           conv2d_136[0][0]                 \n",
            "                                                                 conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 8, 8, 256)    1024        add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 8, 8, 256)    0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 8, 8, 64)     16384       activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 8, 8, 64)     256         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 8, 8, 64)     0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 8, 8, 64)     36928       activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 8, 8, 64)     256         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 8, 8, 64)     0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 8, 8, 256)    16384       activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 8, 8, 256)    0           add_49[0][0]                     \n",
            "                                                                 conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 8, 8, 256)    1024        add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 8, 8, 256)    0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 8, 8, 256)    0           activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 1, 1, 10)     2570        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 10)           0           conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 10)           0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 10)           0           dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 396,810\n",
            "Trainable params: 389,258\n",
            "Non-trainable params: 7,552\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW6MOIbLl7FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "num_classes = 10 \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def get_cutout(sl=0.02, sh=0.4, rand_r=0.3):\n",
        "    def cutout(img):\n",
        "        if np.random.rand() > 0.5:\n",
        "            return img\n",
        "        while True:\n",
        "            s = np.random.uniform(sl, sh) * 32 * 32\n",
        "            r = np.random.uniform(rand_r, 1/rand_r)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, 32)\n",
        "            top = np.random.randint(0, 32)\n",
        "            if left + w <= 32 and top + h <= 32:\n",
        "                break\n",
        "        img[top:top + h, left:left + w, :] = np.random.uniform(0, 1)\n",
        "        return img\n",
        "    return cutout\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    preprocessing_function=get_cutout()\n",
        "    )\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3CHJnIDmNJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = '/content'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHhhaI8BmRKz",
        "colab_type": "code",
        "outputId": "a3099691-a119-482b-c4ed-e943972d0eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                                      epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                      validation_data=(x_test, y_test), callbacks=[cp_callback, warm_up_lr])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2175 - accuracy: 0.1873\n",
            "Epoch 00001: saving model to /content\n",
            "390/390 [==============================] - 46s 119ms/step - loss: 2.2175 - accuracy: 0.1873 - val_loss: 2.5559 - val_accuracy: 0.1338\n",
            "Epoch 2/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8641 - accuracy: 0.3232\n",
            "Epoch 00002: saving model to /content\n",
            "390/390 [==============================] - 45s 115ms/step - loss: 1.8641 - accuracy: 0.3232 - val_loss: 1.6458 - val_accuracy: 0.3850\n",
            "Epoch 3/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.7214 - accuracy: 0.3778\n",
            "Epoch 00003: saving model to /content\n",
            "390/390 [==============================] - 45s 115ms/step - loss: 1.7214 - accuracy: 0.3778 - val_loss: 1.4680 - val_accuracy: 0.4612\n",
            "Epoch 4/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.6265 - accuracy: 0.4117\n",
            "Epoch 00004: saving model to /content\n",
            "390/390 [==============================] - 44s 114ms/step - loss: 1.6265 - accuracy: 0.4117 - val_loss: 1.4026 - val_accuracy: 0.4893\n",
            "Epoch 5/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.5549 - accuracy: 0.4396\n",
            "Epoch 00005: saving model to /content\n",
            "390/390 [==============================] - 45s 114ms/step - loss: 1.5549 - accuracy: 0.4396 - val_loss: 1.3754 - val_accuracy: 0.4986\n",
            "Epoch 6/200\n",
            " 36/390 [=>............................] - ETA: 36s - loss: 1.5124 - accuracy: 0.4522"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1c6c9a76ac87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n\u001b[1;32m      2\u001b[0m                                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                       validation_data=(x_test, y_test), callbacks=[cp_callback, warm_up_lr])  \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2yHhHJ3IOYM",
        "colab_type": "code",
        "outputId": "3618a38f-803d-426f-ec65-fd0d01ce63c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "# hold base\n",
        "history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                                      epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                      validation_data=(x_test, y_test), callbacks=[warm_up_lr, cp_callback])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "241/781 [========>.....................] - ETA: 25s - loss: 0.4182 - accuracy: 0.8191"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-69c1c5b59789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n\u001b[1;32m      2\u001b[0m                                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                       validation_data=(x_test, y_test), callbacks=[warm_up_lr, cp_callback])  \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    303\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \"\"\"\n\u001b[1;32m   3494\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3495\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3401\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3403\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3404\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3526\u001b[0m             \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3528\u001b[0;31m         \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpartition\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGgIw5-Xb8ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrItjjuMseev",
        "colab_type": "code",
        "outputId": "7aa57e8f-b982-4167-9f2e-5f020fa80aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation, Conv2D, BatchNormalization, Add, ZeroPadding2D, Input\n",
        "from tensorflow.keras.layers import MaxPooling2D, UpSampling2D, Multiply, AveragePooling2D, Flatten, Dropout\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "Part 1: Pursuing higher training accuracy\n",
        "\"\"\"\n",
        "# Model Defination\n",
        "class ResBlock:\n",
        "    def __init__(self, out_channels, in_channels=None, strides=1):\n",
        "        self.out_channels = out_channels\n",
        "        self.in_channels = in_channels if in_channels else out_channels\n",
        "        self.strides = strides\n",
        "        \n",
        "    def __call__(self, x):    \n",
        "        res = BatchNormalization()(x)\n",
        "        res_1 = Activation('relu')(res)\n",
        "        \n",
        "        res = Conv2D(filters=self.out_channels//4, kernel_size=(1,1), strides=1, use_bias=False)(res_1)\n",
        "        res = BatchNormalization()(res)\n",
        "        res = Activation('relu')(res)\n",
        "        # res = ZeroPadding2D(padding=(1,1))(res)\n",
        "        res = Conv2D(filters=self.out_channels//4, kernel_size=(3,3), strides=self.strides, padding='same')(res)\n",
        "        res = BatchNormalization()(res)\n",
        "        res = Activation('relu')(res)\n",
        "        res = Conv2D(filters=self.out_channels, kernel_size=(1,1), strides=1, use_bias=False)(res)\n",
        "        \n",
        "        if self.strides != 1 or self.in_channels != self.out_channels:\n",
        "            identity = Conv2D(filters=self.out_channels, kernel_size=(1,1), strides=self.strides, use_bias=False)(res_1)\n",
        "        else:\n",
        "            identity = x\n",
        "        output = Add()([identity, res])\n",
        "        return output\n",
        "    \n",
        "    \n",
        "class MaxPool2D:\n",
        "    def __init__(self, pool_size=(3,3), strides=2, padding=(1,1)):\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        # padd = ZeroPadding2D(padding=self.padding)(x)\n",
        "        # pool = MaxPooling2D(pool_size=self.pool_size, strides=self.strides, padding='same')(padd)\n",
        "        pool = MaxPooling2D(pool_size=self.pool_size, strides=self.strides, padding='same')(x)\n",
        "        return pool\n",
        "\n",
        "\n",
        "class AvgPool2D:\n",
        "    def __init__(self, pool_size=(3,3), strides=2, padding=(1,1)):\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        padd = ZeroPadding2D(padding=self.padding)(x)\n",
        "        pool = AveragePooling2D(pool_size=self.pool_size, strides=self.strides, padding='valid')(padd)\n",
        "        return pool\n",
        "    \n",
        "\n",
        "class AttentionModule_stage1:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        \n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "\n",
        "        out_mpool1 = MaxPool2D()(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_mpool1)\n",
        "        out_skip1_connection = ResBlock(channels)(out_softmax1)\n",
        "\n",
        "        out_mpool2 = MaxPool2D()(out_softmax1)\n",
        "        out_softmax2 = ResBlock(channels)(out_mpool2)\n",
        "        out_softmax2 = ResBlock(channels)(out_softmax2)\n",
        "\n",
        "        out_interp2 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax2), out_softmax1])\n",
        "        out = Add()([out_interp2, out_skip1_connection])\n",
        "\n",
        "        out_softmax3 = ResBlock(channels)(out)\n",
        "        out_interp1 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax3), out_trunk])\n",
        "\n",
        "        out_softmax4 = BatchNormalization()(out_interp1)\n",
        "        out_softmax4 = Activation('relu')(out_softmax4)\n",
        "        out_softmax4 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax4)\n",
        "        out_softmax4 = BatchNormalization()(out_softmax4)\n",
        "        out_softmax4 = Activation('relu')(out_softmax4)\n",
        "        out_softmax4 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax4)\n",
        "        out_softmax4 = Activation('sigmoid')(out_softmax4)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax4), out_softmax4])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    \n",
        "    \n",
        "class AttentionModule_stage2:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        \n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "\n",
        "        out_mpool1 = MaxPool2D()(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_mpool1)\n",
        "        out_softmax1 = ResBlock(channels)(out_softmax1)\n",
        "\n",
        "        out_interp1 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax1), out_trunk])\n",
        "        \n",
        "        out_softmax2 = BatchNormalization()(out_interp1)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = BatchNormalization()(out_softmax2)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = Activation('sigmoid')(out_softmax2)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax2), out_softmax2])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    \n",
        "    \n",
        "class AttentionModule_stage3:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "                \n",
        "        out_softmax1 = ResBlock(channels)(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_softmax1)\n",
        "        \n",
        "        out_softmax2 = BatchNormalization()(out_softmax1)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = BatchNormalization()(out_softmax2)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = Activation('sigmoid')(out_softmax2)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax2), out_softmax2])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    \n",
        "\n",
        "class ResidualAttentionModel_32input_red:\n",
        "    def __init__(self, classes=10):\n",
        "        self.classes = classes\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        classes = self.classes\n",
        "                \n",
        "        # x = ZeroPadding2D(padding=(3,3))(x)\n",
        "        x = Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', use_bias=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        \n",
        "        # x = MaxPool2D()(x)\n",
        "        x = ResBlock(32, in_channels=32)(x)\n",
        "        x = AttentionModule_stage1(32)(x)\n",
        "        x = ResBlock(64, in_channels=32, strides=2)(x)\n",
        "        x = AttentionModule_stage2(64)(x)\n",
        "        x = ResBlock(128, in_channels=64, strides=2)(x)\n",
        "        x = AttentionModule_stage3(128)(x)\n",
        "        x = ResBlock(256, in_channels=128)(x)\n",
        "        # x = ResBlock(1024)(x)\n",
        "        x = ResBlock(256)(x)\n",
        "        \n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = AvgPool2D(pool_size=(8,8), strides=1, padding=(0,0))(x)\n",
        "        \n",
        "        x = Conv2D(classes, kernel_size=(1,1))(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "\n",
        "def cosine_decay_warmup(global_step, lr_base, num_steps, warmup_steps=0):\n",
        "    lr = 0.5 * lr_base * (1 + np.cos( np.pi *\n",
        "        (global_step - warmup_steps) / float(num_steps - warmup_steps)))\n",
        "    if warmup_steps > 0:\n",
        "        slope = lr_base / warmup_steps\n",
        "        lr = np.where(global_step < warmup_steps, slope * global_step, lr)\n",
        "    return np.where(global_step > num_steps, 0.0, lr)\n",
        "\n",
        "\n",
        "class Scheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, lr_base, num_epochs,\n",
        "                 warmup_epochs=0, num_sample = 50000, batch_size=64):\n",
        "\n",
        "        super(Scheduler, self).__init__()\n",
        "        self.lr_base = lr_base\n",
        "        self.num_steps = int(num_epochs * num_sample / batch_size)\n",
        "        self.global_step = 0\n",
        "        self.warmup_steps = int(warmup_epochs * num_sample / batch_size)\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.global_step = self.global_step + 1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        lr = cosine_decay_warmup(global_step=self.global_step,\n",
        "                                 lr_base=self.lr_base,\n",
        "                                 num_steps=self.num_steps,\n",
        "                                 warmup_steps=self.warmup_steps,\n",
        "                                 )\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "num_sample = 50000\n",
        "epochs = 250\n",
        "batch_size = 64\n",
        "warmup_epochs = 25\n",
        "lr_base = 0.001\n",
        "\n",
        "warm_up_lr = Scheduler(lr_base=lr_base,\n",
        "                       num_epochs=epochs,\n",
        "                       warmup_epochs=warmup_epochs, \n",
        "                       num_sample = num_sample, \n",
        "                       batch_size=batch_size,\n",
        "                       )\n",
        "\n",
        "checkpoint_path = '/content'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# Connecting Model\n",
        "model_in = Input((32, 32, 3))\n",
        "model_out = ResidualAttentionModel_32input_red()(model_in)\n",
        "model_out = Dropout(0.2)(model_out)\n",
        "model_out = Activation('softmax')(model_out)\n",
        "red_model = Model(inputs=model_in, outputs=model_out)\n",
        "red_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "red_model.summary()\n",
        "\n",
        "# Handling Dataset\n",
        "num_classes = 10 \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def get_cutout(sl=0.02, sh=0.4, rand_r=0.3):\n",
        "    def cutout(img):\n",
        "        if np.random.rand() > 0.5:\n",
        "            return img\n",
        "        while True:\n",
        "            s = np.random.uniform(sl, sh) * 32 * 32\n",
        "            r = np.random.uniform(rand_r, 1/rand_r)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, 32)\n",
        "            top = np.random.randint(0, 32)\n",
        "            if left + w <= 32 and top + h <= 32:\n",
        "                break\n",
        "        img[top:top + h, left:left + w, :] = np.random.uniform(0, 1)\n",
        "        return img\n",
        "    return cutout\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    preprocessing_function=get_cutout()\n",
        "    )\n",
        "\n",
        "datagen.fit(x_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 8)    256         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 8)    32          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 8)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 8)    584         activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 8)    32          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 8)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   256         activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 32)   0           activation[0][0]                 \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 8)    256         activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 8)    32          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 8)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 8)    584         activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 8)    32          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 8)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 32)   256         activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 32)   0           add[0][0]                        \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 32)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 32)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 8)    256         activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 8)    32          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 8)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 8)    584         activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 8)    32          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 8)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 32)   256         activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 32)   0           max_pooling2d[0][0]              \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 32)     0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 32)     128         max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 32)     0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 8)      256         activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 8)      32          conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 8)      0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 8)      584         activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 8)      32          conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 8)      0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 32)     256         activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 32)     0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 32)     128         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 32)     0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 8)      256         activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 32)   128         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 8)      32          conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 32)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 8)      0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 8)    256         activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 32)   128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 8)      584         activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 8)    32          conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 8)      32          conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 8)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 8)    256         activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 8)      0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 8)    584         activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 8)    32          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 32)     256         activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 8)    32          conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 8)    0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 32)     0           add_6[0][0]                      \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 8)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 8)    584         activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 16, 16, 32)   0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 32)   256         activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 8)    32          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 16, 16, 32)   0           up_sampling2d[0][0]              \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 32)   0           add_4[0][0]                      \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 8)    0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 16, 32)   0           add_8[0][0]                      \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 32)   256         activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 32)   128         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 32)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 32)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 8)    256         activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 8)    32          conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 8)    256         activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 8)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 8)    32          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 8)    584         activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 8)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 8)    32          conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 8)    584         activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 8)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 8)    32          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 32)   256         activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 8)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 16, 16, 32)   0           add_9[0][0]                      \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 32)   256         activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 32)   0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 32)   0           add_2[0][0]                      \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 32, 32, 32)   0           up_sampling2d_1[0][0]            \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 32, 32, 32)   128         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 32)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 32, 32, 32)   1024        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 32, 32, 32)   128         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 32, 32, 32)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 32, 32, 32)   1024        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 32, 32, 32)   0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape (TensorFlowOp [(4,)]               0           activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill (TensorFlowOpL [(None, 32, 32, 32)] 0           tf_op_layer_Shape[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 32, 32, 32)   0           tf_op_layer_Fill[0][0]           \n",
            "                                                                 activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 32, 32, 32)   0           add_12[0][0]                     \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 32, 32, 32)   128         multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 32, 32, 32)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 32, 32, 8)    256         activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 8)    32          conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 32, 32, 8)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 32, 32, 8)    584         activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 8)    32          conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 32, 32, 8)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 32)   256         activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 32, 32, 32)   0           multiply[0][0]                   \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 32)   128         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 32, 32, 32)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 16)   512         activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 32, 32, 16)   64          conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 32, 32, 16)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 16)   2320        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 16)   64          conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 16, 16, 16)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 64)   2048        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 64)   1024        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 64)   0           conv2d_36[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 64)   256         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 16, 16, 64)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 16)   1024        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 16, 16, 16)   64          conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 16)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 16)   2320        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 16)   64          conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 16)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 64)   1024        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 16, 16, 64)   0           add_14[0][0]                     \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 8, 8, 64)     256         max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 8, 64)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 64)   256         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 8, 8, 16)     1024        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 64)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 8, 8, 16)     64          conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 16, 16, 16)   1024        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 16)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 16)   64          conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 8, 8, 16)     2320        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 16)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 16)     64          conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 16, 16, 16)   2320        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 16)     0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 16)   64          conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 8, 8, 64)     1024        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 16, 16, 16)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 8, 8, 64)     0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 64)   1024        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 64)     256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 16, 16, 64)   0           add_15[0][0]                     \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 64)     0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 64)   256         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 8, 8, 16)     1024        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 16, 16, 64)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 16)     64          conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 16)   1024        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 16)     0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 16)   64          conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 8, 8, 16)     2320        activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 16)   0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 16)     64          conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 16)   2320        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 16)     0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 16)   64          conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 8, 8, 64)     1024        activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 16)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 8, 8, 64)     0           add_18[0][0]                     \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 16, 16, 64)   1024        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 64)   0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 16, 16, 64)   0           add_16[0][0]                     \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 16, 16, 64)   0           up_sampling2d_2[0][0]            \n",
            "                                                                 add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 16, 16, 64)   256         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 16, 16, 64)   0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 16, 16, 64)   4096        activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 16, 16, 64)   256         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 16, 16, 64)   0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 16, 16, 64)   4096        activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 16, 16, 64)   0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_1 (TensorFlow [(4,)]               0           activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_1 (TensorFlowO [(None, 16, 16, 64)] 0           tf_op_layer_Shape_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 64)   0           tf_op_layer_Fill_1[0][0]         \n",
            "                                                                 activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 16, 16, 64)   0           add_21[0][0]                     \n",
            "                                                                 add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 16, 16, 64)   256         multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 16, 16, 64)   0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 16, 16, 16)   1024        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 16, 16, 16)   64          conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 16, 16, 16)   0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 16)   2320        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 16, 16, 16)   64          conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 16, 16, 16)   0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 16, 16, 64)   1024        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 64)   0           multiply_1[0][0]                 \n",
            "                                                                 conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 16, 16, 64)   256         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 16, 16, 64)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 16, 16, 32)   2048        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 16, 16, 32)   128         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 16, 16, 32)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 32)     9248        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 8, 8, 32)     128         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 8, 8, 32)     0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 128)    8192        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 128)    4096        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 8, 8, 128)    0           conv2d_60[0][0]                  \n",
            "                                                                 conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 8, 8, 128)    512         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 8, 8, 128)    0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 32)     4096        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 8, 8, 32)     128         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 8, 8, 32)     0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 8, 8, 32)     9248        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 8, 8, 32)     128         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 8, 8, 32)     0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 8, 8, 128)    4096        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 128)    0           add_23[0][0]                     \n",
            "                                                                 conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 8, 8, 128)    512         add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 8, 8, 128)    0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 8, 8, 32)     4096        activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 8, 8, 32)     128         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 32)     0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 32)     9248        activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 8, 8, 32)     128         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 32)     0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 128)    4096        activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 8, 8, 128)    0           add_24[0][0]                     \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 128)    512         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 8, 8, 128)    512         add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 128)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 8, 8, 128)    0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 32)     4096        activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 8, 8, 32)     4096        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 32)     128         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 8, 8, 32)     128         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 32)     0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 8, 8, 32)     0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 32)     9248        activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 8, 8, 32)     9248        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 8, 8, 32)     128         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 8, 8, 32)     128         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 32)     0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 8, 8, 32)     0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 128)    4096        activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 8, 8, 128)    4096        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 8, 8, 128)    0           add_27[0][0]                     \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 128)    0           add_24[0][0]                     \n",
            "                                                                 conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 8, 8, 128)    512         add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 8, 8, 128)    512         add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 128)    0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 8, 8, 128)    0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 128)    16384       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 8, 8, 32)     4096        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 128)    512         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 8, 8, 32)     128         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 128)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 8, 8, 32)     0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 128)    16384       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 8, 8, 32)     9248        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 128)    0           conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 8, 8, 32)     128         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_2 (TensorFlow [(4,)]               0           activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 8, 8, 32)     0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_2 (TensorFlowO [(None, 8, 8, 128)]  0           tf_op_layer_Shape_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 8, 8, 128)    4096        activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 8, 8, 128)    0           tf_op_layer_Fill_2[0][0]         \n",
            "                                                                 activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 8, 8, 128)    0           add_25[0][0]                     \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 8, 8, 128)    0           add_29[0][0]                     \n",
            "                                                                 add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 128)    512         multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 128)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 32)     4096        activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 32)     128         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 32)     0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 8, 8, 32)     9248        activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 32)     128         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 32)     0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 8, 8, 128)    4096        activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 8, 8, 128)    0           multiply_2[0][0]                 \n",
            "                                                                 conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 128)    512         add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 128)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 8, 8, 64)     8192        activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 64)     256         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 64)     0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 8, 8, 64)     36928       activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 64)     256         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 8, 8, 64)     0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 256)    32768       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 256)    16384       activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 8, 8, 256)    0           conv2d_84[0][0]                  \n",
            "                                                                 conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 256)    1024        add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 8, 8, 256)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 64)     16384       activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 64)     256         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 8, 8, 64)     0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 64)     36928       activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 8, 8, 64)     256         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 8, 8, 64)     0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 256)    16384       activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 8, 8, 256)    0           add_31[0][0]                     \n",
            "                                                                 conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 8, 8, 256)    1024        add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 8, 8, 256)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 8, 8, 256)    0           activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 1, 1, 10)     2570        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 10)           0           conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 10)           0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 10)           0           dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 396,810\n",
            "Trainable params: 389,258\n",
            "Non-trainable params: 7,552\n",
            "__________________________________________________________________________________________________\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOdc3w6Esb28",
        "colab_type": "code",
        "outputId": "dd24cc5a-bef4-4c72-ffe6-424794e388ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "PART2: Training with reduced dataset\n",
        "\"\"\"\n",
        "\n",
        "# re-handling dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = np.reshape(x_train, (50000, 32*32*3))\n",
        "\n",
        "factor = 1/32\n",
        "num_train = int(5000 * factor)   # number of each training class\n",
        "\n",
        "data_set = []\n",
        "for i in range(10):\n",
        "    temp = np.concatenate(\n",
        "        (x_train[np.where(y_train == i)[0], :], y_train[np.where(y_train == i)[0], :]), \n",
        "        axis=1)\n",
        "    np.random.shuffle(temp)\n",
        "    temp = temp[:num_train]\n",
        "    if isinstance(data_set, list):\n",
        "        data_set = temp\n",
        "    else:\n",
        "        data_set = np.concatenate((data_set, temp), axis=0)\n",
        "\n",
        "np.random.shuffle(data_set)\n",
        "\n",
        "x_train = np.reshape(data_set[:, :-1], (num_train * 10, 32, 32, 3))\n",
        "y_train = data_set[:, -1][:, None]\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "num_classes = 10 \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# reset scheduler\n",
        "num_sample = num_train * 10\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "warmup_epochs = 10\n",
        "lr_base = 0.001\n",
        "warm_up_lr = Scheduler(lr_base=lr_base,\n",
        "                       num_epochs=epochs,\n",
        "                       warmup_epochs=warmup_epochs, \n",
        "                       num_sample = num_sample, \n",
        "                       batch_size=batch_size,\n",
        "                       )\n",
        "\n",
        "# reconstruct model\n",
        "model_in = Input((32, 32, 3))\n",
        "model_out = ResidualAttentionModel_32input_red()(model_in)\n",
        "model_out = Dropout(0.2)(model_out)\n",
        "model_out = Activation('softmax')(model_out)\n",
        "red_model = Model(inputs=model_in, outputs=model_out)\n",
        "red_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "red_model.summary()\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    preprocessing_function=get_cutout()\n",
        "    )\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_445 (Conv2D)             (None, 32, 32, 32)   864         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_430 (BatchN (None, 32, 32, 32)   128         conv2d_445[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_450 (Activation)     (None, 32, 32, 32)   0           batch_normalization_430[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_431 (BatchN (None, 32, 32, 32)   128         activation_450[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_451 (Activation)     (None, 32, 32, 32)   0           batch_normalization_431[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_446 (Conv2D)             (None, 32, 32, 8)    256         activation_451[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_432 (BatchN (None, 32, 32, 8)    32          conv2d_446[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_452 (Activation)     (None, 32, 32, 8)    0           batch_normalization_432[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_447 (Conv2D)             (None, 32, 32, 8)    584         activation_452[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_433 (BatchN (None, 32, 32, 8)    32          conv2d_447[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_453 (Activation)     (None, 32, 32, 8)    0           batch_normalization_433[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_448 (Conv2D)             (None, 32, 32, 32)   256         activation_453[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_165 (Add)                   (None, 32, 32, 32)   0           activation_450[0][0]             \n",
            "                                                                 conv2d_448[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_434 (BatchN (None, 32, 32, 32)   128         add_165[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_454 (Activation)     (None, 32, 32, 32)   0           batch_normalization_434[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_449 (Conv2D)             (None, 32, 32, 8)    256         activation_454[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_435 (BatchN (None, 32, 32, 8)    32          conv2d_449[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_455 (Activation)     (None, 32, 32, 8)    0           batch_normalization_435[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_450 (Conv2D)             (None, 32, 32, 8)    584         activation_455[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_436 (BatchN (None, 32, 32, 8)    32          conv2d_450[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_456 (Activation)     (None, 32, 32, 8)    0           batch_normalization_436[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_451 (Conv2D)             (None, 32, 32, 32)   256         activation_456[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_166 (Add)                   (None, 32, 32, 32)   0           add_165[0][0]                    \n",
            "                                                                 conv2d_451[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 16, 16, 32)   0           add_166[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_443 (BatchN (None, 16, 16, 32)   128         max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_463 (Activation)     (None, 16, 16, 32)   0           batch_normalization_443[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_458 (Conv2D)             (None, 16, 16, 8)    256         activation_463[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_444 (BatchN (None, 16, 16, 8)    32          conv2d_458[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_464 (Activation)     (None, 16, 16, 8)    0           batch_normalization_444[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_459 (Conv2D)             (None, 16, 16, 8)    584         activation_464[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_445 (BatchN (None, 16, 16, 8)    32          conv2d_459[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_465 (Activation)     (None, 16, 16, 8)    0           batch_normalization_445[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_460 (Conv2D)             (None, 16, 16, 32)   256         activation_465[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_169 (Add)                   (None, 16, 16, 32)   0           max_pooling2d_15[0][0]           \n",
            "                                                                 conv2d_460[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 8, 8, 32)     0           add_169[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_449 (BatchN (None, 8, 8, 32)     128         max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_469 (Activation)     (None, 8, 8, 32)     0           batch_normalization_449[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_464 (Conv2D)             (None, 8, 8, 8)      256         activation_469[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_450 (BatchN (None, 8, 8, 8)      32          conv2d_464[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_470 (Activation)     (None, 8, 8, 8)      0           batch_normalization_450[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_465 (Conv2D)             (None, 8, 8, 8)      584         activation_470[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_451 (BatchN (None, 8, 8, 8)      32          conv2d_465[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_471 (Activation)     (None, 8, 8, 8)      0           batch_normalization_451[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_466 (Conv2D)             (None, 8, 8, 32)     256         activation_471[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_171 (Add)                   (None, 8, 8, 32)     0           max_pooling2d_16[0][0]           \n",
            "                                                                 conv2d_466[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_452 (BatchN (None, 8, 8, 32)     128         add_171[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_472 (Activation)     (None, 8, 8, 32)     0           batch_normalization_452[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_467 (Conv2D)             (None, 8, 8, 8)      256         activation_472[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_446 (BatchN (None, 16, 16, 32)   128         add_169[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_453 (BatchN (None, 8, 8, 8)      32          conv2d_467[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_466 (Activation)     (None, 16, 16, 32)   0           batch_normalization_446[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_473 (Activation)     (None, 8, 8, 8)      0           batch_normalization_453[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_461 (Conv2D)             (None, 16, 16, 8)    256         activation_466[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_437 (BatchN (None, 32, 32, 32)   128         add_166[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_468 (Conv2D)             (None, 8, 8, 8)      584         activation_473[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_447 (BatchN (None, 16, 16, 8)    32          conv2d_461[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_457 (Activation)     (None, 32, 32, 32)   0           batch_normalization_437[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_454 (BatchN (None, 8, 8, 8)      32          conv2d_468[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_467 (Activation)     (None, 16, 16, 8)    0           batch_normalization_447[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_452 (Conv2D)             (None, 32, 32, 8)    256         activation_457[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_474 (Activation)     (None, 8, 8, 8)      0           batch_normalization_454[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_462 (Conv2D)             (None, 16, 16, 8)    584         activation_467[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_438 (BatchN (None, 32, 32, 8)    32          conv2d_452[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_469 (Conv2D)             (None, 8, 8, 32)     256         activation_474[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_448 (BatchN (None, 16, 16, 8)    32          conv2d_462[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_458 (Activation)     (None, 32, 32, 8)    0           batch_normalization_438[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_172 (Add)                   (None, 8, 8, 32)     0           add_171[0][0]                    \n",
            "                                                                 conv2d_469[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_468 (Activation)     (None, 16, 16, 8)    0           batch_normalization_448[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_453 (Conv2D)             (None, 32, 32, 8)    584         activation_458[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_15 (UpSampling2D) (None, 16, 16, 32)   0           add_172[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_463 (Conv2D)             (None, 16, 16, 32)   256         activation_468[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_439 (BatchN (None, 32, 32, 8)    32          conv2d_453[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_173 (Add)                   (None, 16, 16, 32)   0           up_sampling2d_15[0][0]           \n",
            "                                                                 add_169[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_170 (Add)                   (None, 16, 16, 32)   0           add_169[0][0]                    \n",
            "                                                                 conv2d_463[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_459 (Activation)     (None, 32, 32, 8)    0           batch_normalization_439[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_174 (Add)                   (None, 16, 16, 32)   0           add_173[0][0]                    \n",
            "                                                                 add_170[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_454 (Conv2D)             (None, 32, 32, 32)   256         activation_459[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_455 (BatchN (None, 16, 16, 32)   128         add_174[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_167 (Add)                   (None, 32, 32, 32)   0           add_166[0][0]                    \n",
            "                                                                 conv2d_454[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_475 (Activation)     (None, 16, 16, 32)   0           batch_normalization_455[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_440 (BatchN (None, 32, 32, 32)   128         add_167[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_470 (Conv2D)             (None, 16, 16, 8)    256         activation_475[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_460 (Activation)     (None, 32, 32, 32)   0           batch_normalization_440[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_456 (BatchN (None, 16, 16, 8)    32          conv2d_470[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_455 (Conv2D)             (None, 32, 32, 8)    256         activation_460[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_476 (Activation)     (None, 16, 16, 8)    0           batch_normalization_456[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_441 (BatchN (None, 32, 32, 8)    32          conv2d_455[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_471 (Conv2D)             (None, 16, 16, 8)    584         activation_476[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_461 (Activation)     (None, 32, 32, 8)    0           batch_normalization_441[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_457 (BatchN (None, 16, 16, 8)    32          conv2d_471[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_456 (Conv2D)             (None, 32, 32, 8)    584         activation_461[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_477 (Activation)     (None, 16, 16, 8)    0           batch_normalization_457[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_442 (BatchN (None, 32, 32, 8)    32          conv2d_456[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_472 (Conv2D)             (None, 16, 16, 32)   256         activation_477[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_462 (Activation)     (None, 32, 32, 8)    0           batch_normalization_442[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_175 (Add)                   (None, 16, 16, 32)   0           add_174[0][0]                    \n",
            "                                                                 conv2d_472[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_457 (Conv2D)             (None, 32, 32, 32)   256         activation_462[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_16 (UpSampling2D) (None, 32, 32, 32)   0           add_175[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_168 (Add)                   (None, 32, 32, 32)   0           add_167[0][0]                    \n",
            "                                                                 conv2d_457[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_176 (Add)                   (None, 32, 32, 32)   0           up_sampling2d_16[0][0]           \n",
            "                                                                 add_168[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_458 (BatchN (None, 32, 32, 32)   128         add_176[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_478 (Activation)     (None, 32, 32, 32)   0           batch_normalization_458[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_473 (Conv2D)             (None, 32, 32, 32)   1024        activation_478[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_459 (BatchN (None, 32, 32, 32)   128         conv2d_473[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_479 (Activation)     (None, 32, 32, 32)   0           batch_normalization_459[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_474 (Conv2D)             (None, 32, 32, 32)   1024        activation_479[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_480 (Activation)     (None, 32, 32, 32)   0           conv2d_474[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_15 (TensorFlo [(4,)]               0           activation_480[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_15 (TensorFlow [(None, 32, 32, 32)] 0           tf_op_layer_Shape_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_177 (Add)                   (None, 32, 32, 32)   0           tf_op_layer_Fill_15[0][0]        \n",
            "                                                                 activation_480[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_15 (Multiply)          (None, 32, 32, 32)   0           add_177[0][0]                    \n",
            "                                                                 add_168[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_460 (BatchN (None, 32, 32, 32)   128         multiply_15[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_481 (Activation)     (None, 32, 32, 32)   0           batch_normalization_460[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_475 (Conv2D)             (None, 32, 32, 8)    256         activation_481[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_461 (BatchN (None, 32, 32, 8)    32          conv2d_475[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_482 (Activation)     (None, 32, 32, 8)    0           batch_normalization_461[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_476 (Conv2D)             (None, 32, 32, 8)    584         activation_482[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_462 (BatchN (None, 32, 32, 8)    32          conv2d_476[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_483 (Activation)     (None, 32, 32, 8)    0           batch_normalization_462[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_477 (Conv2D)             (None, 32, 32, 32)   256         activation_483[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_178 (Add)                   (None, 32, 32, 32)   0           multiply_15[0][0]                \n",
            "                                                                 conv2d_477[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_463 (BatchN (None, 32, 32, 32)   128         add_178[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_484 (Activation)     (None, 32, 32, 32)   0           batch_normalization_463[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_478 (Conv2D)             (None, 32, 32, 16)   512         activation_484[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_464 (BatchN (None, 32, 32, 16)   64          conv2d_478[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_485 (Activation)     (None, 32, 32, 16)   0           batch_normalization_464[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_479 (Conv2D)             (None, 16, 16, 16)   2320        activation_485[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_465 (BatchN (None, 16, 16, 16)   64          conv2d_479[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_486 (Activation)     (None, 16, 16, 16)   0           batch_normalization_465[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_481 (Conv2D)             (None, 16, 16, 64)   2048        activation_484[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_480 (Conv2D)             (None, 16, 16, 64)   1024        activation_486[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_179 (Add)                   (None, 16, 16, 64)   0           conv2d_481[0][0]                 \n",
            "                                                                 conv2d_480[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_466 (BatchN (None, 16, 16, 64)   256         add_179[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_487 (Activation)     (None, 16, 16, 64)   0           batch_normalization_466[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_482 (Conv2D)             (None, 16, 16, 16)   1024        activation_487[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_467 (BatchN (None, 16, 16, 16)   64          conv2d_482[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_488 (Activation)     (None, 16, 16, 16)   0           batch_normalization_467[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_483 (Conv2D)             (None, 16, 16, 16)   2320        activation_488[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_468 (BatchN (None, 16, 16, 16)   64          conv2d_483[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_489 (Activation)     (None, 16, 16, 16)   0           batch_normalization_468[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_484 (Conv2D)             (None, 16, 16, 64)   1024        activation_489[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_180 (Add)                   (None, 16, 16, 64)   0           add_179[0][0]                    \n",
            "                                                                 conv2d_484[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 8, 8, 64)     0           add_180[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_475 (BatchN (None, 8, 8, 64)     256         max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_496 (Activation)     (None, 8, 8, 64)     0           batch_normalization_475[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_469 (BatchN (None, 16, 16, 64)   256         add_180[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_491 (Conv2D)             (None, 8, 8, 16)     1024        activation_496[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_490 (Activation)     (None, 16, 16, 64)   0           batch_normalization_469[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_476 (BatchN (None, 8, 8, 16)     64          conv2d_491[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_485 (Conv2D)             (None, 16, 16, 16)   1024        activation_490[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_497 (Activation)     (None, 8, 8, 16)     0           batch_normalization_476[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_470 (BatchN (None, 16, 16, 16)   64          conv2d_485[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_492 (Conv2D)             (None, 8, 8, 16)     2320        activation_497[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_491 (Activation)     (None, 16, 16, 16)   0           batch_normalization_470[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_477 (BatchN (None, 8, 8, 16)     64          conv2d_492[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_486 (Conv2D)             (None, 16, 16, 16)   2320        activation_491[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_498 (Activation)     (None, 8, 8, 16)     0           batch_normalization_477[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_471 (BatchN (None, 16, 16, 16)   64          conv2d_486[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_493 (Conv2D)             (None, 8, 8, 64)     1024        activation_498[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_492 (Activation)     (None, 16, 16, 16)   0           batch_normalization_471[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_183 (Add)                   (None, 8, 8, 64)     0           max_pooling2d_17[0][0]           \n",
            "                                                                 conv2d_493[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_487 (Conv2D)             (None, 16, 16, 64)   1024        activation_492[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_478 (BatchN (None, 8, 8, 64)     256         add_183[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_181 (Add)                   (None, 16, 16, 64)   0           add_180[0][0]                    \n",
            "                                                                 conv2d_487[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_499 (Activation)     (None, 8, 8, 64)     0           batch_normalization_478[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_472 (BatchN (None, 16, 16, 64)   256         add_181[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_494 (Conv2D)             (None, 8, 8, 16)     1024        activation_499[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_493 (Activation)     (None, 16, 16, 64)   0           batch_normalization_472[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_479 (BatchN (None, 8, 8, 16)     64          conv2d_494[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_488 (Conv2D)             (None, 16, 16, 16)   1024        activation_493[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_500 (Activation)     (None, 8, 8, 16)     0           batch_normalization_479[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_473 (BatchN (None, 16, 16, 16)   64          conv2d_488[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_495 (Conv2D)             (None, 8, 8, 16)     2320        activation_500[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_494 (Activation)     (None, 16, 16, 16)   0           batch_normalization_473[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_480 (BatchN (None, 8, 8, 16)     64          conv2d_495[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_489 (Conv2D)             (None, 16, 16, 16)   2320        activation_494[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_501 (Activation)     (None, 8, 8, 16)     0           batch_normalization_480[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_474 (BatchN (None, 16, 16, 16)   64          conv2d_489[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_496 (Conv2D)             (None, 8, 8, 64)     1024        activation_501[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_495 (Activation)     (None, 16, 16, 16)   0           batch_normalization_474[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_184 (Add)                   (None, 8, 8, 64)     0           add_183[0][0]                    \n",
            "                                                                 conv2d_496[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_490 (Conv2D)             (None, 16, 16, 64)   1024        activation_495[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_17 (UpSampling2D) (None, 16, 16, 64)   0           add_184[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_182 (Add)                   (None, 16, 16, 64)   0           add_181[0][0]                    \n",
            "                                                                 conv2d_490[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_185 (Add)                   (None, 16, 16, 64)   0           up_sampling2d_17[0][0]           \n",
            "                                                                 add_182[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_481 (BatchN (None, 16, 16, 64)   256         add_185[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_502 (Activation)     (None, 16, 16, 64)   0           batch_normalization_481[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_497 (Conv2D)             (None, 16, 16, 64)   4096        activation_502[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_482 (BatchN (None, 16, 16, 64)   256         conv2d_497[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_503 (Activation)     (None, 16, 16, 64)   0           batch_normalization_482[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_498 (Conv2D)             (None, 16, 16, 64)   4096        activation_503[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_504 (Activation)     (None, 16, 16, 64)   0           conv2d_498[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_16 (TensorFlo [(4,)]               0           activation_504[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_16 (TensorFlow [(None, 16, 16, 64)] 0           tf_op_layer_Shape_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_186 (Add)                   (None, 16, 16, 64)   0           tf_op_layer_Fill_16[0][0]        \n",
            "                                                                 activation_504[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_16 (Multiply)          (None, 16, 16, 64)   0           add_186[0][0]                    \n",
            "                                                                 add_182[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_483 (BatchN (None, 16, 16, 64)   256         multiply_16[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_505 (Activation)     (None, 16, 16, 64)   0           batch_normalization_483[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_499 (Conv2D)             (None, 16, 16, 16)   1024        activation_505[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_484 (BatchN (None, 16, 16, 16)   64          conv2d_499[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_506 (Activation)     (None, 16, 16, 16)   0           batch_normalization_484[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_500 (Conv2D)             (None, 16, 16, 16)   2320        activation_506[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_485 (BatchN (None, 16, 16, 16)   64          conv2d_500[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_507 (Activation)     (None, 16, 16, 16)   0           batch_normalization_485[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_501 (Conv2D)             (None, 16, 16, 64)   1024        activation_507[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_187 (Add)                   (None, 16, 16, 64)   0           multiply_16[0][0]                \n",
            "                                                                 conv2d_501[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_486 (BatchN (None, 16, 16, 64)   256         add_187[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_508 (Activation)     (None, 16, 16, 64)   0           batch_normalization_486[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_502 (Conv2D)             (None, 16, 16, 32)   2048        activation_508[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_487 (BatchN (None, 16, 16, 32)   128         conv2d_502[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_509 (Activation)     (None, 16, 16, 32)   0           batch_normalization_487[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_503 (Conv2D)             (None, 8, 8, 32)     9248        activation_509[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_488 (BatchN (None, 8, 8, 32)     128         conv2d_503[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_510 (Activation)     (None, 8, 8, 32)     0           batch_normalization_488[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_505 (Conv2D)             (None, 8, 8, 128)    8192        activation_508[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_504 (Conv2D)             (None, 8, 8, 128)    4096        activation_510[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_188 (Add)                   (None, 8, 8, 128)    0           conv2d_505[0][0]                 \n",
            "                                                                 conv2d_504[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_489 (BatchN (None, 8, 8, 128)    512         add_188[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_511 (Activation)     (None, 8, 8, 128)    0           batch_normalization_489[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_506 (Conv2D)             (None, 8, 8, 32)     4096        activation_511[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_490 (BatchN (None, 8, 8, 32)     128         conv2d_506[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_512 (Activation)     (None, 8, 8, 32)     0           batch_normalization_490[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_507 (Conv2D)             (None, 8, 8, 32)     9248        activation_512[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_491 (BatchN (None, 8, 8, 32)     128         conv2d_507[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_513 (Activation)     (None, 8, 8, 32)     0           batch_normalization_491[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_508 (Conv2D)             (None, 8, 8, 128)    4096        activation_513[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_189 (Add)                   (None, 8, 8, 128)    0           add_188[0][0]                    \n",
            "                                                                 conv2d_508[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_498 (BatchN (None, 8, 8, 128)    512         add_189[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_520 (Activation)     (None, 8, 8, 128)    0           batch_normalization_498[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_515 (Conv2D)             (None, 8, 8, 32)     4096        activation_520[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_499 (BatchN (None, 8, 8, 32)     128         conv2d_515[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_521 (Activation)     (None, 8, 8, 32)     0           batch_normalization_499[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_516 (Conv2D)             (None, 8, 8, 32)     9248        activation_521[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_500 (BatchN (None, 8, 8, 32)     128         conv2d_516[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_522 (Activation)     (None, 8, 8, 32)     0           batch_normalization_500[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_517 (Conv2D)             (None, 8, 8, 128)    4096        activation_522[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_192 (Add)                   (None, 8, 8, 128)    0           add_189[0][0]                    \n",
            "                                                                 conv2d_517[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_501 (BatchN (None, 8, 8, 128)    512         add_192[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_492 (BatchN (None, 8, 8, 128)    512         add_189[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_523 (Activation)     (None, 8, 8, 128)    0           batch_normalization_501[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_514 (Activation)     (None, 8, 8, 128)    0           batch_normalization_492[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_518 (Conv2D)             (None, 8, 8, 32)     4096        activation_523[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_509 (Conv2D)             (None, 8, 8, 32)     4096        activation_514[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_502 (BatchN (None, 8, 8, 32)     128         conv2d_518[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_493 (BatchN (None, 8, 8, 32)     128         conv2d_509[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_524 (Activation)     (None, 8, 8, 32)     0           batch_normalization_502[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_515 (Activation)     (None, 8, 8, 32)     0           batch_normalization_493[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_519 (Conv2D)             (None, 8, 8, 32)     9248        activation_524[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_510 (Conv2D)             (None, 8, 8, 32)     9248        activation_515[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_503 (BatchN (None, 8, 8, 32)     128         conv2d_519[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_494 (BatchN (None, 8, 8, 32)     128         conv2d_510[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_525 (Activation)     (None, 8, 8, 32)     0           batch_normalization_503[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_516 (Activation)     (None, 8, 8, 32)     0           batch_normalization_494[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_520 (Conv2D)             (None, 8, 8, 128)    4096        activation_525[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_511 (Conv2D)             (None, 8, 8, 128)    4096        activation_516[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_193 (Add)                   (None, 8, 8, 128)    0           add_192[0][0]                    \n",
            "                                                                 conv2d_520[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_190 (Add)                   (None, 8, 8, 128)    0           add_189[0][0]                    \n",
            "                                                                 conv2d_511[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_504 (BatchN (None, 8, 8, 128)    512         add_193[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_495 (BatchN (None, 8, 8, 128)    512         add_190[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_526 (Activation)     (None, 8, 8, 128)    0           batch_normalization_504[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_517 (Activation)     (None, 8, 8, 128)    0           batch_normalization_495[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_521 (Conv2D)             (None, 8, 8, 128)    16384       activation_526[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_512 (Conv2D)             (None, 8, 8, 32)     4096        activation_517[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_505 (BatchN (None, 8, 8, 128)    512         conv2d_521[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_496 (BatchN (None, 8, 8, 32)     128         conv2d_512[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_527 (Activation)     (None, 8, 8, 128)    0           batch_normalization_505[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_518 (Activation)     (None, 8, 8, 32)     0           batch_normalization_496[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_522 (Conv2D)             (None, 8, 8, 128)    16384       activation_527[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_513 (Conv2D)             (None, 8, 8, 32)     9248        activation_518[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_528 (Activation)     (None, 8, 8, 128)    0           conv2d_522[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_497 (BatchN (None, 8, 8, 32)     128         conv2d_513[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_17 (TensorFlo [(4,)]               0           activation_528[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_519 (Activation)     (None, 8, 8, 32)     0           batch_normalization_497[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_17 (TensorFlow [(None, 8, 8, 128)]  0           tf_op_layer_Shape_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_514 (Conv2D)             (None, 8, 8, 128)    4096        activation_519[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_194 (Add)                   (None, 8, 8, 128)    0           tf_op_layer_Fill_17[0][0]        \n",
            "                                                                 activation_528[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_191 (Add)                   (None, 8, 8, 128)    0           add_190[0][0]                    \n",
            "                                                                 conv2d_514[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "multiply_17 (Multiply)          (None, 8, 8, 128)    0           add_194[0][0]                    \n",
            "                                                                 add_191[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_506 (BatchN (None, 8, 8, 128)    512         multiply_17[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_529 (Activation)     (None, 8, 8, 128)    0           batch_normalization_506[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_523 (Conv2D)             (None, 8, 8, 32)     4096        activation_529[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_507 (BatchN (None, 8, 8, 32)     128         conv2d_523[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_530 (Activation)     (None, 8, 8, 32)     0           batch_normalization_507[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_524 (Conv2D)             (None, 8, 8, 32)     9248        activation_530[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_508 (BatchN (None, 8, 8, 32)     128         conv2d_524[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_531 (Activation)     (None, 8, 8, 32)     0           batch_normalization_508[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_525 (Conv2D)             (None, 8, 8, 128)    4096        activation_531[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_195 (Add)                   (None, 8, 8, 128)    0           multiply_17[0][0]                \n",
            "                                                                 conv2d_525[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_509 (BatchN (None, 8, 8, 128)    512         add_195[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_532 (Activation)     (None, 8, 8, 128)    0           batch_normalization_509[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_526 (Conv2D)             (None, 8, 8, 64)     8192        activation_532[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_510 (BatchN (None, 8, 8, 64)     256         conv2d_526[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_533 (Activation)     (None, 8, 8, 64)     0           batch_normalization_510[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_527 (Conv2D)             (None, 8, 8, 64)     36928       activation_533[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_511 (BatchN (None, 8, 8, 64)     256         conv2d_527[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_534 (Activation)     (None, 8, 8, 64)     0           batch_normalization_511[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_529 (Conv2D)             (None, 8, 8, 256)    32768       activation_532[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_528 (Conv2D)             (None, 8, 8, 256)    16384       activation_534[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_196 (Add)                   (None, 8, 8, 256)    0           conv2d_529[0][0]                 \n",
            "                                                                 conv2d_528[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_512 (BatchN (None, 8, 8, 256)    1024        add_196[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_535 (Activation)     (None, 8, 8, 256)    0           batch_normalization_512[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_530 (Conv2D)             (None, 8, 8, 64)     16384       activation_535[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_513 (BatchN (None, 8, 8, 64)     256         conv2d_530[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_536 (Activation)     (None, 8, 8, 64)     0           batch_normalization_513[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_531 (Conv2D)             (None, 8, 8, 64)     36928       activation_536[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_514 (BatchN (None, 8, 8, 64)     256         conv2d_531[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_537 (Activation)     (None, 8, 8, 64)     0           batch_normalization_514[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_532 (Conv2D)             (None, 8, 8, 256)    16384       activation_537[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_197 (Add)                   (None, 8, 8, 256)    0           add_196[0][0]                    \n",
            "                                                                 conv2d_532[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_515 (BatchN (None, 8, 8, 256)    1024        add_197[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_538 (Activation)     (None, 8, 8, 256)    0           batch_normalization_515[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, 8, 8, 256)    0           activation_538[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 1, 1, 256)    0           zero_padding2d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_533 (Conv2D)             (None, 1, 1, 10)     2570        average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 10)           0           conv2d_533[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 10)           0           flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_539 (Activation)     (None, 10)           0           dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 396,810\n",
            "Trainable params: 389,258\n",
            "Non-trainable params: 7,552\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbhILzMPtAMH",
        "colab_type": "code",
        "outputId": "d26c7d5a-f910-4ec0-cb67-93e4d67515bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1/2\n",
        "history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                                      epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                      validation_data=(x_test, y_test), callbacks=[cp_callback, warm_up_lr])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-1c6c9a76ac87>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 2.2242 - accuracy: 0.1669\n",
            "Epoch 00001: saving model to /content\n",
            "390/390 [==============================] - 22s 55ms/step - loss: 2.2235 - accuracy: 0.1674 - val_loss: 2.3660 - val_accuracy: 0.1358\n",
            "Epoch 2/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9598 - accuracy: 0.2896\n",
            "Epoch 00002: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.9598 - accuracy: 0.2896 - val_loss: 1.7530 - val_accuracy: 0.3718\n",
            "Epoch 3/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.7735 - accuracy: 0.3567\n",
            "Epoch 00003: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.7733 - accuracy: 0.3570 - val_loss: 1.6825 - val_accuracy: 0.3914\n",
            "Epoch 4/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.6814 - accuracy: 0.3962\n",
            "Epoch 00004: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.6814 - accuracy: 0.3962 - val_loss: 1.5697 - val_accuracy: 0.4293\n",
            "Epoch 5/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.6145 - accuracy: 0.4183\n",
            "Epoch 00005: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.6145 - accuracy: 0.4183 - val_loss: 1.5770 - val_accuracy: 0.4559\n",
            "Epoch 6/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.5484 - accuracy: 0.4418\n",
            "Epoch 00006: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 1.5484 - accuracy: 0.4418 - val_loss: 1.5464 - val_accuracy: 0.4737\n",
            "Epoch 7/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4962 - accuracy: 0.4593\n",
            "Epoch 00007: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.4956 - accuracy: 0.4595 - val_loss: 1.8120 - val_accuracy: 0.4026\n",
            "Epoch 8/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4286 - accuracy: 0.4832\n",
            "Epoch 00008: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 1.4290 - accuracy: 0.4830 - val_loss: 1.6639 - val_accuracy: 0.4318\n",
            "Epoch 9/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.3670 - accuracy: 0.5047\n",
            "Epoch 00009: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.3670 - accuracy: 0.5047 - val_loss: 1.4248 - val_accuracy: 0.4995\n",
            "Epoch 10/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3261 - accuracy: 0.5195\n",
            "Epoch 00010: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.3263 - accuracy: 0.5194 - val_loss: 1.2156 - val_accuracy: 0.5670\n",
            "Epoch 11/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2714 - accuracy: 0.5423\n",
            "Epoch 00011: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.2716 - accuracy: 0.5423 - val_loss: 1.3914 - val_accuracy: 0.5243\n",
            "Epoch 12/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2170 - accuracy: 0.5609\n",
            "Epoch 00012: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.2171 - accuracy: 0.5609 - val_loss: 1.9260 - val_accuracy: 0.4783\n",
            "Epoch 13/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.1745 - accuracy: 0.5755\n",
            "Epoch 00013: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.1745 - accuracy: 0.5755 - val_loss: 1.1454 - val_accuracy: 0.6136\n",
            "Epoch 14/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.1215 - accuracy: 0.5946\n",
            "Epoch 00014: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 1.1215 - accuracy: 0.5946 - val_loss: 1.1045 - val_accuracy: 0.6149\n",
            "Epoch 15/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0849 - accuracy: 0.6103\n",
            "Epoch 00015: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 1.0850 - accuracy: 0.6102 - val_loss: 1.1306 - val_accuracy: 0.6156\n",
            "Epoch 16/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0595 - accuracy: 0.6119\n",
            "Epoch 00016: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 1.0595 - accuracy: 0.6119 - val_loss: 0.9204 - val_accuracy: 0.6840\n",
            "Epoch 17/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0315 - accuracy: 0.6272\n",
            "Epoch 00017: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.0318 - accuracy: 0.6271 - val_loss: 0.9387 - val_accuracy: 0.6855\n",
            "Epoch 18/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9982 - accuracy: 0.6371\n",
            "Epoch 00018: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.9978 - accuracy: 0.6373 - val_loss: 0.8495 - val_accuracy: 0.7079\n",
            "Epoch 19/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9778 - accuracy: 0.6420\n",
            "Epoch 00019: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.9777 - accuracy: 0.6420 - val_loss: 1.0228 - val_accuracy: 0.6542\n",
            "Epoch 20/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9548 - accuracy: 0.6528\n",
            "Epoch 00020: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.9548 - accuracy: 0.6528 - val_loss: 1.0667 - val_accuracy: 0.6500\n",
            "Epoch 21/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9378 - accuracy: 0.6601\n",
            "Epoch 00021: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.9377 - accuracy: 0.6602 - val_loss: 0.9750 - val_accuracy: 0.6831\n",
            "Epoch 22/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9211 - accuracy: 0.6644\n",
            "Epoch 00022: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.9216 - accuracy: 0.6643 - val_loss: 0.7382 - val_accuracy: 0.7455\n",
            "Epoch 23/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9104 - accuracy: 0.6675\n",
            "Epoch 00023: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.9102 - accuracy: 0.6673 - val_loss: 0.8159 - val_accuracy: 0.7385\n",
            "Epoch 24/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8786 - accuracy: 0.6758\n",
            "Epoch 00024: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.8786 - accuracy: 0.6758 - val_loss: 0.7552 - val_accuracy: 0.7484\n",
            "Epoch 25/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8655 - accuracy: 0.6811\n",
            "Epoch 00025: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.8655 - accuracy: 0.6811 - val_loss: 1.0502 - val_accuracy: 0.6746\n",
            "Epoch 26/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8591 - accuracy: 0.6813\n",
            "Epoch 00026: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.8591 - accuracy: 0.6813 - val_loss: 0.9610 - val_accuracy: 0.6952\n",
            "Epoch 27/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8384 - accuracy: 0.6893\n",
            "Epoch 00027: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.8384 - accuracy: 0.6894 - val_loss: 0.7866 - val_accuracy: 0.7321\n",
            "Epoch 28/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8286 - accuracy: 0.6924\n",
            "Epoch 00028: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.8288 - accuracy: 0.6923 - val_loss: 0.7473 - val_accuracy: 0.7517\n",
            "Epoch 29/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8189 - accuracy: 0.6962\n",
            "Epoch 00029: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.8188 - accuracy: 0.6961 - val_loss: 0.8711 - val_accuracy: 0.7350\n",
            "Epoch 30/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8146 - accuracy: 0.6961\n",
            "Epoch 00030: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.8146 - accuracy: 0.6961 - val_loss: 0.8155 - val_accuracy: 0.7506\n",
            "Epoch 31/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7862 - accuracy: 0.7028\n",
            "Epoch 00031: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7855 - accuracy: 0.7031 - val_loss: 0.8449 - val_accuracy: 0.7274\n",
            "Epoch 32/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7823 - accuracy: 0.7071\n",
            "Epoch 00032: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7823 - accuracy: 0.7071 - val_loss: 0.6493 - val_accuracy: 0.7884\n",
            "Epoch 33/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7747 - accuracy: 0.7101\n",
            "Epoch 00033: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7747 - accuracy: 0.7101 - val_loss: 0.6953 - val_accuracy: 0.7758\n",
            "Epoch 34/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7635 - accuracy: 0.7159\n",
            "Epoch 00034: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7634 - accuracy: 0.7160 - val_loss: 0.5582 - val_accuracy: 0.8131\n",
            "Epoch 35/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7494 - accuracy: 0.7167\n",
            "Epoch 00035: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7494 - accuracy: 0.7167 - val_loss: 0.7130 - val_accuracy: 0.7716\n",
            "Epoch 36/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7465 - accuracy: 0.7192\n",
            "Epoch 00036: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7463 - accuracy: 0.7194 - val_loss: 0.6028 - val_accuracy: 0.7995\n",
            "Epoch 37/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7321 - accuracy: 0.7247\n",
            "Epoch 00037: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7321 - accuracy: 0.7247 - val_loss: 0.5732 - val_accuracy: 0.8141\n",
            "Epoch 38/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7148 - accuracy: 0.7300\n",
            "Epoch 00038: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7149 - accuracy: 0.7299 - val_loss: 0.6813 - val_accuracy: 0.7876\n",
            "Epoch 39/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7125 - accuracy: 0.7308\n",
            "Epoch 00039: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7125 - accuracy: 0.7308 - val_loss: 0.6184 - val_accuracy: 0.7990\n",
            "Epoch 40/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7045 - accuracy: 0.7333\n",
            "Epoch 00040: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7045 - accuracy: 0.7331 - val_loss: 0.7483 - val_accuracy: 0.7749\n",
            "Epoch 41/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7051 - accuracy: 0.7319\n",
            "Epoch 00041: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.7052 - accuracy: 0.7320 - val_loss: 0.5334 - val_accuracy: 0.8260\n",
            "Epoch 42/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6797 - accuracy: 0.7390\n",
            "Epoch 00042: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6797 - accuracy: 0.7390 - val_loss: 0.7220 - val_accuracy: 0.7720\n",
            "Epoch 43/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6753 - accuracy: 0.7416\n",
            "Epoch 00043: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6750 - accuracy: 0.7416 - val_loss: 0.6263 - val_accuracy: 0.7931\n",
            "Epoch 44/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6738 - accuracy: 0.7416\n",
            "Epoch 00044: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6738 - accuracy: 0.7416 - val_loss: 0.7215 - val_accuracy: 0.7738\n",
            "Epoch 45/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.7455\n",
            "Epoch 00045: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6553 - accuracy: 0.7455 - val_loss: 0.6070 - val_accuracy: 0.8055\n",
            "Epoch 46/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6521 - accuracy: 0.7474\n",
            "Epoch 00046: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6521 - accuracy: 0.7474 - val_loss: 0.6131 - val_accuracy: 0.8060\n",
            "Epoch 47/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.7571\n",
            "Epoch 00047: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6311 - accuracy: 0.7571 - val_loss: 0.5868 - val_accuracy: 0.8165\n",
            "Epoch 48/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6379 - accuracy: 0.7502\n",
            "Epoch 00048: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6379 - accuracy: 0.7502 - val_loss: 0.5644 - val_accuracy: 0.8241\n",
            "Epoch 49/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.7542\n",
            "Epoch 00049: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6263 - accuracy: 0.7542 - val_loss: 0.5622 - val_accuracy: 0.8230\n",
            "Epoch 50/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.7580\n",
            "Epoch 00050: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6200 - accuracy: 0.7580 - val_loss: 0.5690 - val_accuracy: 0.8207\n",
            "Epoch 51/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.7578\n",
            "Epoch 00051: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6169 - accuracy: 0.7578 - val_loss: 0.6121 - val_accuracy: 0.8069\n",
            "Epoch 52/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6092 - accuracy: 0.7644\n",
            "Epoch 00052: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6090 - accuracy: 0.7644 - val_loss: 0.5875 - val_accuracy: 0.8121\n",
            "Epoch 53/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6031 - accuracy: 0.7624\n",
            "Epoch 00053: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6031 - accuracy: 0.7624 - val_loss: 0.5940 - val_accuracy: 0.8131\n",
            "Epoch 54/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6010 - accuracy: 0.7592\n",
            "Epoch 00054: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.6006 - accuracy: 0.7593 - val_loss: 0.5660 - val_accuracy: 0.8276\n",
            "Epoch 55/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5872 - accuracy: 0.7696\n",
            "Epoch 00055: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5872 - accuracy: 0.7696 - val_loss: 0.5363 - val_accuracy: 0.8283\n",
            "Epoch 56/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7737\n",
            "Epoch 00056: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5719 - accuracy: 0.7737 - val_loss: 0.5514 - val_accuracy: 0.8280\n",
            "Epoch 57/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5768 - accuracy: 0.7693\n",
            "Epoch 00057: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5768 - accuracy: 0.7693 - val_loss: 0.5158 - val_accuracy: 0.8407\n",
            "Epoch 58/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5727 - accuracy: 0.7709\n",
            "Epoch 00058: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.5727 - accuracy: 0.7709 - val_loss: 0.5325 - val_accuracy: 0.8363\n",
            "Epoch 59/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5502 - accuracy: 0.7804\n",
            "Epoch 00059: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.5502 - accuracy: 0.7804 - val_loss: 0.5437 - val_accuracy: 0.8378\n",
            "Epoch 60/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5517 - accuracy: 0.7784\n",
            "Epoch 00060: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.5517 - accuracy: 0.7784 - val_loss: 0.5750 - val_accuracy: 0.8296\n",
            "Epoch 61/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.7797\n",
            "Epoch 00061: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5525 - accuracy: 0.7797 - val_loss: 0.4785 - val_accuracy: 0.8489\n",
            "Epoch 62/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7803\n",
            "Epoch 00062: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5405 - accuracy: 0.7802 - val_loss: 0.5319 - val_accuracy: 0.8365\n",
            "Epoch 63/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5281 - accuracy: 0.7883\n",
            "Epoch 00063: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5283 - accuracy: 0.7882 - val_loss: 0.5208 - val_accuracy: 0.8428\n",
            "Epoch 64/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5277 - accuracy: 0.7859\n",
            "Epoch 00064: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5282 - accuracy: 0.7859 - val_loss: 0.5225 - val_accuracy: 0.8434\n",
            "Epoch 65/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.7861\n",
            "Epoch 00065: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5286 - accuracy: 0.7861 - val_loss: 0.5092 - val_accuracy: 0.8439\n",
            "Epoch 66/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5246 - accuracy: 0.7881\n",
            "Epoch 00066: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5247 - accuracy: 0.7881 - val_loss: 0.4983 - val_accuracy: 0.8546\n",
            "Epoch 67/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5152 - accuracy: 0.7890\n",
            "Epoch 00067: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5152 - accuracy: 0.7890 - val_loss: 0.4671 - val_accuracy: 0.8533\n",
            "Epoch 68/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5074 - accuracy: 0.7906\n",
            "Epoch 00068: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5070 - accuracy: 0.7907 - val_loss: 0.5091 - val_accuracy: 0.8458\n",
            "Epoch 69/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.7934\n",
            "Epoch 00069: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5120 - accuracy: 0.7934 - val_loss: 0.4791 - val_accuracy: 0.8515\n",
            "Epoch 70/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.7992\n",
            "Epoch 00070: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4902 - accuracy: 0.7992 - val_loss: 0.4912 - val_accuracy: 0.8503\n",
            "Epoch 71/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5047 - accuracy: 0.7898\n",
            "Epoch 00071: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.5049 - accuracy: 0.7896 - val_loss: 0.5065 - val_accuracy: 0.8480\n",
            "Epoch 72/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4802 - accuracy: 0.8010\n",
            "Epoch 00072: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4802 - accuracy: 0.8010 - val_loss: 0.5594 - val_accuracy: 0.8379\n",
            "Epoch 73/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4771 - accuracy: 0.8019\n",
            "Epoch 00073: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4771 - accuracy: 0.8020 - val_loss: 0.4884 - val_accuracy: 0.8540\n",
            "Epoch 74/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.8002\n",
            "Epoch 00074: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4797 - accuracy: 0.8002 - val_loss: 0.4806 - val_accuracy: 0.8562\n",
            "Epoch 75/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4743 - accuracy: 0.8038\n",
            "Epoch 00075: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4745 - accuracy: 0.8037 - val_loss: 0.5358 - val_accuracy: 0.8488\n",
            "Epoch 76/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4667 - accuracy: 0.8049\n",
            "Epoch 00076: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4667 - accuracy: 0.8049 - val_loss: 0.4813 - val_accuracy: 0.8580\n",
            "Epoch 77/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4617 - accuracy: 0.8046\n",
            "Epoch 00077: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4619 - accuracy: 0.8044 - val_loss: 0.5056 - val_accuracy: 0.8564\n",
            "Epoch 78/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4594 - accuracy: 0.8070\n",
            "Epoch 00078: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4594 - accuracy: 0.8070 - val_loss: 0.4918 - val_accuracy: 0.8575\n",
            "Epoch 79/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4625 - accuracy: 0.8068\n",
            "Epoch 00079: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4620 - accuracy: 0.8070 - val_loss: 0.4831 - val_accuracy: 0.8590\n",
            "Epoch 80/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.8075\n",
            "Epoch 00080: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4537 - accuracy: 0.8075 - val_loss: 0.4785 - val_accuracy: 0.8596\n",
            "Epoch 81/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4528 - accuracy: 0.8095\n",
            "Epoch 00081: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4528 - accuracy: 0.8095 - val_loss: 0.4766 - val_accuracy: 0.8622\n",
            "Epoch 82/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4526 - accuracy: 0.8113\n",
            "Epoch 00082: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4525 - accuracy: 0.8114 - val_loss: 0.4931 - val_accuracy: 0.8574\n",
            "Epoch 83/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4583 - accuracy: 0.8071\n",
            "Epoch 00083: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4583 - accuracy: 0.8071 - val_loss: 0.4836 - val_accuracy: 0.8622\n",
            "Epoch 84/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4468 - accuracy: 0.8115\n",
            "Epoch 00084: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4468 - accuracy: 0.8114 - val_loss: 0.4913 - val_accuracy: 0.8617\n",
            "Epoch 85/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4464 - accuracy: 0.8097\n",
            "Epoch 00085: saving model to /content\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.4464 - accuracy: 0.8097 - val_loss: 0.4692 - val_accuracy: 0.8642\n",
            "Epoch 86/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.8096\n",
            "Epoch 00086: saving model to /content\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.4461 - accuracy: 0.8096 - val_loss: 0.4883 - val_accuracy: 0.8605\n",
            "Epoch 87/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4396 - accuracy: 0.8147\n",
            "Epoch 00087: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4393 - accuracy: 0.8149 - val_loss: 0.4735 - val_accuracy: 0.8648\n",
            "Epoch 88/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.8128\n",
            "Epoch 00088: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4431 - accuracy: 0.8128 - val_loss: 0.4725 - val_accuracy: 0.8645\n",
            "Epoch 89/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4414 - accuracy: 0.8127\n",
            "Epoch 00089: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4415 - accuracy: 0.8126 - val_loss: 0.4696 - val_accuracy: 0.8683\n",
            "Epoch 90/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4404 - accuracy: 0.8096\n",
            "Epoch 00090: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4408 - accuracy: 0.8096 - val_loss: 0.4705 - val_accuracy: 0.8660\n",
            "Epoch 91/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4385 - accuracy: 0.8122\n",
            "Epoch 00091: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4386 - accuracy: 0.8121 - val_loss: 0.4649 - val_accuracy: 0.8671\n",
            "Epoch 92/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4360 - accuracy: 0.8151\n",
            "Epoch 00092: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4360 - accuracy: 0.8151 - val_loss: 0.4751 - val_accuracy: 0.8646\n",
            "Epoch 93/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4307 - accuracy: 0.8171\n",
            "Epoch 00093: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4307 - accuracy: 0.8171 - val_loss: 0.4683 - val_accuracy: 0.8674\n",
            "Epoch 94/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4365 - accuracy: 0.8149\n",
            "Epoch 00094: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4365 - accuracy: 0.8149 - val_loss: 0.4705 - val_accuracy: 0.8681\n",
            "Epoch 95/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4314 - accuracy: 0.8154\n",
            "Epoch 00095: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4317 - accuracy: 0.8153 - val_loss: 0.4692 - val_accuracy: 0.8670\n",
            "Epoch 96/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4333 - accuracy: 0.8139\n",
            "Epoch 00096: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4333 - accuracy: 0.8139 - val_loss: 0.4701 - val_accuracy: 0.8669\n",
            "Epoch 97/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4281 - accuracy: 0.8187\n",
            "Epoch 00097: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4283 - accuracy: 0.8187 - val_loss: 0.4697 - val_accuracy: 0.8676\n",
            "Epoch 98/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4247 - accuracy: 0.8215\n",
            "Epoch 00098: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4247 - accuracy: 0.8215 - val_loss: 0.4710 - val_accuracy: 0.8667\n",
            "Epoch 99/100\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4281 - accuracy: 0.8178\n",
            "Epoch 00099: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4277 - accuracy: 0.8180 - val_loss: 0.4692 - val_accuracy: 0.8672\n",
            "Epoch 100/100\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4343 - accuracy: 0.8141\n",
            "Epoch 00100: saving model to /content\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.4343 - accuracy: 0.8141 - val_loss: 0.4711 - val_accuracy: 0.8675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8VTcjIwtnUH",
        "colab_type": "code",
        "outputId": "fce78b87-e1ae-4c06-945a-878cd5521848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "red_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3719 - accuracy: 0.8908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37192878127098083, 0.8907999992370605]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMBikhv7B-qB",
        "colab_type": "code",
        "outputId": "4e87027e-f7e6-45c4-f8b7-e2a7689de168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "red_model.evaluate(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1012 - accuracy: 0.9643\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1011546328663826, 0.9643250107765198]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5kMDdHVC_99",
        "colab_type": "code",
        "outputId": "5bd28271-b68b-459a-b19b-1ff4a5d1aae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1/4\n",
        "history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                                      epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                      validation_data=(x_test, y_test), callbacks=[cp_callback, warm_up_lr])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 2.3477 - accuracy: 0.1243\n",
            "Epoch 00001: saving model to /content\n",
            "195/195 [==============================] - 13s 66ms/step - loss: 2.3467 - accuracy: 0.1250 - val_loss: 2.3969 - val_accuracy: 0.1000\n",
            "Epoch 2/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 2.0571 - accuracy: 0.2468\n",
            "Epoch 00002: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 2.0562 - accuracy: 0.2472 - val_loss: 2.3732 - val_accuracy: 0.1404\n",
            "Epoch 3/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.9121 - accuracy: 0.3028\n",
            "Epoch 00003: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.9121 - accuracy: 0.3028 - val_loss: 1.8216 - val_accuracy: 0.3202\n",
            "Epoch 4/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.8289 - accuracy: 0.3424\n",
            "Epoch 00004: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.8289 - accuracy: 0.3424 - val_loss: 1.6833 - val_accuracy: 0.3799\n",
            "Epoch 5/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.7594 - accuracy: 0.3589\n",
            "Epoch 00005: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 1.7594 - accuracy: 0.3589 - val_loss: 1.7534 - val_accuracy: 0.3790\n",
            "Epoch 6/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.7060 - accuracy: 0.3822\n",
            "Epoch 00006: saving model to /content\n",
            "195/195 [==============================] - 12s 62ms/step - loss: 1.7060 - accuracy: 0.3822 - val_loss: 1.5973 - val_accuracy: 0.4200\n",
            "Epoch 7/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.6792 - accuracy: 0.3969\n",
            "Epoch 00007: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 1.6784 - accuracy: 0.3972 - val_loss: 1.6149 - val_accuracy: 0.4209\n",
            "Epoch 8/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.6242 - accuracy: 0.4214\n",
            "Epoch 00008: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.6237 - accuracy: 0.4215 - val_loss: 3.3947 - val_accuracy: 0.1895\n",
            "Epoch 9/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.6048 - accuracy: 0.4251\n",
            "Epoch 00009: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 1.6048 - accuracy: 0.4251 - val_loss: 1.7056 - val_accuracy: 0.4032\n",
            "Epoch 10/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.5665 - accuracy: 0.4376\n",
            "Epoch 00010: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.5660 - accuracy: 0.4375 - val_loss: 2.0552 - val_accuracy: 0.3583\n",
            "Epoch 11/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.5334 - accuracy: 0.4497\n",
            "Epoch 00011: saving model to /content\n",
            "195/195 [==============================] - 12s 62ms/step - loss: 1.5321 - accuracy: 0.4501 - val_loss: 1.7286 - val_accuracy: 0.4141\n",
            "Epoch 12/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.4663 - accuracy: 0.4735\n",
            "Epoch 00012: saving model to /content\n",
            "195/195 [==============================] - 12s 62ms/step - loss: 1.4661 - accuracy: 0.4735 - val_loss: 1.7796 - val_accuracy: 0.4190\n",
            "Epoch 13/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.4451 - accuracy: 0.4833\n",
            "Epoch 00013: saving model to /content\n",
            "195/195 [==============================] - 12s 64ms/step - loss: 1.4451 - accuracy: 0.4833 - val_loss: 1.5617 - val_accuracy: 0.4519\n",
            "Epoch 14/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.3977 - accuracy: 0.4986\n",
            "Epoch 00014: saving model to /content\n",
            "195/195 [==============================] - 12s 62ms/step - loss: 1.3977 - accuracy: 0.4986 - val_loss: 1.3900 - val_accuracy: 0.5141\n",
            "Epoch 15/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.3624 - accuracy: 0.5091\n",
            "Epoch 00015: saving model to /content\n",
            "195/195 [==============================] - 13s 64ms/step - loss: 1.3624 - accuracy: 0.5091 - val_loss: 1.6024 - val_accuracy: 0.4793\n",
            "Epoch 16/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.3318 - accuracy: 0.5237\n",
            "Epoch 00016: saving model to /content\n",
            "195/195 [==============================] - 12s 64ms/step - loss: 1.3328 - accuracy: 0.5230 - val_loss: 1.3477 - val_accuracy: 0.5305\n",
            "Epoch 17/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.2902 - accuracy: 0.5389\n",
            "Epoch 00017: saving model to /content\n",
            "195/195 [==============================] - 12s 63ms/step - loss: 1.2891 - accuracy: 0.5396 - val_loss: 1.2007 - val_accuracy: 0.5814\n",
            "Epoch 18/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.2576 - accuracy: 0.5483\n",
            "Epoch 00018: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 1.2576 - accuracy: 0.5483 - val_loss: 1.5264 - val_accuracy: 0.5007\n",
            "Epoch 19/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.2325 - accuracy: 0.5568\n",
            "Epoch 00019: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.2325 - accuracy: 0.5568 - val_loss: 1.4002 - val_accuracy: 0.5498\n",
            "Epoch 20/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.2155 - accuracy: 0.5626\n",
            "Epoch 00020: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.2155 - accuracy: 0.5626 - val_loss: 1.5305 - val_accuracy: 0.5059\n",
            "Epoch 21/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.1882 - accuracy: 0.5737\n",
            "Epoch 00021: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.1882 - accuracy: 0.5737 - val_loss: 1.2455 - val_accuracy: 0.5692\n",
            "Epoch 22/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.1612 - accuracy: 0.5789\n",
            "Epoch 00022: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 1.1612 - accuracy: 0.5789 - val_loss: 0.9827 - val_accuracy: 0.6532\n",
            "Epoch 23/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.1503 - accuracy: 0.5866\n",
            "Epoch 00023: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.1497 - accuracy: 0.5867 - val_loss: 1.1450 - val_accuracy: 0.6128\n",
            "Epoch 24/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.0961 - accuracy: 0.6067\n",
            "Epoch 00024: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.0961 - accuracy: 0.6067 - val_loss: 1.0201 - val_accuracy: 0.6524\n",
            "Epoch 25/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.0842 - accuracy: 0.6091\n",
            "Epoch 00025: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.0832 - accuracy: 0.6091 - val_loss: 0.9720 - val_accuracy: 0.6632\n",
            "Epoch 26/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.0760 - accuracy: 0.6120\n",
            "Epoch 00026: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.0772 - accuracy: 0.6115 - val_loss: 1.3712 - val_accuracy: 0.5769\n",
            "Epoch 27/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.0656 - accuracy: 0.6139\n",
            "Epoch 00027: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.0656 - accuracy: 0.6139 - val_loss: 0.9575 - val_accuracy: 0.6717\n",
            "Epoch 28/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.0438 - accuracy: 0.6221\n",
            "Epoch 00028: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 1.0426 - accuracy: 0.6225 - val_loss: 1.1877 - val_accuracy: 0.6166\n",
            "Epoch 29/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 1.0328 - accuracy: 0.6253\n",
            "Epoch 00029: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 1.0319 - accuracy: 0.6255 - val_loss: 1.2112 - val_accuracy: 0.6287\n",
            "Epoch 30/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 1.0025 - accuracy: 0.6345\n",
            "Epoch 00030: saving model to /content\n",
            "195/195 [==============================] - 12s 62ms/step - loss: 1.0025 - accuracy: 0.6345 - val_loss: 1.1904 - val_accuracy: 0.6228\n",
            "Epoch 31/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.9863 - accuracy: 0.6363\n",
            "Epoch 00031: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.9863 - accuracy: 0.6363 - val_loss: 1.4149 - val_accuracy: 0.5762\n",
            "Epoch 32/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.9857 - accuracy: 0.6347\n",
            "Epoch 00032: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 0.9857 - accuracy: 0.6347 - val_loss: 0.9175 - val_accuracy: 0.6889\n",
            "Epoch 33/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.9521 - accuracy: 0.6530\n",
            "Epoch 00033: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.9518 - accuracy: 0.6532 - val_loss: 0.8488 - val_accuracy: 0.7108\n",
            "Epoch 34/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.9335 - accuracy: 0.6568\n",
            "Epoch 00034: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.9335 - accuracy: 0.6568 - val_loss: 0.8320 - val_accuracy: 0.7209\n",
            "Epoch 35/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.9335 - accuracy: 0.6594\n",
            "Epoch 00035: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.9327 - accuracy: 0.6598 - val_loss: 0.8665 - val_accuracy: 0.7166\n",
            "Epoch 36/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.9210 - accuracy: 0.6604\n",
            "Epoch 00036: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.9210 - accuracy: 0.6606 - val_loss: 1.0205 - val_accuracy: 0.6701\n",
            "Epoch 37/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.9281 - accuracy: 0.6598\n",
            "Epoch 00037: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.9281 - accuracy: 0.6598 - val_loss: 0.8540 - val_accuracy: 0.7173\n",
            "Epoch 38/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.8792 - accuracy: 0.6725\n",
            "Epoch 00038: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 0.8786 - accuracy: 0.6730 - val_loss: 0.9523 - val_accuracy: 0.6917\n",
            "Epoch 39/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.8835 - accuracy: 0.6732\n",
            "Epoch 00039: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.8835 - accuracy: 0.6732 - val_loss: 0.8843 - val_accuracy: 0.7218\n",
            "Epoch 40/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.8915 - accuracy: 0.6718\n",
            "Epoch 00040: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.8915 - accuracy: 0.6718 - val_loss: 0.7763 - val_accuracy: 0.7388\n",
            "Epoch 41/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.8587 - accuracy: 0.6798\n",
            "Epoch 00041: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.8587 - accuracy: 0.6798 - val_loss: 0.8732 - val_accuracy: 0.7103\n",
            "Epoch 42/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.8519 - accuracy: 0.6836\n",
            "Epoch 00042: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.8526 - accuracy: 0.6834 - val_loss: 0.8191 - val_accuracy: 0.7280\n",
            "Epoch 43/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.8388 - accuracy: 0.6886\n",
            "Epoch 00043: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.8388 - accuracy: 0.6886 - val_loss: 1.1043 - val_accuracy: 0.6632\n",
            "Epoch 44/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.8293 - accuracy: 0.6907\n",
            "Epoch 00044: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.8293 - accuracy: 0.6907 - val_loss: 0.7706 - val_accuracy: 0.7432\n",
            "Epoch 45/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.8182 - accuracy: 0.6940\n",
            "Epoch 00045: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.8182 - accuracy: 0.6940 - val_loss: 0.7836 - val_accuracy: 0.7484\n",
            "Epoch 46/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.8027 - accuracy: 0.7043\n",
            "Epoch 00046: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.8025 - accuracy: 0.7044 - val_loss: 0.7214 - val_accuracy: 0.7615\n",
            "Epoch 47/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.7687 - accuracy: 0.7116\n",
            "Epoch 00047: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.7687 - accuracy: 0.7116 - val_loss: 0.8680 - val_accuracy: 0.7229\n",
            "Epoch 48/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.7774 - accuracy: 0.7072\n",
            "Epoch 00048: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 0.7778 - accuracy: 0.7067 - val_loss: 1.0285 - val_accuracy: 0.6846\n",
            "Epoch 49/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.7689 - accuracy: 0.7057\n",
            "Epoch 00049: saving model to /content\n",
            "195/195 [==============================] - 12s 62ms/step - loss: 0.7685 - accuracy: 0.7059 - val_loss: 0.9863 - val_accuracy: 0.7018\n",
            "Epoch 50/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.7545 - accuracy: 0.7158\n",
            "Epoch 00050: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.7542 - accuracy: 0.7161 - val_loss: 0.7210 - val_accuracy: 0.7660\n",
            "Epoch 51/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.7570 - accuracy: 0.7153\n",
            "Epoch 00051: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.7570 - accuracy: 0.7153 - val_loss: 0.7439 - val_accuracy: 0.7522\n",
            "Epoch 52/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.7535 - accuracy: 0.7161\n",
            "Epoch 00052: saving model to /content\n",
            "195/195 [==============================] - 12s 61ms/step - loss: 0.7537 - accuracy: 0.7162 - val_loss: 0.7985 - val_accuracy: 0.7507\n",
            "Epoch 53/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.7303 - accuracy: 0.7235\n",
            "Epoch 00053: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.7303 - accuracy: 0.7235 - val_loss: 0.6599 - val_accuracy: 0.7843\n",
            "Epoch 54/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.7226\n",
            "Epoch 00054: saving model to /content\n",
            "195/195 [==============================] - 12s 62ms/step - loss: 0.7279 - accuracy: 0.7226 - val_loss: 0.7180 - val_accuracy: 0.7596\n",
            "Epoch 55/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.7250\n",
            "Epoch 00055: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.7148 - accuracy: 0.7250 - val_loss: 0.7728 - val_accuracy: 0.7486\n",
            "Epoch 56/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.7168 - accuracy: 0.7262\n",
            "Epoch 00056: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.7168 - accuracy: 0.7262 - val_loss: 0.7662 - val_accuracy: 0.7589\n",
            "Epoch 57/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.6956 - accuracy: 0.7342\n",
            "Epoch 00057: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.6977 - accuracy: 0.7339 - val_loss: 0.6945 - val_accuracy: 0.7770\n",
            "Epoch 58/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.7396\n",
            "Epoch 00058: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.6789 - accuracy: 0.7396 - val_loss: 0.6824 - val_accuracy: 0.7762\n",
            "Epoch 59/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.6820 - accuracy: 0.7382\n",
            "Epoch 00059: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.6827 - accuracy: 0.7379 - val_loss: 0.7328 - val_accuracy: 0.7700\n",
            "Epoch 60/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.6639 - accuracy: 0.7491\n",
            "Epoch 00060: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.6639 - accuracy: 0.7491 - val_loss: 0.6390 - val_accuracy: 0.7906\n",
            "Epoch 61/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.6659 - accuracy: 0.7430\n",
            "Epoch 00061: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.6659 - accuracy: 0.7430 - val_loss: 0.7062 - val_accuracy: 0.7782\n",
            "Epoch 62/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.6532 - accuracy: 0.7485\n",
            "Epoch 00062: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.6541 - accuracy: 0.7480 - val_loss: 0.8048 - val_accuracy: 0.7546\n",
            "Epoch 63/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.6464 - accuracy: 0.7521\n",
            "Epoch 00063: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.6469 - accuracy: 0.7523 - val_loss: 0.7046 - val_accuracy: 0.7847\n",
            "Epoch 64/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.6373 - accuracy: 0.7507\n",
            "Epoch 00064: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.6370 - accuracy: 0.7511 - val_loss: 0.7416 - val_accuracy: 0.7710\n",
            "Epoch 65/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.6219 - accuracy: 0.7629\n",
            "Epoch 00065: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.6217 - accuracy: 0.7629 - val_loss: 0.7680 - val_accuracy: 0.7639\n",
            "Epoch 66/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.7576\n",
            "Epoch 00066: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.6219 - accuracy: 0.7576 - val_loss: 0.7417 - val_accuracy: 0.7746\n",
            "Epoch 67/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.6126 - accuracy: 0.7580\n",
            "Epoch 00067: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.6124 - accuracy: 0.7580 - val_loss: 0.6678 - val_accuracy: 0.7952\n",
            "Epoch 68/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.6158 - accuracy: 0.7566\n",
            "Epoch 00068: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.6175 - accuracy: 0.7560 - val_loss: 0.7053 - val_accuracy: 0.7848\n",
            "Epoch 69/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.7728\n",
            "Epoch 00069: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5877 - accuracy: 0.7728 - val_loss: 0.7924 - val_accuracy: 0.7590\n",
            "Epoch 70/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5966 - accuracy: 0.7658\n",
            "Epoch 00070: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.5966 - accuracy: 0.7658 - val_loss: 0.6797 - val_accuracy: 0.7925\n",
            "Epoch 71/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5829 - accuracy: 0.7724\n",
            "Epoch 00071: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5833 - accuracy: 0.7718 - val_loss: 0.7072 - val_accuracy: 0.7855\n",
            "Epoch 72/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5895 - accuracy: 0.7643\n",
            "Epoch 00072: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5883 - accuracy: 0.7647 - val_loss: 0.7320 - val_accuracy: 0.7883\n",
            "Epoch 73/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5768 - accuracy: 0.7709\n",
            "Epoch 00073: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.5771 - accuracy: 0.7710 - val_loss: 0.6704 - val_accuracy: 0.7954\n",
            "Epoch 74/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5817 - accuracy: 0.7683\n",
            "Epoch 00074: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5817 - accuracy: 0.7683 - val_loss: 0.6535 - val_accuracy: 0.7992\n",
            "Epoch 75/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7683\n",
            "Epoch 00075: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.5943 - accuracy: 0.7683 - val_loss: 0.6579 - val_accuracy: 0.8006\n",
            "Epoch 76/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5582 - accuracy: 0.7778\n",
            "Epoch 00076: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.5575 - accuracy: 0.7781 - val_loss: 0.6456 - val_accuracy: 0.8004\n",
            "Epoch 77/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.7757\n",
            "Epoch 00077: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.5528 - accuracy: 0.7757 - val_loss: 0.6784 - val_accuracy: 0.7948\n",
            "Epoch 78/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.7724\n",
            "Epoch 00078: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5627 - accuracy: 0.7724 - val_loss: 0.6415 - val_accuracy: 0.8040\n",
            "Epoch 79/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5495 - accuracy: 0.7772\n",
            "Epoch 00079: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5502 - accuracy: 0.7769 - val_loss: 0.6626 - val_accuracy: 0.7995\n",
            "Epoch 80/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.7888\n",
            "Epoch 00080: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.5297 - accuracy: 0.7888 - val_loss: 0.6478 - val_accuracy: 0.8025\n",
            "Epoch 81/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7841\n",
            "Epoch 00081: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5427 - accuracy: 0.7839 - val_loss: 0.6451 - val_accuracy: 0.8038\n",
            "Epoch 82/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5398 - accuracy: 0.7813\n",
            "Epoch 00082: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5397 - accuracy: 0.7813 - val_loss: 0.6573 - val_accuracy: 0.8013\n",
            "Epoch 83/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.7800\n",
            "Epoch 00083: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5408 - accuracy: 0.7800 - val_loss: 0.6583 - val_accuracy: 0.8021\n",
            "Epoch 84/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5219 - accuracy: 0.7885\n",
            "Epoch 00084: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.5226 - accuracy: 0.7882 - val_loss: 0.6462 - val_accuracy: 0.8030\n",
            "Epoch 85/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5362 - accuracy: 0.7832\n",
            "Epoch 00085: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.5362 - accuracy: 0.7832 - val_loss: 0.6228 - val_accuracy: 0.8120\n",
            "Epoch 86/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5203 - accuracy: 0.7860\n",
            "Epoch 00086: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5203 - accuracy: 0.7860 - val_loss: 0.6522 - val_accuracy: 0.8055\n",
            "Epoch 87/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5206 - accuracy: 0.7868\n",
            "Epoch 00087: saving model to /content\n",
            "195/195 [==============================] - 12s 59ms/step - loss: 0.5230 - accuracy: 0.7861 - val_loss: 0.6448 - val_accuracy: 0.8071\n",
            "Epoch 88/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.7892\n",
            "Epoch 00088: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5100 - accuracy: 0.7892 - val_loss: 0.6411 - val_accuracy: 0.8082\n",
            "Epoch 89/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.7952\n",
            "Epoch 00089: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.5108 - accuracy: 0.7952 - val_loss: 0.6331 - val_accuracy: 0.8101\n",
            "Epoch 90/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.7978\n",
            "Epoch 00090: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5091 - accuracy: 0.7978 - val_loss: 0.6522 - val_accuracy: 0.8058\n",
            "Epoch 91/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5099 - accuracy: 0.7961\n",
            "Epoch 00091: saving model to /content\n",
            "195/195 [==============================] - 12s 60ms/step - loss: 0.5099 - accuracy: 0.7961 - val_loss: 0.6487 - val_accuracy: 0.8061\n",
            "Epoch 92/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.8022\n",
            "Epoch 00092: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5040 - accuracy: 0.8022 - val_loss: 0.6325 - val_accuracy: 0.8102\n",
            "Epoch 93/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5142 - accuracy: 0.7907\n",
            "Epoch 00093: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5139 - accuracy: 0.7906 - val_loss: 0.6343 - val_accuracy: 0.8101\n",
            "Epoch 94/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.4930 - accuracy: 0.7963\n",
            "Epoch 00094: saving model to /content\n",
            "195/195 [==============================] - 11s 59ms/step - loss: 0.4929 - accuracy: 0.7964 - val_loss: 0.6391 - val_accuracy: 0.8107\n",
            "Epoch 95/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.7953\n",
            "Epoch 00095: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5072 - accuracy: 0.7953 - val_loss: 0.6406 - val_accuracy: 0.8095\n",
            "Epoch 96/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5165 - accuracy: 0.7868\n",
            "Epoch 00096: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5166 - accuracy: 0.7869 - val_loss: 0.6434 - val_accuracy: 0.8102\n",
            "Epoch 97/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5020 - accuracy: 0.7979\n",
            "Epoch 00097: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5023 - accuracy: 0.7978 - val_loss: 0.6374 - val_accuracy: 0.8104\n",
            "Epoch 98/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5057 - accuracy: 0.7964\n",
            "Epoch 00098: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5059 - accuracy: 0.7967 - val_loss: 0.6413 - val_accuracy: 0.8088\n",
            "Epoch 99/100\n",
            "194/195 [============================>.] - ETA: 0s - loss: 0.5033 - accuracy: 0.7949\n",
            "Epoch 00099: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.5028 - accuracy: 0.7953 - val_loss: 0.6386 - val_accuracy: 0.8088\n",
            "Epoch 100/100\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.4992 - accuracy: 0.7945\n",
            "Epoch 00100: saving model to /content\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 0.4992 - accuracy: 0.7945 - val_loss: 0.6397 - val_accuracy: 0.8089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSao7ieaUQs5",
        "colab_type": "code",
        "outputId": "e953ae94-38fa-46e1-9411-1b7f8ba54971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "red_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 8ms/step - loss: 0.6397 - accuracy: 0.8089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6397327184677124, 0.808899998664856]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2PUWg1uUTTH",
        "colab_type": "code",
        "outputId": "b9f66e9b-b887-4c6e-f82c-7bdb4a2a5a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "red_model.evaluate(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 3s 7ms/step - loss: 0.1529 - accuracy: 0.9516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.15293511748313904, 0.9516000151634216]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjYsihkZUtad",
        "colab_type": "code",
        "outputId": "d9b90669-0794-4a7b-c050-b1c48b4e6b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 6250\n",
        "history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                                      epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                      validation_data=(x_test, y_test), callbacks=[cp_callback, warm_up_lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 2.3549 - accuracy: 0.1363\n",
            "Epoch 00001: saving model to /content\n",
            "97/97 [==============================] - 8s 84ms/step - loss: 2.3549 - accuracy: 0.1363 - val_loss: 2.3470 - val_accuracy: 0.1000\n",
            "Epoch 2/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 2.1524 - accuracy: 0.2012\n",
            "Epoch 00002: saving model to /content\n",
            "97/97 [==============================] - 7s 74ms/step - loss: 2.1512 - accuracy: 0.2016 - val_loss: 2.6241 - val_accuracy: 0.1005\n",
            "Epoch 3/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 2.0223 - accuracy: 0.2623\n",
            "Epoch 00003: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 2.0218 - accuracy: 0.2629 - val_loss: 2.8972 - val_accuracy: 0.1094\n",
            "Epoch 4/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.9094 - accuracy: 0.3107\n",
            "Epoch 00004: saving model to /content\n",
            "97/97 [==============================] - 7s 75ms/step - loss: 1.9094 - accuracy: 0.3107 - val_loss: 2.5330 - val_accuracy: 0.1631\n",
            "Epoch 5/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.8516 - accuracy: 0.3242\n",
            "Epoch 00005: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.8539 - accuracy: 0.3233 - val_loss: 2.1181 - val_accuracy: 0.2335\n",
            "Epoch 6/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.8169 - accuracy: 0.3336\n",
            "Epoch 00006: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.8159 - accuracy: 0.3341 - val_loss: 1.8430 - val_accuracy: 0.3308\n",
            "Epoch 7/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.7761 - accuracy: 0.3503\n",
            "Epoch 00007: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.7761 - accuracy: 0.3503 - val_loss: 1.6801 - val_accuracy: 0.3870\n",
            "Epoch 8/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.7430 - accuracy: 0.3661\n",
            "Epoch 00008: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.7430 - accuracy: 0.3661 - val_loss: 1.6598 - val_accuracy: 0.4004\n",
            "Epoch 9/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.7145 - accuracy: 0.3780\n",
            "Epoch 00009: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 1.7145 - accuracy: 0.3780 - val_loss: 1.6847 - val_accuracy: 0.3781\n",
            "Epoch 10/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.6721 - accuracy: 0.3901\n",
            "Epoch 00010: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.6721 - accuracy: 0.3901 - val_loss: 2.7394 - val_accuracy: 0.2725\n",
            "Epoch 11/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.6724 - accuracy: 0.3973\n",
            "Epoch 00011: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.6724 - accuracy: 0.3973 - val_loss: 1.7237 - val_accuracy: 0.3933\n",
            "Epoch 12/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.6113 - accuracy: 0.4166\n",
            "Epoch 00012: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.6113 - accuracy: 0.4166 - val_loss: 1.5177 - val_accuracy: 0.4500\n",
            "Epoch 13/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.5964 - accuracy: 0.4297\n",
            "Epoch 00013: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.5964 - accuracy: 0.4297 - val_loss: 1.4893 - val_accuracy: 0.4601\n",
            "Epoch 14/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.5812 - accuracy: 0.4306\n",
            "Epoch 00014: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.5812 - accuracy: 0.4306 - val_loss: 1.7787 - val_accuracy: 0.4027\n",
            "Epoch 15/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.5228 - accuracy: 0.4475\n",
            "Epoch 00015: saving model to /content\n",
            "97/97 [==============================] - 7s 71ms/step - loss: 1.5228 - accuracy: 0.4475 - val_loss: 1.5217 - val_accuracy: 0.4622\n",
            "Epoch 16/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.5031 - accuracy: 0.4665\n",
            "Epoch 00016: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.5023 - accuracy: 0.4669 - val_loss: 1.8809 - val_accuracy: 0.3785\n",
            "Epoch 17/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.4740 - accuracy: 0.4703\n",
            "Epoch 00017: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.4740 - accuracy: 0.4703 - val_loss: 1.4294 - val_accuracy: 0.4895\n",
            "Epoch 18/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.4355 - accuracy: 0.4804\n",
            "Epoch 00018: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.4355 - accuracy: 0.4804 - val_loss: 1.5224 - val_accuracy: 0.4629\n",
            "Epoch 19/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.4541 - accuracy: 0.4760\n",
            "Epoch 00019: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.4511 - accuracy: 0.4772 - val_loss: 1.3547 - val_accuracy: 0.5100\n",
            "Epoch 20/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.4135 - accuracy: 0.4889\n",
            "Epoch 00020: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 1.4147 - accuracy: 0.4884 - val_loss: 1.6692 - val_accuracy: 0.4374\n",
            "Epoch 21/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.3944 - accuracy: 0.4964\n",
            "Epoch 00021: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.3950 - accuracy: 0.4968 - val_loss: 1.5338 - val_accuracy: 0.4606\n",
            "Epoch 22/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.3843 - accuracy: 0.5071\n",
            "Epoch 00022: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.3843 - accuracy: 0.5071 - val_loss: 1.5094 - val_accuracy: 0.4687\n",
            "Epoch 23/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.3199 - accuracy: 0.5256\n",
            "Epoch 00023: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.3177 - accuracy: 0.5262 - val_loss: 1.4691 - val_accuracy: 0.4992\n",
            "Epoch 24/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.3368 - accuracy: 0.5181\n",
            "Epoch 00024: saving model to /content\n",
            "97/97 [==============================] - 7s 71ms/step - loss: 1.3367 - accuracy: 0.5178 - val_loss: 1.3063 - val_accuracy: 0.5426\n",
            "Epoch 25/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.3104 - accuracy: 0.5260\n",
            "Epoch 00025: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.3076 - accuracy: 0.5276 - val_loss: 1.3928 - val_accuracy: 0.5147\n",
            "Epoch 26/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.2953 - accuracy: 0.5256\n",
            "Epoch 00026: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.2921 - accuracy: 0.5268 - val_loss: 1.4722 - val_accuracy: 0.4986\n",
            "Epoch 27/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.2845 - accuracy: 0.5421\n",
            "Epoch 00027: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.2827 - accuracy: 0.5428 - val_loss: 1.5315 - val_accuracy: 0.4802\n",
            "Epoch 28/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.2704 - accuracy: 0.5402\n",
            "Epoch 00028: saving model to /content\n",
            "97/97 [==============================] - 7s 74ms/step - loss: 1.2718 - accuracy: 0.5404 - val_loss: 1.3836 - val_accuracy: 0.5197\n",
            "Epoch 29/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.2183 - accuracy: 0.5613\n",
            "Epoch 00029: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.2183 - accuracy: 0.5613 - val_loss: 1.2077 - val_accuracy: 0.5664\n",
            "Epoch 30/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.1907 - accuracy: 0.5674\n",
            "Epoch 00030: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.1907 - accuracy: 0.5674 - val_loss: 1.5485 - val_accuracy: 0.4986\n",
            "Epoch 31/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.1970 - accuracy: 0.5601\n",
            "Epoch 00031: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.1988 - accuracy: 0.5593 - val_loss: 1.3774 - val_accuracy: 0.5292\n",
            "Epoch 32/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.1817 - accuracy: 0.5732\n",
            "Epoch 00032: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 1.1800 - accuracy: 0.5732 - val_loss: 1.1399 - val_accuracy: 0.5979\n",
            "Epoch 33/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.1823 - accuracy: 0.5699\n",
            "Epoch 00033: saving model to /content\n",
            "97/97 [==============================] - 7s 71ms/step - loss: 1.1835 - accuracy: 0.5695 - val_loss: 1.1649 - val_accuracy: 0.6008\n",
            "Epoch 34/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.1729 - accuracy: 0.5735\n",
            "Epoch 00034: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 1.1715 - accuracy: 0.5736 - val_loss: 1.2092 - val_accuracy: 0.5875\n",
            "Epoch 35/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.1366 - accuracy: 0.5896\n",
            "Epoch 00035: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.1366 - accuracy: 0.5896 - val_loss: 1.2100 - val_accuracy: 0.5842\n",
            "Epoch 36/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.1325 - accuracy: 0.5925\n",
            "Epoch 00036: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.1325 - accuracy: 0.5925 - val_loss: 1.3722 - val_accuracy: 0.5591\n",
            "Epoch 37/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.1092 - accuracy: 0.6003\n",
            "Epoch 00037: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.1087 - accuracy: 0.6001 - val_loss: 1.1474 - val_accuracy: 0.6079\n",
            "Epoch 38/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.0921 - accuracy: 0.6060\n",
            "Epoch 00038: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.0921 - accuracy: 0.6060 - val_loss: 1.3668 - val_accuracy: 0.5438\n",
            "Epoch 39/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.0364 - accuracy: 0.6196\n",
            "Epoch 00039: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.0364 - accuracy: 0.6196 - val_loss: 1.4005 - val_accuracy: 0.5558\n",
            "Epoch 40/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.0741 - accuracy: 0.6103\n",
            "Epoch 00040: saving model to /content\n",
            "97/97 [==============================] - 7s 71ms/step - loss: 1.0753 - accuracy: 0.6104 - val_loss: 1.2098 - val_accuracy: 0.5862\n",
            "Epoch 41/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.0334 - accuracy: 0.6188\n",
            "Epoch 00041: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.0334 - accuracy: 0.6188 - val_loss: 1.2244 - val_accuracy: 0.5982\n",
            "Epoch 42/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 1.0304 - accuracy: 0.6210\n",
            "Epoch 00042: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.0294 - accuracy: 0.6220 - val_loss: 1.0988 - val_accuracy: 0.6280\n",
            "Epoch 43/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.0181 - accuracy: 0.6351\n",
            "Epoch 00043: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 1.0181 - accuracy: 0.6351 - val_loss: 1.0142 - val_accuracy: 0.6500\n",
            "Epoch 44/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.9665 - accuracy: 0.6473\n",
            "Epoch 00044: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.9688 - accuracy: 0.6468 - val_loss: 1.1174 - val_accuracy: 0.6234\n",
            "Epoch 45/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.9681 - accuracy: 0.6505\n",
            "Epoch 00045: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.9681 - accuracy: 0.6505 - val_loss: 1.3071 - val_accuracy: 0.5975\n",
            "Epoch 46/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.9623 - accuracy: 0.6437\n",
            "Epoch 00046: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.9623 - accuracy: 0.6437 - val_loss: 1.0996 - val_accuracy: 0.6293\n",
            "Epoch 47/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.9752 - accuracy: 0.6390\n",
            "Epoch 00047: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.9742 - accuracy: 0.6392 - val_loss: 1.3388 - val_accuracy: 0.5735\n",
            "Epoch 48/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.9363 - accuracy: 0.6520\n",
            "Epoch 00048: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.9363 - accuracy: 0.6520 - val_loss: 1.2928 - val_accuracy: 0.5865\n",
            "Epoch 49/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.9191 - accuracy: 0.6619\n",
            "Epoch 00049: saving model to /content\n",
            "97/97 [==============================] - 7s 71ms/step - loss: 0.9183 - accuracy: 0.6617 - val_loss: 1.1696 - val_accuracy: 0.6262\n",
            "Epoch 50/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.9316 - accuracy: 0.6539\n",
            "Epoch 00050: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.9316 - accuracy: 0.6539 - val_loss: 1.1468 - val_accuracy: 0.6252\n",
            "Epoch 51/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.9303 - accuracy: 0.6544\n",
            "Epoch 00051: saving model to /content\n",
            "97/97 [==============================] - 7s 71ms/step - loss: 0.9282 - accuracy: 0.6549 - val_loss: 1.1305 - val_accuracy: 0.6195\n",
            "Epoch 52/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.8980 - accuracy: 0.6686\n",
            "Epoch 00052: saving model to /content\n",
            "97/97 [==============================] - 7s 74ms/step - loss: 0.8980 - accuracy: 0.6686 - val_loss: 1.1321 - val_accuracy: 0.6382\n",
            "Epoch 53/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.8764 - accuracy: 0.6747\n",
            "Epoch 00053: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.8764 - accuracy: 0.6747 - val_loss: 1.1477 - val_accuracy: 0.6324\n",
            "Epoch 54/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.8643 - accuracy: 0.6749\n",
            "Epoch 00054: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.8652 - accuracy: 0.6746 - val_loss: 1.1358 - val_accuracy: 0.6382\n",
            "Epoch 55/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.8561 - accuracy: 0.6854\n",
            "Epoch 00055: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.8551 - accuracy: 0.6862 - val_loss: 1.0594 - val_accuracy: 0.6540\n",
            "Epoch 56/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.8552 - accuracy: 0.6798\n",
            "Epoch 00056: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.8552 - accuracy: 0.6798 - val_loss: 0.9890 - val_accuracy: 0.6757\n",
            "Epoch 57/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.8205 - accuracy: 0.6919\n",
            "Epoch 00057: saving model to /content\n",
            "97/97 [==============================] - 7s 71ms/step - loss: 0.8205 - accuracy: 0.6919 - val_loss: 1.0435 - val_accuracy: 0.6581\n",
            "Epoch 58/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.8419 - accuracy: 0.6867\n",
            "Epoch 00058: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.8419 - accuracy: 0.6867 - val_loss: 1.1359 - val_accuracy: 0.6463\n",
            "Epoch 59/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7962 - accuracy: 0.7030\n",
            "Epoch 00059: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.7962 - accuracy: 0.7030 - val_loss: 1.1090 - val_accuracy: 0.6537\n",
            "Epoch 60/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.8014 - accuracy: 0.7022\n",
            "Epoch 00060: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.8009 - accuracy: 0.7021 - val_loss: 1.1009 - val_accuracy: 0.6529\n",
            "Epoch 61/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7728 - accuracy: 0.7072\n",
            "Epoch 00061: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.7728 - accuracy: 0.7072 - val_loss: 1.0780 - val_accuracy: 0.6527\n",
            "Epoch 62/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7651 - accuracy: 0.7161\n",
            "Epoch 00062: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.7651 - accuracy: 0.7161 - val_loss: 1.1886 - val_accuracy: 0.6356\n",
            "Epoch 63/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7831 - accuracy: 0.6982\n",
            "Epoch 00063: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.7831 - accuracy: 0.6982 - val_loss: 1.1103 - val_accuracy: 0.6589\n",
            "Epoch 64/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7509 - accuracy: 0.7110\n",
            "Epoch 00064: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.7509 - accuracy: 0.7110 - val_loss: 1.0128 - val_accuracy: 0.6852\n",
            "Epoch 65/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7424 - accuracy: 0.7197\n",
            "Epoch 00065: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.7424 - accuracy: 0.7197 - val_loss: 1.0457 - val_accuracy: 0.6747\n",
            "Epoch 66/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.7253 - accuracy: 0.7228\n",
            "Epoch 00066: saving model to /content\n",
            "97/97 [==============================] - 7s 74ms/step - loss: 0.7264 - accuracy: 0.7224 - val_loss: 1.0515 - val_accuracy: 0.6762\n",
            "Epoch 67/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.7266 - accuracy: 0.7230\n",
            "Epoch 00067: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.7269 - accuracy: 0.7226 - val_loss: 1.0721 - val_accuracy: 0.6721\n",
            "Epoch 68/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7097 - accuracy: 0.7226\n",
            "Epoch 00068: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.7097 - accuracy: 0.7226 - val_loss: 1.0451 - val_accuracy: 0.6738\n",
            "Epoch 69/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7087 - accuracy: 0.7262\n",
            "Epoch 00069: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.7087 - accuracy: 0.7262 - val_loss: 0.9698 - val_accuracy: 0.6991\n",
            "Epoch 70/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.7131 - accuracy: 0.7284\n",
            "Epoch 00070: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.7128 - accuracy: 0.7284 - val_loss: 1.0395 - val_accuracy: 0.6786\n",
            "Epoch 71/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6807 - accuracy: 0.7373\n",
            "Epoch 00071: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.6807 - accuracy: 0.7373 - val_loss: 0.9586 - val_accuracy: 0.7002\n",
            "Epoch 72/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.7357\n",
            "Epoch 00072: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6933 - accuracy: 0.7357 - val_loss: 1.0210 - val_accuracy: 0.6939\n",
            "Epoch 73/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.6649 - accuracy: 0.7422\n",
            "Epoch 00073: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6659 - accuracy: 0.7418 - val_loss: 1.0333 - val_accuracy: 0.6883\n",
            "Epoch 74/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.7559\n",
            "Epoch 00074: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6369 - accuracy: 0.7559 - val_loss: 1.0148 - val_accuracy: 0.6980\n",
            "Epoch 75/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.7401\n",
            "Epoch 00075: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6722 - accuracy: 0.7401 - val_loss: 1.0130 - val_accuracy: 0.6987\n",
            "Epoch 76/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.6636 - accuracy: 0.7450\n",
            "Epoch 00076: saving model to /content\n",
            "97/97 [==============================] - 7s 76ms/step - loss: 0.6634 - accuracy: 0.7452 - val_loss: 1.0097 - val_accuracy: 0.6897\n",
            "Epoch 77/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.6413 - accuracy: 0.7504\n",
            "Epoch 00077: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6399 - accuracy: 0.7512 - val_loss: 0.9848 - val_accuracy: 0.7006\n",
            "Epoch 78/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.6396 - accuracy: 0.7516\n",
            "Epoch 00078: saving model to /content\n",
            "97/97 [==============================] - 7s 74ms/step - loss: 0.6405 - accuracy: 0.7512 - val_loss: 0.9806 - val_accuracy: 0.7078\n",
            "Epoch 79/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6381 - accuracy: 0.7525\n",
            "Epoch 00079: saving model to /content\n",
            "97/97 [==============================] - 7s 72ms/step - loss: 0.6381 - accuracy: 0.7525 - val_loss: 0.9612 - val_accuracy: 0.7057\n",
            "Epoch 80/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.6142 - accuracy: 0.7618\n",
            "Epoch 00080: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6137 - accuracy: 0.7617 - val_loss: 0.9667 - val_accuracy: 0.7113\n",
            "Epoch 81/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.6079 - accuracy: 0.7636\n",
            "Epoch 00081: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6079 - accuracy: 0.7638 - val_loss: 0.9826 - val_accuracy: 0.7086\n",
            "Epoch 82/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.6123 - accuracy: 0.7568\n",
            "Epoch 00082: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6126 - accuracy: 0.7569 - val_loss: 0.9741 - val_accuracy: 0.7094\n",
            "Epoch 83/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6051 - accuracy: 0.7632\n",
            "Epoch 00083: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6051 - accuracy: 0.7632 - val_loss: 0.9805 - val_accuracy: 0.7089\n",
            "Epoch 84/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.5884 - accuracy: 0.7708\n",
            "Epoch 00084: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5888 - accuracy: 0.7711 - val_loss: 1.0149 - val_accuracy: 0.7019\n",
            "Epoch 85/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.7620\n",
            "Epoch 00085: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.6072 - accuracy: 0.7620 - val_loss: 0.9798 - val_accuracy: 0.7105\n",
            "Epoch 86/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.7750\n",
            "Epoch 00086: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5858 - accuracy: 0.7750 - val_loss: 0.9990 - val_accuracy: 0.7087\n",
            "Epoch 87/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.5908 - accuracy: 0.7729\n",
            "Epoch 00087: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5895 - accuracy: 0.7732 - val_loss: 1.0047 - val_accuracy: 0.7057\n",
            "Epoch 88/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5743 - accuracy: 0.7779\n",
            "Epoch 00088: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5743 - accuracy: 0.7779 - val_loss: 0.9807 - val_accuracy: 0.7135\n",
            "Epoch 89/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.7622\n",
            "Epoch 00089: saving model to /content\n",
            "97/97 [==============================] - 7s 74ms/step - loss: 0.6102 - accuracy: 0.7622 - val_loss: 0.9705 - val_accuracy: 0.7149\n",
            "Epoch 90/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7741\n",
            "Epoch 00090: saving model to /content\n",
            "97/97 [==============================] - 7s 75ms/step - loss: 0.5692 - accuracy: 0.7742 - val_loss: 0.9574 - val_accuracy: 0.7179\n",
            "Epoch 91/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.7730\n",
            "Epoch 00091: saving model to /content\n",
            "97/97 [==============================] - 7s 74ms/step - loss: 0.5783 - accuracy: 0.7730 - val_loss: 0.9568 - val_accuracy: 0.7171\n",
            "Epoch 92/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.5908 - accuracy: 0.7635\n",
            "Epoch 00092: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5891 - accuracy: 0.7645 - val_loss: 0.9724 - val_accuracy: 0.7157\n",
            "Epoch 93/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.5682 - accuracy: 0.7734\n",
            "Epoch 00093: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5695 - accuracy: 0.7732 - val_loss: 0.9784 - val_accuracy: 0.7133\n",
            "Epoch 94/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.5867 - accuracy: 0.7708\n",
            "Epoch 00094: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5848 - accuracy: 0.7714 - val_loss: 0.9685 - val_accuracy: 0.7135\n",
            "Epoch 95/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5986 - accuracy: 0.7620\n",
            "Epoch 00095: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5986 - accuracy: 0.7620 - val_loss: 0.9754 - val_accuracy: 0.7122\n",
            "Epoch 96/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7762\n",
            "Epoch 00096: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5603 - accuracy: 0.7772 - val_loss: 0.9715 - val_accuracy: 0.7151\n",
            "Epoch 97/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.7695\n",
            "Epoch 00097: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5856 - accuracy: 0.7695 - val_loss: 0.9664 - val_accuracy: 0.7148\n",
            "Epoch 98/100\n",
            "96/97 [============================>.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7788\n",
            "Epoch 00098: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5656 - accuracy: 0.7792 - val_loss: 0.9669 - val_accuracy: 0.7149\n",
            "Epoch 99/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.7838\n",
            "Epoch 00099: saving model to /content\n",
            "97/97 [==============================] - 7s 73ms/step - loss: 0.5679 - accuracy: 0.7838 - val_loss: 0.9659 - val_accuracy: 0.7148\n",
            "Epoch 100/100\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5606 - accuracy: 0.7803\n",
            "Epoch 00100: saving model to /content\n",
            "97/97 [==============================] - 8s 78ms/step - loss: 0.5606 - accuracy: 0.7803 - val_loss: 0.9670 - val_accuracy: 0.7153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6rIzWLtUuf4",
        "colab_type": "code",
        "outputId": "1c6ee042-12ad-4373-ff07-be7911bb9d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "red_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 8ms/step - loss: 0.9670 - accuracy: 0.7153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9669798612594604, 0.7153000235557556]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79Pr3C1ZemxA",
        "colab_type": "code",
        "outputId": "9fcd58cf-f8de-4aff-a185-b84ae8febb00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "red_model.evaluate(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 2s 8ms/step - loss: 0.1841 - accuracy: 0.9432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18405070900917053, 0.9431999921798706]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNkfHPz8fD1v",
        "colab_type": "code",
        "outputId": "39867785-3554-4d8c-a002-99beffb1d42a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 3120\n",
        "history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                                      epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                      validation_data=(x_test, y_test), callbacks=[cp_callback, warm_up_lr])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 2.3715 - accuracy: 0.1057\n",
            "Epoch 00001: saving model to /content\n",
            "48/48 [==============================] - 7s 137ms/step - loss: 2.3714 - accuracy: 0.1058 - val_loss: 2.3049 - val_accuracy: 0.0943\n",
            "Epoch 2/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 2.1997 - accuracy: 0.1852\n",
            "Epoch 00002: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 2.1983 - accuracy: 0.1875 - val_loss: 2.3798 - val_accuracy: 0.1077\n",
            "Epoch 3/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 2.0711 - accuracy: 0.2435\n",
            "Epoch 00003: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 2.0711 - accuracy: 0.2435 - val_loss: 2.8695 - val_accuracy: 0.1024\n",
            "Epoch 4/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 2.0139 - accuracy: 0.2674\n",
            "Epoch 00004: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 2.0130 - accuracy: 0.2673 - val_loss: 3.3488 - val_accuracy: 0.1020\n",
            "Epoch 5/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.9564 - accuracy: 0.2881\n",
            "Epoch 00005: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.9569 - accuracy: 0.2873 - val_loss: 3.0100 - val_accuracy: 0.1000\n",
            "Epoch 6/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.9061 - accuracy: 0.3024\n",
            "Epoch 00006: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.9061 - accuracy: 0.3024 - val_loss: 2.6980 - val_accuracy: 0.1120\n",
            "Epoch 7/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.8862 - accuracy: 0.3082\n",
            "Epoch 00007: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.8862 - accuracy: 0.3082 - val_loss: 2.4929 - val_accuracy: 0.1555\n",
            "Epoch 8/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.8348 - accuracy: 0.3259\n",
            "Epoch 00008: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.8348 - accuracy: 0.3259 - val_loss: 2.0141 - val_accuracy: 0.2503\n",
            "Epoch 9/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.8003 - accuracy: 0.3406\n",
            "Epoch 00009: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.7979 - accuracy: 0.3423 - val_loss: 2.0709 - val_accuracy: 0.2375\n",
            "Epoch 10/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.7795 - accuracy: 0.3590\n",
            "Epoch 00010: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.7795 - accuracy: 0.3590 - val_loss: 2.0590 - val_accuracy: 0.2861\n",
            "Epoch 11/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.7458 - accuracy: 0.3636\n",
            "Epoch 00011: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.7426 - accuracy: 0.3675 - val_loss: 1.8958 - val_accuracy: 0.3279\n",
            "Epoch 12/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.7616 - accuracy: 0.3698\n",
            "Epoch 00012: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.7616 - accuracy: 0.3698 - val_loss: 1.8197 - val_accuracy: 0.3343\n",
            "Epoch 13/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.7312 - accuracy: 0.3750\n",
            "Epoch 00013: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.7336 - accuracy: 0.3730 - val_loss: 1.6810 - val_accuracy: 0.3747\n",
            "Epoch 14/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.6506 - accuracy: 0.3937\n",
            "Epoch 00014: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.6523 - accuracy: 0.3940 - val_loss: 2.7476 - val_accuracy: 0.2797\n",
            "Epoch 15/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.6513 - accuracy: 0.3914\n",
            "Epoch 00015: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.6489 - accuracy: 0.3933 - val_loss: 1.9185 - val_accuracy: 0.3548\n",
            "Epoch 16/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.6091 - accuracy: 0.4144\n",
            "Epoch 00016: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.6077 - accuracy: 0.4159 - val_loss: 2.0312 - val_accuracy: 0.3207\n",
            "Epoch 17/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.6224 - accuracy: 0.4120\n",
            "Epoch 00017: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.6224 - accuracy: 0.4120 - val_loss: 1.5245 - val_accuracy: 0.4500\n",
            "Epoch 18/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.6006 - accuracy: 0.4054\n",
            "Epoch 00018: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.5961 - accuracy: 0.4067 - val_loss: 2.4931 - val_accuracy: 0.2651\n",
            "Epoch 19/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.5926 - accuracy: 0.4318\n",
            "Epoch 00019: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.5920 - accuracy: 0.4326 - val_loss: 1.6858 - val_accuracy: 0.3829\n",
            "Epoch 20/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.5243 - accuracy: 0.4565\n",
            "Epoch 00020: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.5243 - accuracy: 0.4565 - val_loss: 1.8712 - val_accuracy: 0.3734\n",
            "Epoch 21/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.5261 - accuracy: 0.4467\n",
            "Epoch 00021: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.5261 - accuracy: 0.4467 - val_loss: 1.4402 - val_accuracy: 0.4777\n",
            "Epoch 22/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.5045 - accuracy: 0.4607\n",
            "Epoch 00022: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.5045 - accuracy: 0.4607 - val_loss: 1.5913 - val_accuracy: 0.4292\n",
            "Epoch 23/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.4812 - accuracy: 0.4722\n",
            "Epoch 00023: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.4812 - accuracy: 0.4722 - val_loss: 1.5031 - val_accuracy: 0.4635\n",
            "Epoch 24/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.5176 - accuracy: 0.4519\n",
            "Epoch 00024: saving model to /content\n",
            "48/48 [==============================] - 5s 106ms/step - loss: 1.5176 - accuracy: 0.4519 - val_loss: 1.6893 - val_accuracy: 0.4129\n",
            "Epoch 25/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.4659 - accuracy: 0.4771\n",
            "Epoch 00025: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.4659 - accuracy: 0.4771 - val_loss: 1.4813 - val_accuracy: 0.4751\n",
            "Epoch 26/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.4328 - accuracy: 0.4692\n",
            "Epoch 00026: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.4328 - accuracy: 0.4692 - val_loss: 2.0988 - val_accuracy: 0.3713\n",
            "Epoch 27/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.4434 - accuracy: 0.4728\n",
            "Epoch 00027: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.4434 - accuracy: 0.4728 - val_loss: 1.6129 - val_accuracy: 0.4490\n",
            "Epoch 28/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.4271 - accuracy: 0.4951\n",
            "Epoch 00028: saving model to /content\n",
            "48/48 [==============================] - 5s 98ms/step - loss: 1.4271 - accuracy: 0.4951 - val_loss: 1.8905 - val_accuracy: 0.4061\n",
            "Epoch 29/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.3851 - accuracy: 0.5173\n",
            "Epoch 00029: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.3851 - accuracy: 0.5173 - val_loss: 1.4585 - val_accuracy: 0.4794\n",
            "Epoch 30/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.3914 - accuracy: 0.4918\n",
            "Epoch 00030: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.3914 - accuracy: 0.4918 - val_loss: 1.4868 - val_accuracy: 0.4705\n",
            "Epoch 31/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.3701 - accuracy: 0.5029\n",
            "Epoch 00031: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.3701 - accuracy: 0.5029 - val_loss: 1.4050 - val_accuracy: 0.5003\n",
            "Epoch 32/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.3297 - accuracy: 0.5196\n",
            "Epoch 00032: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.3297 - accuracy: 0.5196 - val_loss: 1.6653 - val_accuracy: 0.4368\n",
            "Epoch 33/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.3432 - accuracy: 0.5160\n",
            "Epoch 00033: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.3432 - accuracy: 0.5160 - val_loss: 1.5869 - val_accuracy: 0.4555\n",
            "Epoch 34/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.2953 - accuracy: 0.5265\n",
            "Epoch 00034: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.2953 - accuracy: 0.5265 - val_loss: 1.6908 - val_accuracy: 0.4488\n",
            "Epoch 35/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.2840 - accuracy: 0.5364\n",
            "Epoch 00035: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.2836 - accuracy: 0.5360 - val_loss: 1.5347 - val_accuracy: 0.4798\n",
            "Epoch 36/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.2832 - accuracy: 0.5399\n",
            "Epoch 00036: saving model to /content\n",
            "48/48 [==============================] - 5s 102ms/step - loss: 1.2832 - accuracy: 0.5399 - val_loss: 1.3800 - val_accuracy: 0.5210\n",
            "Epoch 37/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.2665 - accuracy: 0.5429\n",
            "Epoch 00037: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.2665 - accuracy: 0.5429 - val_loss: 2.1646 - val_accuracy: 0.4062\n",
            "Epoch 38/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.2851 - accuracy: 0.5388\n",
            "Epoch 00038: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.2876 - accuracy: 0.5376 - val_loss: 1.6723 - val_accuracy: 0.4695\n",
            "Epoch 39/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.2336 - accuracy: 0.5511\n",
            "Epoch 00039: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.2326 - accuracy: 0.5514 - val_loss: 1.8520 - val_accuracy: 0.4267\n",
            "Epoch 40/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.2575 - accuracy: 0.5488\n",
            "Epoch 00040: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.2575 - accuracy: 0.5488 - val_loss: 1.6961 - val_accuracy: 0.4574\n",
            "Epoch 41/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.2146 - accuracy: 0.5658\n",
            "Epoch 00041: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.2146 - accuracy: 0.5658 - val_loss: 1.4234 - val_accuracy: 0.5123\n",
            "Epoch 42/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.2047 - accuracy: 0.5563\n",
            "Epoch 00042: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 1.2047 - accuracy: 0.5563 - val_loss: 1.4353 - val_accuracy: 0.5175\n",
            "Epoch 43/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.1768 - accuracy: 0.5681\n",
            "Epoch 00043: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.1768 - accuracy: 0.5681 - val_loss: 1.5459 - val_accuracy: 0.4866\n",
            "Epoch 44/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.1738 - accuracy: 0.5772\n",
            "Epoch 00044: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.1738 - accuracy: 0.5772 - val_loss: 1.4821 - val_accuracy: 0.5136\n",
            "Epoch 45/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.1744 - accuracy: 0.5839\n",
            "Epoch 00045: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.1695 - accuracy: 0.5867 - val_loss: 1.5727 - val_accuracy: 0.4859\n",
            "Epoch 46/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.1243 - accuracy: 0.5866\n",
            "Epoch 00046: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.1220 - accuracy: 0.5870 - val_loss: 1.3924 - val_accuracy: 0.5223\n",
            "Epoch 47/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.1243 - accuracy: 0.5902\n",
            "Epoch 00047: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.1191 - accuracy: 0.5929 - val_loss: 1.3488 - val_accuracy: 0.5331\n",
            "Epoch 48/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.0993 - accuracy: 0.6029\n",
            "Epoch 00048: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.0961 - accuracy: 0.6047 - val_loss: 1.3690 - val_accuracy: 0.5521\n",
            "Epoch 49/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.1031 - accuracy: 0.5946\n",
            "Epoch 00049: saving model to /content\n",
            "48/48 [==============================] - 5s 106ms/step - loss: 1.1031 - accuracy: 0.5946 - val_loss: 1.7411 - val_accuracy: 0.4721\n",
            "Epoch 50/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.0894 - accuracy: 0.6104\n",
            "Epoch 00050: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 1.0894 - accuracy: 0.6104 - val_loss: 1.6487 - val_accuracy: 0.4962\n",
            "Epoch 51/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.0414 - accuracy: 0.6293\n",
            "Epoch 00051: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.0414 - accuracy: 0.6293 - val_loss: 1.4396 - val_accuracy: 0.5420\n",
            "Epoch 52/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.0554 - accuracy: 0.6057\n",
            "Epoch 00052: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.0554 - accuracy: 0.6057 - val_loss: 1.6167 - val_accuracy: 0.5124\n",
            "Epoch 53/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 1.0371 - accuracy: 0.6300\n",
            "Epoch 00053: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.0335 - accuracy: 0.6312 - val_loss: 1.3353 - val_accuracy: 0.5529\n",
            "Epoch 54/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 1.0250 - accuracy: 0.6260\n",
            "Epoch 00054: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 1.0250 - accuracy: 0.6260 - val_loss: 1.3888 - val_accuracy: 0.5448\n",
            "Epoch 55/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.9759 - accuracy: 0.6474\n",
            "Epoch 00055: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 0.9757 - accuracy: 0.6489 - val_loss: 1.2957 - val_accuracy: 0.5646\n",
            "Epoch 56/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.9886 - accuracy: 0.6380\n",
            "Epoch 00056: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.9896 - accuracy: 0.6368 - val_loss: 1.3001 - val_accuracy: 0.5668\n",
            "Epoch 57/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.9786 - accuracy: 0.6397\n",
            "Epoch 00057: saving model to /content\n",
            "48/48 [==============================] - 5s 103ms/step - loss: 0.9758 - accuracy: 0.6414 - val_loss: 1.3768 - val_accuracy: 0.5509\n",
            "Epoch 58/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.9505 - accuracy: 0.6558\n",
            "Epoch 00058: saving model to /content\n",
            "48/48 [==============================] - 5s 103ms/step - loss: 0.9505 - accuracy: 0.6558 - val_loss: 1.3774 - val_accuracy: 0.5512\n",
            "Epoch 59/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.9543 - accuracy: 0.6499\n",
            "Epoch 00059: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.9543 - accuracy: 0.6499 - val_loss: 1.6954 - val_accuracy: 0.4937\n",
            "Epoch 60/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.9622 - accuracy: 0.6371\n",
            "Epoch 00060: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.9622 - accuracy: 0.6371 - val_loss: 1.2818 - val_accuracy: 0.5774\n",
            "Epoch 61/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.9407 - accuracy: 0.6557\n",
            "Epoch 00061: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.9384 - accuracy: 0.6580 - val_loss: 1.3396 - val_accuracy: 0.5713\n",
            "Epoch 62/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.9079 - accuracy: 0.6620\n",
            "Epoch 00062: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.9079 - accuracy: 0.6620 - val_loss: 1.3494 - val_accuracy: 0.5641\n",
            "Epoch 63/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.9248 - accuracy: 0.6611\n",
            "Epoch 00063: saving model to /content\n",
            "48/48 [==============================] - 5s 99ms/step - loss: 0.9280 - accuracy: 0.6600 - val_loss: 1.4164 - val_accuracy: 0.5667\n",
            "Epoch 64/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.9000 - accuracy: 0.6734\n",
            "Epoch 00064: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.9000 - accuracy: 0.6734 - val_loss: 1.3505 - val_accuracy: 0.5704\n",
            "Epoch 65/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.8757 - accuracy: 0.6813\n",
            "Epoch 00065: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.8757 - accuracy: 0.6813 - val_loss: 1.3628 - val_accuracy: 0.5639\n",
            "Epoch 66/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.8661 - accuracy: 0.6806\n",
            "Epoch 00066: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.8661 - accuracy: 0.6806 - val_loss: 1.3152 - val_accuracy: 0.5858\n",
            "Epoch 67/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.8525 - accuracy: 0.6839\n",
            "Epoch 00067: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.8525 - accuracy: 0.6839 - val_loss: 1.3221 - val_accuracy: 0.5841\n",
            "Epoch 68/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.8492 - accuracy: 0.6918\n",
            "Epoch 00068: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.8492 - accuracy: 0.6918 - val_loss: 1.3174 - val_accuracy: 0.5751\n",
            "Epoch 69/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.8414 - accuracy: 0.6878\n",
            "Epoch 00069: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.8463 - accuracy: 0.6865 - val_loss: 1.3776 - val_accuracy: 0.5700\n",
            "Epoch 70/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.8177 - accuracy: 0.6965\n",
            "Epoch 00070: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.8180 - accuracy: 0.6980 - val_loss: 1.3977 - val_accuracy: 0.5718\n",
            "Epoch 71/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.6999\n",
            "Epoch 00071: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.8070 - accuracy: 0.6999 - val_loss: 1.3656 - val_accuracy: 0.5822\n",
            "Epoch 72/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.7717 - accuracy: 0.7216\n",
            "Epoch 00072: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.7709 - accuracy: 0.7219 - val_loss: 1.2646 - val_accuracy: 0.6001\n",
            "Epoch 73/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.7801 - accuracy: 0.7160\n",
            "Epoch 00073: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.7801 - accuracy: 0.7160 - val_loss: 1.3280 - val_accuracy: 0.5897\n",
            "Epoch 74/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.7824 - accuracy: 0.7124\n",
            "Epoch 00074: saving model to /content\n",
            "48/48 [==============================] - 5s 106ms/step - loss: 0.7824 - accuracy: 0.7124 - val_loss: 1.3368 - val_accuracy: 0.5857\n",
            "Epoch 75/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.7583 - accuracy: 0.7156\n",
            "Epoch 00075: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.7582 - accuracy: 0.7160 - val_loss: 1.3443 - val_accuracy: 0.5867\n",
            "Epoch 76/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.7670 - accuracy: 0.7192\n",
            "Epoch 00076: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.7670 - accuracy: 0.7192 - val_loss: 1.2961 - val_accuracy: 0.5922\n",
            "Epoch 77/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.7307 - accuracy: 0.7248\n",
            "Epoch 00077: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.7307 - accuracy: 0.7248 - val_loss: 1.2633 - val_accuracy: 0.6010\n",
            "Epoch 78/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.7320 - accuracy: 0.7248\n",
            "Epoch 00078: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.7320 - accuracy: 0.7248 - val_loss: 1.2650 - val_accuracy: 0.6003\n",
            "Epoch 79/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.7239 - accuracy: 0.7261\n",
            "Epoch 00079: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.7239 - accuracy: 0.7261 - val_loss: 1.3477 - val_accuracy: 0.5895\n",
            "Epoch 80/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6995 - accuracy: 0.7385\n",
            "Epoch 00080: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.6995 - accuracy: 0.7385 - val_loss: 1.3061 - val_accuracy: 0.5979\n",
            "Epoch 81/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.7061 - accuracy: 0.7269\n",
            "Epoch 00081: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.7050 - accuracy: 0.7271 - val_loss: 1.3197 - val_accuracy: 0.6005\n",
            "Epoch 82/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.7513\n",
            "Epoch 00082: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.6905 - accuracy: 0.7513 - val_loss: 1.3204 - val_accuracy: 0.5960\n",
            "Epoch 83/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6966 - accuracy: 0.7279\n",
            "Epoch 00083: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.6966 - accuracy: 0.7291 - val_loss: 1.3803 - val_accuracy: 0.5877\n",
            "Epoch 84/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.7408\n",
            "Epoch 00084: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.6981 - accuracy: 0.7408 - val_loss: 1.3423 - val_accuracy: 0.6020\n",
            "Epoch 85/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.7025 - accuracy: 0.7313\n",
            "Epoch 00085: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.7054 - accuracy: 0.7310 - val_loss: 1.3377 - val_accuracy: 0.5947\n",
            "Epoch 86/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6513 - accuracy: 0.7560\n",
            "Epoch 00086: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.6527 - accuracy: 0.7556 - val_loss: 1.3030 - val_accuracy: 0.6059\n",
            "Epoch 87/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6636 - accuracy: 0.7487\n",
            "Epoch 00087: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.6608 - accuracy: 0.7507 - val_loss: 1.3188 - val_accuracy: 0.6003\n",
            "Epoch 88/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.7467\n",
            "Epoch 00088: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.6827 - accuracy: 0.7467 - val_loss: 1.3595 - val_accuracy: 0.5966\n",
            "Epoch 89/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6547 - accuracy: 0.7543\n",
            "Epoch 00089: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.6547 - accuracy: 0.7543 - val_loss: 1.3112 - val_accuracy: 0.6032\n",
            "Epoch 90/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6735 - accuracy: 0.7457\n",
            "Epoch 00090: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.6747 - accuracy: 0.7457 - val_loss: 1.3352 - val_accuracy: 0.6006\n",
            "Epoch 91/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6448 - accuracy: 0.7533\n",
            "Epoch 00091: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.6459 - accuracy: 0.7523 - val_loss: 1.3267 - val_accuracy: 0.6059\n",
            "Epoch 92/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6599 - accuracy: 0.7467\n",
            "Epoch 00092: saving model to /content\n",
            "48/48 [==============================] - 5s 102ms/step - loss: 0.6599 - accuracy: 0.7467 - val_loss: 1.3101 - val_accuracy: 0.6055\n",
            "Epoch 93/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6508 - accuracy: 0.7585\n",
            "Epoch 00093: saving model to /content\n",
            "48/48 [==============================] - 5s 102ms/step - loss: 0.6508 - accuracy: 0.7585 - val_loss: 1.3219 - val_accuracy: 0.6052\n",
            "Epoch 94/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6625 - accuracy: 0.7444\n",
            "Epoch 00094: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.6625 - accuracy: 0.7444 - val_loss: 1.3153 - val_accuracy: 0.6061\n",
            "Epoch 95/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6431 - accuracy: 0.7503\n",
            "Epoch 00095: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.6431 - accuracy: 0.7503 - val_loss: 1.3135 - val_accuracy: 0.6067\n",
            "Epoch 96/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6443 - accuracy: 0.7565\n",
            "Epoch 00096: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.6443 - accuracy: 0.7565 - val_loss: 1.3123 - val_accuracy: 0.6066\n",
            "Epoch 97/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6548 - accuracy: 0.7564\n",
            "Epoch 00097: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.6570 - accuracy: 0.7536 - val_loss: 1.3165 - val_accuracy: 0.6075\n",
            "Epoch 98/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6576 - accuracy: 0.7513\n",
            "Epoch 00098: saving model to /content\n",
            "48/48 [==============================] - 5s 101ms/step - loss: 0.6599 - accuracy: 0.7507 - val_loss: 1.3160 - val_accuracy: 0.6064\n",
            "Epoch 99/100\n",
            "47/48 [============================>.] - ETA: 0s - loss: 0.6659 - accuracy: 0.7500\n",
            "Epoch 00099: saving model to /content\n",
            "48/48 [==============================] - 5s 109ms/step - loss: 0.6645 - accuracy: 0.7510 - val_loss: 1.3171 - val_accuracy: 0.6066\n",
            "Epoch 100/100\n",
            "48/48 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.7664\n",
            "Epoch 00100: saving model to /content\n",
            "48/48 [==============================] - 5s 100ms/step - loss: 0.6236 - accuracy: 0.7664 - val_loss: 1.3181 - val_accuracy: 0.6076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOEXSkrFkzGq",
        "colab_type": "code",
        "outputId": "d5029fb4-727e-4376-b6ba-3b5321f36d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "red_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 8ms/step - loss: 1.3181 - accuracy: 0.6076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3181103467941284, 0.6075999736785889]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIukU2r9k96-",
        "colab_type": "code",
        "outputId": "0f319718-45b6-4d4b-d554-39b8fb80977f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "red_model.evaluate(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98/98 [==============================] - 1s 8ms/step - loss: 0.2395 - accuracy: 0.9375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23954753577709198, 0.9375]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NRx8htDGFRf",
        "colab_type": "code",
        "outputId": "f95c3fa6-b5c3-41a9-953c-af31d8f7b819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1560\n",
        "history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                                      epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                      validation_data=(x_test, y_test), callbacks=[cp_callback, warm_up_lr])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 2.4742 - accuracy: 0.0964\n",
            "Epoch 00001: saving model to /content\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 2.4742 - accuracy: 0.0964 - val_loss: 2.3012 - val_accuracy: 0.1054\n",
            "Epoch 2/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 2.3173 - accuracy: 0.1397\n",
            "Epoch 00002: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 2.3132 - accuracy: 0.1430 - val_loss: 2.3047 - val_accuracy: 0.1000\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 2.1971 - accuracy: 0.1805\n",
            "Epoch 00003: saving model to /content\n",
            "24/24 [==============================] - 4s 156ms/step - loss: 2.1971 - accuracy: 0.1805 - val_loss: 2.3336 - val_accuracy: 0.0996\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 2.1040 - accuracy: 0.2273\n",
            "Epoch 00004: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 2.1040 - accuracy: 0.2273 - val_loss: 2.4712 - val_accuracy: 0.1000\n",
            "Epoch 5/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 2.0678 - accuracy: 0.2430\n",
            "Epoch 00005: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 2.0718 - accuracy: 0.2380 - val_loss: 2.7146 - val_accuracy: 0.1000\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.9951 - accuracy: 0.2694\n",
            "Epoch 00006: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.9951 - accuracy: 0.2694 - val_loss: 3.0059 - val_accuracy: 0.1000\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.9503 - accuracy: 0.2901\n",
            "Epoch 00007: saving model to /content\n",
            "24/24 [==============================] - 4s 156ms/step - loss: 1.9503 - accuracy: 0.2901 - val_loss: 3.0628 - val_accuracy: 0.1000\n",
            "Epoch 8/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.9092 - accuracy: 0.2919\n",
            "Epoch 00008: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.9127 - accuracy: 0.2914 - val_loss: 3.3212 - val_accuracy: 0.1005\n",
            "Epoch 9/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.8543 - accuracy: 0.3331\n",
            "Epoch 00009: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.8599 - accuracy: 0.3295 - val_loss: 2.9814 - val_accuracy: 0.0946\n",
            "Epoch 10/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.8549 - accuracy: 0.3303\n",
            "Epoch 00010: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.8555 - accuracy: 0.3316 - val_loss: 2.3823 - val_accuracy: 0.1723\n",
            "Epoch 11/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.8482 - accuracy: 0.3142\n",
            "Epoch 00011: saving model to /content\n",
            "24/24 [==============================] - 4s 157ms/step - loss: 1.8386 - accuracy: 0.3202 - val_loss: 2.4212 - val_accuracy: 0.1669\n",
            "Epoch 12/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.7992 - accuracy: 0.3415\n",
            "Epoch 00012: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.8022 - accuracy: 0.3402 - val_loss: 2.5997 - val_accuracy: 0.1295\n",
            "Epoch 13/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.7797 - accuracy: 0.3534\n",
            "Epoch 00013: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.7838 - accuracy: 0.3529 - val_loss: 2.6164 - val_accuracy: 0.1339\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.7379 - accuracy: 0.3650\n",
            "Epoch 00014: saving model to /content\n",
            "24/24 [==============================] - 4s 156ms/step - loss: 1.7379 - accuracy: 0.3650 - val_loss: 2.6461 - val_accuracy: 0.1438\n",
            "Epoch 15/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.7188 - accuracy: 0.3708\n",
            "Epoch 00015: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.7205 - accuracy: 0.3683 - val_loss: 2.4365 - val_accuracy: 0.2197\n",
            "Epoch 16/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.6673 - accuracy: 0.4092\n",
            "Epoch 00016: saving model to /content\n",
            "24/24 [==============================] - 4s 157ms/step - loss: 1.6584 - accuracy: 0.4118 - val_loss: 2.1991 - val_accuracy: 0.2201\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.6855 - accuracy: 0.3877\n",
            "Epoch 00017: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 1.6855 - accuracy: 0.3877 - val_loss: 2.3922 - val_accuracy: 0.2206\n",
            "Epoch 18/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.6553 - accuracy: 0.4085\n",
            "Epoch 00018: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 1.6624 - accuracy: 0.4024 - val_loss: 1.8876 - val_accuracy: 0.3157\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.6511 - accuracy: 0.3890\n",
            "Epoch 00019: saving model to /content\n",
            "24/24 [==============================] - 4s 159ms/step - loss: 1.6511 - accuracy: 0.3890 - val_loss: 2.0931 - val_accuracy: 0.2854\n",
            "Epoch 20/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.6117 - accuracy: 0.4127\n",
            "Epoch 00020: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.6100 - accuracy: 0.4104 - val_loss: 1.8158 - val_accuracy: 0.3406\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.5788 - accuracy: 0.4338\n",
            "Epoch 00021: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 1.5788 - accuracy: 0.4338 - val_loss: 1.7582 - val_accuracy: 0.3565\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.5645 - accuracy: 0.4412\n",
            "Epoch 00022: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.5645 - accuracy: 0.4412 - val_loss: 2.0846 - val_accuracy: 0.3294\n",
            "Epoch 23/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.5145 - accuracy: 0.4546\n",
            "Epoch 00023: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.5148 - accuracy: 0.4566 - val_loss: 2.0518 - val_accuracy: 0.3297\n",
            "Epoch 24/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.4413\n",
            "Epoch 00024: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.5130 - accuracy: 0.4432 - val_loss: 1.6626 - val_accuracy: 0.4084\n",
            "Epoch 25/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.5329 - accuracy: 0.4518\n",
            "Epoch 00025: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 1.5390 - accuracy: 0.4499 - val_loss: 1.5656 - val_accuracy: 0.4359\n",
            "Epoch 26/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.4577 - accuracy: 0.4613\n",
            "Epoch 00026: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.4530 - accuracy: 0.4668 - val_loss: 2.0090 - val_accuracy: 0.3344\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.4727 - accuracy: 0.4719\n",
            "Epoch 00027: saving model to /content\n",
            "24/24 [==============================] - 4s 170ms/step - loss: 1.4727 - accuracy: 0.4719 - val_loss: 1.8492 - val_accuracy: 0.3762\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.4502 - accuracy: 0.4739\n",
            "Epoch 00028: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.4502 - accuracy: 0.4739 - val_loss: 1.7033 - val_accuracy: 0.3863\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.4331 - accuracy: 0.4719\n",
            "Epoch 00029: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.4331 - accuracy: 0.4719 - val_loss: 1.7273 - val_accuracy: 0.4025\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.4629 - accuracy: 0.4846\n",
            "Epoch 00030: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.4629 - accuracy: 0.4846 - val_loss: 1.8315 - val_accuracy: 0.3802\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.4322 - accuracy: 0.4773\n",
            "Epoch 00031: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.4322 - accuracy: 0.4773 - val_loss: 2.2309 - val_accuracy: 0.3232\n",
            "Epoch 32/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.3927 - accuracy: 0.4972\n",
            "Epoch 00032: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.3984 - accuracy: 0.4987 - val_loss: 1.7914 - val_accuracy: 0.3882\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.4049 - accuracy: 0.5007\n",
            "Epoch 00033: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.4049 - accuracy: 0.5007 - val_loss: 1.6985 - val_accuracy: 0.4123\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.3549 - accuracy: 0.5221\n",
            "Epoch 00034: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.3549 - accuracy: 0.5221 - val_loss: 1.9661 - val_accuracy: 0.3658\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.3967 - accuracy: 0.4940\n",
            "Epoch 00035: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.3967 - accuracy: 0.4940 - val_loss: 2.0491 - val_accuracy: 0.3485\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.3646 - accuracy: 0.5074\n",
            "Epoch 00036: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.3646 - accuracy: 0.5074 - val_loss: 1.6781 - val_accuracy: 0.4188\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.3279 - accuracy: 0.5114\n",
            "Epoch 00037: saving model to /content\n",
            "24/24 [==============================] - 4s 157ms/step - loss: 1.3279 - accuracy: 0.5114 - val_loss: 1.7975 - val_accuracy: 0.4048\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.2885 - accuracy: 0.5261\n",
            "Epoch 00038: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.2885 - accuracy: 0.5261 - val_loss: 1.7456 - val_accuracy: 0.4197\n",
            "Epoch 39/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.2972 - accuracy: 0.5175\n",
            "Epoch 00039: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.3061 - accuracy: 0.5120 - val_loss: 1.7706 - val_accuracy: 0.4130\n",
            "Epoch 40/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.3360 - accuracy: 0.5314\n",
            "Epoch 00040: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.3342 - accuracy: 0.5307 - val_loss: 1.7462 - val_accuracy: 0.4405\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.3135 - accuracy: 0.5234\n",
            "Epoch 00041: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.3135 - accuracy: 0.5234 - val_loss: 1.7560 - val_accuracy: 0.4314\n",
            "Epoch 42/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.2105 - accuracy: 0.5587\n",
            "Epoch 00042: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 1.2082 - accuracy: 0.5575 - val_loss: 1.8900 - val_accuracy: 0.3809\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.2417 - accuracy: 0.5528\n",
            "Epoch 00043: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 1.2417 - accuracy: 0.5528 - val_loss: 1.7754 - val_accuracy: 0.4307\n",
            "Epoch 44/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.2226 - accuracy: 0.5468\n",
            "Epoch 00044: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.2123 - accuracy: 0.5535 - val_loss: 1.6831 - val_accuracy: 0.4441\n",
            "Epoch 45/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.1966 - accuracy: 0.5615\n",
            "Epoch 00045: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.1952 - accuracy: 0.5622 - val_loss: 1.6168 - val_accuracy: 0.4632\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.1456 - accuracy: 0.5889\n",
            "Epoch 00046: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 1.1456 - accuracy: 0.5889 - val_loss: 1.8293 - val_accuracy: 0.4438\n",
            "Epoch 47/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.1636 - accuracy: 0.5852\n",
            "Epoch 00047: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.1576 - accuracy: 0.5869 - val_loss: 1.6932 - val_accuracy: 0.4582\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.2164 - accuracy: 0.5655\n",
            "Epoch 00048: saving model to /content\n",
            "24/24 [==============================] - 4s 159ms/step - loss: 1.2164 - accuracy: 0.5655 - val_loss: 1.7040 - val_accuracy: 0.4466\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.1326 - accuracy: 0.5949\n",
            "Epoch 00049: saving model to /content\n",
            "24/24 [==============================] - 4s 160ms/step - loss: 1.1326 - accuracy: 0.5949 - val_loss: 1.7760 - val_accuracy: 0.4463\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.1167 - accuracy: 0.5809\n",
            "Epoch 00050: saving model to /content\n",
            "24/24 [==============================] - 4s 156ms/step - loss: 1.1167 - accuracy: 0.5809 - val_loss: 1.7671 - val_accuracy: 0.4205\n",
            "Epoch 51/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.0666 - accuracy: 0.6162\n",
            "Epoch 00051: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.0686 - accuracy: 0.6133 - val_loss: 1.5260 - val_accuracy: 0.4848\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.0497 - accuracy: 0.6250\n",
            "Epoch 00052: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.0497 - accuracy: 0.6250 - val_loss: 1.6764 - val_accuracy: 0.4658\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.1063 - accuracy: 0.5836\n",
            "Epoch 00053: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 1.1063 - accuracy: 0.5836 - val_loss: 1.7605 - val_accuracy: 0.4428\n",
            "Epoch 54/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.0652 - accuracy: 0.6271\n",
            "Epoch 00054: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 1.0638 - accuracy: 0.6270 - val_loss: 1.6905 - val_accuracy: 0.4708\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.0226 - accuracy: 0.6303\n",
            "Epoch 00055: saving model to /content\n",
            "24/24 [==============================] - 4s 168ms/step - loss: 1.0226 - accuracy: 0.6303 - val_loss: 2.1429 - val_accuracy: 0.4031\n",
            "Epoch 56/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.0066 - accuracy: 0.6320\n",
            "Epoch 00056: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.9994 - accuracy: 0.6350 - val_loss: 1.7729 - val_accuracy: 0.4551\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.0467 - accuracy: 0.6203\n",
            "Epoch 00057: saving model to /content\n",
            "24/24 [==============================] - 4s 156ms/step - loss: 1.0467 - accuracy: 0.6203 - val_loss: 1.9271 - val_accuracy: 0.4548\n",
            "Epoch 58/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.9920 - accuracy: 0.6425\n",
            "Epoch 00058: saving model to /content\n",
            "24/24 [==============================] - 4s 156ms/step - loss: 0.9932 - accuracy: 0.6404 - val_loss: 1.7598 - val_accuracy: 0.4532\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9612 - accuracy: 0.6524\n",
            "Epoch 00059: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.9612 - accuracy: 0.6524 - val_loss: 1.9355 - val_accuracy: 0.4240\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9444 - accuracy: 0.6471\n",
            "Epoch 00060: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.9444 - accuracy: 0.6471 - val_loss: 1.6097 - val_accuracy: 0.4864\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9373 - accuracy: 0.6531\n",
            "Epoch 00061: saving model to /content\n",
            "24/24 [==============================] - 4s 156ms/step - loss: 0.9373 - accuracy: 0.6531 - val_loss: 1.7473 - val_accuracy: 0.4578\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9240 - accuracy: 0.6698\n",
            "Epoch 00062: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.9240 - accuracy: 0.6698 - val_loss: 1.7677 - val_accuracy: 0.4661\n",
            "Epoch 63/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.9425 - accuracy: 0.6592\n",
            "Epoch 00063: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.9375 - accuracy: 0.6611 - val_loss: 1.8525 - val_accuracy: 0.4600\n",
            "Epoch 64/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.8933 - accuracy: 0.6795\n",
            "Epoch 00064: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.8947 - accuracy: 0.6798 - val_loss: 1.7833 - val_accuracy: 0.4780\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9268 - accuracy: 0.6604\n",
            "Epoch 00065: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.9268 - accuracy: 0.6604 - val_loss: 1.6525 - val_accuracy: 0.4921\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9094 - accuracy: 0.6651\n",
            "Epoch 00066: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.9094 - accuracy: 0.6651 - val_loss: 1.6278 - val_accuracy: 0.4915\n",
            "Epoch 67/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.8648 - accuracy: 0.6865\n",
            "Epoch 00067: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.8647 - accuracy: 0.6878 - val_loss: 1.6328 - val_accuracy: 0.4958\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8050 - accuracy: 0.7059\n",
            "Epoch 00068: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.8050 - accuracy: 0.7059 - val_loss: 1.8774 - val_accuracy: 0.4709\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8123 - accuracy: 0.6979\n",
            "Epoch 00069: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.8123 - accuracy: 0.6979 - val_loss: 1.7573 - val_accuracy: 0.4809\n",
            "Epoch 70/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.8277 - accuracy: 0.7067\n",
            "Epoch 00070: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.8273 - accuracy: 0.7045 - val_loss: 1.7447 - val_accuracy: 0.4889\n",
            "Epoch 71/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7508 - accuracy: 0.7374\n",
            "Epoch 00071: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.7505 - accuracy: 0.7373 - val_loss: 1.6342 - val_accuracy: 0.5044\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8051 - accuracy: 0.7112\n",
            "Epoch 00072: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.8051 - accuracy: 0.7112 - val_loss: 1.6977 - val_accuracy: 0.4903\n",
            "Epoch 73/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.8313 - accuracy: 0.6955\n",
            "Epoch 00073: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.8351 - accuracy: 0.6952 - val_loss: 1.7495 - val_accuracy: 0.4913\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.7126\n",
            "Epoch 00074: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.7952 - accuracy: 0.7126 - val_loss: 1.7007 - val_accuracy: 0.4989\n",
            "Epoch 75/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7870 - accuracy: 0.7086\n",
            "Epoch 00075: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.7771 - accuracy: 0.7112 - val_loss: 1.7020 - val_accuracy: 0.5016\n",
            "Epoch 76/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7640 - accuracy: 0.7147\n",
            "Epoch 00076: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.7606 - accuracy: 0.7168 - val_loss: 1.6388 - val_accuracy: 0.5067\n",
            "Epoch 77/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7665 - accuracy: 0.7221\n",
            "Epoch 00077: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.7674 - accuracy: 0.7193 - val_loss: 1.6333 - val_accuracy: 0.5086\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7785 - accuracy: 0.7079\n",
            "Epoch 00078: saving model to /content\n",
            "24/24 [==============================] - 4s 156ms/step - loss: 0.7785 - accuracy: 0.7079 - val_loss: 1.6240 - val_accuracy: 0.5113\n",
            "Epoch 79/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7128 - accuracy: 0.7291\n",
            "Epoch 00079: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.7190 - accuracy: 0.7273 - val_loss: 1.6539 - val_accuracy: 0.5055\n",
            "Epoch 80/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7116 - accuracy: 0.7500\n",
            "Epoch 00080: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.7073 - accuracy: 0.7527 - val_loss: 1.6649 - val_accuracy: 0.5125\n",
            "Epoch 81/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7260 - accuracy: 0.7339\n",
            "Epoch 00081: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.7206 - accuracy: 0.7353 - val_loss: 1.6689 - val_accuracy: 0.5102\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7167 - accuracy: 0.7340\n",
            "Epoch 00082: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.7167 - accuracy: 0.7340 - val_loss: 1.6259 - val_accuracy: 0.5183\n",
            "Epoch 83/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6772 - accuracy: 0.7549\n",
            "Epoch 00083: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.6702 - accuracy: 0.7580 - val_loss: 1.6299 - val_accuracy: 0.5167\n",
            "Epoch 84/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7070 - accuracy: 0.7402\n",
            "Epoch 00084: saving model to /content\n",
            "24/24 [==============================] - 4s 169ms/step - loss: 0.7060 - accuracy: 0.7393 - val_loss: 1.6524 - val_accuracy: 0.5105\n",
            "Epoch 85/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6944 - accuracy: 0.7395\n",
            "Epoch 00085: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.6923 - accuracy: 0.7400 - val_loss: 1.6811 - val_accuracy: 0.5059\n",
            "Epoch 86/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6909 - accuracy: 0.7479\n",
            "Epoch 00086: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.6837 - accuracy: 0.7513 - val_loss: 1.6937 - val_accuracy: 0.5036\n",
            "Epoch 87/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7018 - accuracy: 0.7409\n",
            "Epoch 00087: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.6968 - accuracy: 0.7420 - val_loss: 1.6726 - val_accuracy: 0.5056\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6502 - accuracy: 0.7701\n",
            "Epoch 00088: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.6502 - accuracy: 0.7701 - val_loss: 1.6731 - val_accuracy: 0.5080\n",
            "Epoch 89/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.7367\n",
            "Epoch 00089: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.6852 - accuracy: 0.7373 - val_loss: 1.6846 - val_accuracy: 0.5077\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.7647\n",
            "Epoch 00090: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.6467 - accuracy: 0.7647 - val_loss: 1.6551 - val_accuracy: 0.5111\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6465 - accuracy: 0.7627\n",
            "Epoch 00091: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.6465 - accuracy: 0.7627 - val_loss: 1.6564 - val_accuracy: 0.5131\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6295 - accuracy: 0.7767\n",
            "Epoch 00092: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.6295 - accuracy: 0.7767 - val_loss: 1.6751 - val_accuracy: 0.5086\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6340 - accuracy: 0.7640\n",
            "Epoch 00093: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.6340 - accuracy: 0.7640 - val_loss: 1.6796 - val_accuracy: 0.5093\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6756 - accuracy: 0.7373\n",
            "Epoch 00094: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.6756 - accuracy: 0.7373 - val_loss: 1.6711 - val_accuracy: 0.5117\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6314 - accuracy: 0.7694\n",
            "Epoch 00095: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.6314 - accuracy: 0.7694 - val_loss: 1.6721 - val_accuracy: 0.5130\n",
            "Epoch 96/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6225 - accuracy: 0.7703\n",
            "Epoch 00096: saving model to /content\n",
            "24/24 [==============================] - 4s 155ms/step - loss: 0.6166 - accuracy: 0.7727 - val_loss: 1.6644 - val_accuracy: 0.5154\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6334 - accuracy: 0.7660\n",
            "Epoch 00097: saving model to /content\n",
            "24/24 [==============================] - 4s 154ms/step - loss: 0.6334 - accuracy: 0.7660 - val_loss: 1.6628 - val_accuracy: 0.5157\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6423 - accuracy: 0.7460\n",
            "Epoch 00098: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.6423 - accuracy: 0.7460 - val_loss: 1.6618 - val_accuracy: 0.5159\n",
            "Epoch 99/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6650 - accuracy: 0.7542\n",
            "Epoch 00099: saving model to /content\n",
            "24/24 [==============================] - 4s 153ms/step - loss: 0.6640 - accuracy: 0.7547 - val_loss: 1.6624 - val_accuracy: 0.5153\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6619 - accuracy: 0.7467\n",
            "Epoch 00100: saving model to /content\n",
            "24/24 [==============================] - 4s 158ms/step - loss: 0.6619 - accuracy: 0.7467 - val_loss: 1.6618 - val_accuracy: 0.5157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5dOAqiTGHp4",
        "colab_type": "code",
        "outputId": "cd54d717-04e5-4eb2-86ab-b53d822a5e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "red_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 8ms/step - loss: 1.6618 - accuracy: 0.5157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6617707014083862, 0.5156999826431274]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEvoC-xEGIDS",
        "colab_type": "code",
        "outputId": "d7789368-a2ab-4436-e8d1-81310d9c4ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "red_model.evaluate(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49/49 [==============================] - 0s 7ms/step - loss: 0.2087 - accuracy: 0.9429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.208671435713768, 0.9429486989974976]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QXETcQDnmcC",
        "colab_type": "code",
        "outputId": "c13ceba8-1d37-4b3e-aec9-ee9ce59281eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "a = [92.01, 86.75, 80.89, 71.53, 60.76, 51.57]\n",
        "b = [50000, 25000, 12500, 6250, 3120, 1560]\n",
        "plt.plot(b, a)\n",
        "plt.xticks(np.arange(10000, 50001, 10000))\n",
        "ax = plt.gca()\n",
        "ax.invert_xaxis()\n",
        "plt.title(\"Test Accuracy Against # of Training Sample\")\n",
        "plt.xlabel(\"Number of Training Sample\")\n",
        "plt.ylabel(\"Accuracy\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}