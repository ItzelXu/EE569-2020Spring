{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10 NEW",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hztcr8m_XrmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation, Conv2D, BatchNormalization, Add, ZeroPadding2D, Input, Dense, MaxPooling2D\n",
        "\n",
        "class ResBlock:\n",
        "    def __init__(self, out_channels, in_channels=None, strides=1):\n",
        "        self.out_channels = out_channels\n",
        "        self.in_channels = in_channels if in_channels else out_channels\n",
        "        self.strides = strides\n",
        "        \n",
        "        \n",
        "    def __call__(self, x):    \n",
        "        res = BatchNormalization()(x)\n",
        "        res_1 = Activation('relu')(res)\n",
        "        \n",
        "        res = Conv2D(filters=self.out_channels//4, kernel_size=(1,1), strides=1, use_bias=False)(res_1)\n",
        "        res = BatchNormalization()(res)\n",
        "        res = Activation('relu')(res)\n",
        "        # res = ZeroPadding2D(padding=(1,1))(res)\n",
        "        res = Conv2D(filters=self.out_channels//4, kernel_size=(3,3), strides=self.strides, padding='same')(res)\n",
        "        res = BatchNormalization()(res)\n",
        "        res = Activation('relu')(res)\n",
        "        res = Conv2D(filters=self.out_channels, kernel_size=(1,1), strides=1, use_bias=False)(res)\n",
        "        \n",
        "        if self.strides != 1 or self.in_channels != self.out_channels:\n",
        "            identity = Conv2D(filters=self.out_channels, kernel_size=(1,1), strides=self.strides, use_bias=False)(res_1)\n",
        "        else:\n",
        "            identity = x\n",
        "        output = Add()([identity, res])\n",
        "        return output\n",
        "    \n",
        "    \n",
        "class MaxPool2D:\n",
        "    def __init__(self, pool_size=(3,3), strides=2, padding=(1,1)):\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        \n",
        "        \n",
        "    def __call__(self, x):\n",
        "        # padd = ZeroPadding2D(padding=self.padding)(x)\n",
        "        # pool = MaxPooling2D(pool_size=self.pool_size, strides=self.strides, padding='same')(padd)\n",
        "        pool = MaxPooling2D(pool_size=self.pool_size, strides=self.strides, padding='same')(x)\n",
        "        return pool\n",
        "\n",
        "class AvgPool2D:\n",
        "    def __init__(self, pool_size=(3,3), strides=2, padding=(1,1)):\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        \n",
        "        \n",
        "    def __call__(self, x):\n",
        "        padd = ZeroPadding2D(padding=self.padding)(x)\n",
        "        pool = AveragePooling2D(pool_size=self.pool_size, strides=self.strides, padding='valid')(padd)\n",
        "        return pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BksVI2rMXyp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# from basic_module import ResBlock, MaxPool2D\n",
        "from tensorflow.keras.layers import Activation, Conv2D, BatchNormalization, Add, ZeroPadding2D, MaxPooling2D, UpSampling2D, Multiply, AveragePooling2D, Flatten, Dropout\n",
        "    \n",
        "\n",
        "class AttentionModule_stage1:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        \n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "\n",
        "        out_mpool1 = MaxPool2D()(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_mpool1)\n",
        "        out_skip1_connection = ResBlock(channels)(out_softmax1)\n",
        "\n",
        "        out_mpool2 = MaxPool2D()(out_softmax1)\n",
        "        out_softmax2 = ResBlock(channels)(out_mpool2)\n",
        "        out_softmax2 = ResBlock(channels)(out_softmax2)\n",
        "\n",
        "        out_interp2 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax2), out_softmax1])\n",
        "        out = Add()([out_interp2, out_skip1_connection])\n",
        "\n",
        "        out_softmax3 = ResBlock(channels)(out)\n",
        "        out_interp1 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax3), out_trunk])\n",
        "\n",
        "        out_softmax4 = BatchNormalization()(out_interp1)\n",
        "        out_softmax4 = Activation('relu')(out_softmax4)\n",
        "        out_softmax4 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax4)\n",
        "        out_softmax4 = BatchNormalization()(out_softmax4)\n",
        "        out_softmax4 = Activation('relu')(out_softmax4)\n",
        "        out_softmax4 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax4)\n",
        "        out_softmax4 = Activation('sigmoid')(out_softmax4)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax4), out_softmax4])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    \n",
        "    \n",
        "class AttentionModule_stage2:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        \n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "\n",
        "        out_mpool1 = MaxPool2D()(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_mpool1)\n",
        "        out_softmax1 = ResBlock(channels)(out_softmax1)\n",
        "\n",
        "        out_interp1 = Add()([UpSampling2D(interpolation='bilinear')(out_softmax1), out_trunk])\n",
        "        \n",
        "        out_softmax2 = BatchNormalization()(out_interp1)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = BatchNormalization()(out_softmax2)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = Activation('sigmoid')(out_softmax2)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax2), out_softmax2])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    \n",
        "    \n",
        "class AttentionModule_stage3:\n",
        "    def __init__(self, channels):\n",
        "        self.channels = channels\n",
        "    \n",
        "    \n",
        "    def __call__(self, x):\n",
        "        channels = self.channels\n",
        "        \n",
        "        x = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(x)\n",
        "        out_trunk = ResBlock(channels)(out_trunk)\n",
        "                \n",
        "        out_softmax1 = ResBlock(channels)(x)\n",
        "        out_softmax1 = ResBlock(channels)(out_softmax1)\n",
        "        \n",
        "        out_softmax2 = BatchNormalization()(out_softmax1)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = BatchNormalization()(out_softmax2)\n",
        "        out_softmax2 = Activation('relu')(out_softmax2)\n",
        "        out_softmax2 = Conv2D(channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(out_softmax2)\n",
        "        out_softmax2 = Activation('sigmoid')(out_softmax2)\n",
        "        \n",
        "        out = Add()([tf.ones_like(out_softmax2), out_softmax2])\n",
        "        out = Multiply()([out, out_trunk])\n",
        "\n",
        "        out_last = ResBlock(channels)(out)\n",
        "\n",
        "        return out_last\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI86QNHDYDjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualAttentionModel_32input_red:\n",
        "\n",
        "    def __init__(self, classes=10):\n",
        "        self.classes = classes\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        classes = self.classes\n",
        "                \n",
        "        # x = ZeroPadding2D(padding=(3,3))(x)\n",
        "        x = Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', use_bias=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        \n",
        "        # x = MaxPool2D()(x)\n",
        "        x = ResBlock(32, in_channels=32)(x)\n",
        "        x = AttentionModule_stage1(32)(x)\n",
        "        x = ResBlock(64, in_channels=32, strides=2)(x)\n",
        "        x = AttentionModule_stage2(64)(x)\n",
        "        x = ResBlock(128, in_channels=64, strides=2)(x)\n",
        "        x = AttentionModule_stage3(128)(x)\n",
        "        x = ResBlock(256, in_channels=128)(x)\n",
        "#        x = ResBlock(1024)(x)\n",
        "        x = ResBlock(256)(x)\n",
        "        \n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = AvgPool2D(pool_size=(8,8), strides=1, padding=(0,0))(x)\n",
        "        \n",
        "        x = Conv2D(classes, kernel_size=(1,1))(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H2lIg-uX0kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def cosine_decay_warmup(global_step, lr_base, num_steps, warmup_steps=0):\n",
        "\n",
        "    lr = 0.5 * lr_base * (1 + np.cos( np.pi *\n",
        "        (global_step - warmup_steps) / float(num_steps - warmup_steps)))\n",
        "    if warmup_steps > 0:\n",
        "        slope = lr_base / warmup_steps\n",
        "        lr = np.where(global_step < warmup_steps, slope * global_step, lr)\n",
        "    return np.where(global_step > num_steps, 0.0, lr)\n",
        "\n",
        "\n",
        "class Scheduler(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, lr_base, num_epochs,\n",
        "                 warmup_epochs=0, num_sample = 50000, batch_size=64):\n",
        "\n",
        "        super(Scheduler, self).__init__()\n",
        "        self.lr_base = lr_base\n",
        "        self.num_steps = int(num_epochs * num_sample / batch_size)\n",
        "        self.global_step = 0\n",
        "        self.warmup_steps = int(warmup_epochs * num_sample / batch_size)\n",
        "\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.global_step = self.global_step + 1\n",
        "\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        lr = cosine_decay_warmup(global_step=self.global_step,\n",
        "                                 lr_base=self.lr_base,\n",
        "                                 num_steps=self.num_steps,\n",
        "                                 warmup_steps=self.warmup_steps,\n",
        "                                 )\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "num_sample = 50000\n",
        "epochs = 250\n",
        "batch_size = 64\n",
        "warmup_epochs = 25\n",
        "lr_base = 0.001\n",
        "\n",
        "\n",
        "warm_up_lr = Scheduler(lr_base=lr_base,\n",
        "                       num_epochs=epochs,\n",
        "                       warmup_epochs=warmup_epochs, \n",
        "                       num_sample = num_sample, \n",
        "                       batch_size=batch_size,\n",
        "                       )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKqb_TW9X7Q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = '/content'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yMTeUCvX84D",
        "colab_type": "code",
        "outputId": "90561294-bdda-4690-8587-dea808e50c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "model_in = Input((32, 32, 3))\n",
        "\n",
        "model_out = ResidualAttentionModel_32input_red()(model_in)\n",
        "model_out = Dropout(0.2)(model_out)\n",
        "model_out = Activation('softmax')(model_out)\n",
        "\n",
        "red_model = Model(inputs=model_in, outputs=model_out)\n",
        "\n",
        "red_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "red_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 32, 32, 32)   864         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 32, 32, 32)   128         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 32, 32, 32)   0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 32, 32, 32)   128         activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 32, 32, 32)   0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 32, 32, 8)    256         activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 32, 32, 8)    32          conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 32, 32, 8)    0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 32, 32, 8)    584         activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 32, 32, 8)    32          conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 32, 32, 8)    0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 32, 32, 32)   256         activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_66 (Add)                    (None, 32, 32, 32)   0           activation_180[0][0]             \n",
            "                                                                 conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 32, 32, 32)   128         add_66[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 32, 32, 32)   0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 32, 32, 8)    256         activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 32, 32, 8)    32          conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 32, 32, 8)    0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 32, 32, 8)    584         activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 32, 32, 8)    32          conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 32, 32, 8)    0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 32, 32, 32)   256         activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_67 (Add)                    (None, 32, 32, 32)   0           add_66[0][0]                     \n",
            "                                                                 conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 32)   0           add_67[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 16, 16, 32)   128         max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 16, 16, 32)   0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 16, 16, 8)    256         activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 16, 16, 8)    32          conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 16, 16, 8)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 16, 16, 8)    584         activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 16, 16, 8)    32          conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 16, 16, 8)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 16, 16, 32)   256         activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_70 (Add)                    (None, 16, 16, 32)   0           max_pooling2d_6[0][0]            \n",
            "                                                                 conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 32)     0           add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 8, 8, 32)     128         max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 8, 8, 32)     0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 8, 8, 8)      256         activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 8, 8, 8)      32          conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 8, 8, 8)      0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 8, 8, 8)      584         activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 8, 8, 8)      32          conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 8, 8, 8)      0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 8, 8, 32)     256         activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_72 (Add)                    (None, 8, 8, 32)     0           max_pooling2d_7[0][0]            \n",
            "                                                                 conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 8, 8, 32)     128         add_72[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 8, 8, 32)     0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 8, 8, 8)      256         activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 16, 16, 32)   128         add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 8, 8, 8)      32          conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 16, 16, 32)   0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 8, 8, 8)      0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 16, 16, 8)    256         activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 32, 32, 32)   128         add_67[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 8, 8, 8)      584         activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 16, 16, 8)    32          conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 32, 32, 32)   0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 8, 8, 8)      32          conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 16, 16, 8)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 32, 32, 8)    256         activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 8, 8, 8)      0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 16, 16, 8)    584         activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 32, 32, 8)    32          conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 8, 8, 32)     256         activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 16, 16, 8)    32          conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 32, 32, 8)    0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_73 (Add)                    (None, 8, 8, 32)     0           add_72[0][0]                     \n",
            "                                                                 conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 16, 16, 8)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 32, 32, 8)    584         activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 16, 16, 32)   0           add_73[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 16, 16, 32)   256         activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 32, 32, 8)    32          conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_74 (Add)                    (None, 16, 16, 32)   0           up_sampling2d_6[0][0]            \n",
            "                                                                 add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_71 (Add)                    (None, 16, 16, 32)   0           add_70[0][0]                     \n",
            "                                                                 conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 32, 32, 8)    0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_75 (Add)                    (None, 16, 16, 32)   0           add_74[0][0]                     \n",
            "                                                                 add_71[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 32, 32, 32)   256         activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 16, 16, 32)   128         add_75[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_68 (Add)                    (None, 32, 32, 32)   0           add_67[0][0]                     \n",
            "                                                                 conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 16, 16, 32)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 32, 32, 32)   128         add_68[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 16, 16, 8)    256         activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 32, 32, 32)   0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 16, 16, 8)    32          conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 32, 32, 8)    256         activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 16, 16, 8)    0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 32, 32, 8)    32          conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 16, 16, 8)    584         activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 32, 32, 8)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 16, 16, 8)    32          conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 32, 32, 8)    584         activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 16, 16, 8)    0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 32, 32, 8)    32          conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 16, 16, 32)   256         activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 32, 32, 8)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_76 (Add)                    (None, 16, 16, 32)   0           add_75[0][0]                     \n",
            "                                                                 conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 32, 32, 32)   256         activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 32, 32, 32)   0           add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_69 (Add)                    (None, 32, 32, 32)   0           add_68[0][0]                     \n",
            "                                                                 conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_77 (Add)                    (None, 32, 32, 32)   0           up_sampling2d_7[0][0]            \n",
            "                                                                 add_69[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 32, 32, 32)   128         add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 32, 32, 32)   0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 32, 32, 32)   1024        activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 32, 32, 32)   128         conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 32, 32, 32)   0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 32, 32, 32)   1024        activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 32, 32, 32)   0           conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_6 (TensorFlow [(4,)]               0           activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_6 (TensorFlowO [(None, 32, 32, 32)] 0           tf_op_layer_Shape_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_78 (Add)                    (None, 32, 32, 32)   0           tf_op_layer_Fill_6[0][0]         \n",
            "                                                                 activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_6 (Multiply)           (None, 32, 32, 32)   0           add_78[0][0]                     \n",
            "                                                                 add_69[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 32, 32, 32)   128         multiply_6[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 32, 32, 32)   0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 32, 32, 8)    256         activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 32, 32, 8)    32          conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 32, 32, 8)    0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 32, 32, 8)    584         activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 32, 32, 8)    32          conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 32, 32, 8)    0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 32, 32, 32)   256         activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_79 (Add)                    (None, 32, 32, 32)   0           multiply_6[0][0]                 \n",
            "                                                                 conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 32, 32, 32)   128         add_79[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 32, 32, 32)   0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 32, 32, 16)   512         activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 32, 32, 16)   64          conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 32, 32, 16)   0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 16, 16, 16)   2320        activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 16, 16, 16)   64          conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 16, 16, 16)   0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 16, 16, 64)   2048        activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 16, 16, 64)   1024        activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_80 (Add)                    (None, 16, 16, 64)   0           conv2d_214[0][0]                 \n",
            "                                                                 conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 16, 16, 64)   256         add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 16, 16, 64)   0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 16, 16, 16)   1024        activation_217[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 16, 16, 16)   64          conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 16, 16, 16)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 16, 16, 16)   2320        activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 16, 16, 16)   64          conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 16, 16, 16)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 16, 16, 64)   1024        activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_81 (Add)                    (None, 16, 16, 64)   0           add_80[0][0]                     \n",
            "                                                                 conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 8, 8, 64)     0           add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 8, 8, 64)     256         max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 8, 8, 64)     0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 16, 16, 64)   256         add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 8, 8, 16)     1024        activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 16, 16, 64)   0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 8, 8, 16)     64          conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 16, 16, 16)   1024        activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 8, 8, 16)     0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 16, 16, 16)   64          conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 8, 8, 16)     2320        activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 16, 16, 16)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 8, 8, 16)     64          conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 16, 16, 16)   2320        activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 8, 8, 16)     0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 16, 16, 16)   64          conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 8, 8, 64)     1024        activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 16, 16, 16)   0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_84 (Add)                    (None, 8, 8, 64)     0           max_pooling2d_8[0][0]            \n",
            "                                                                 conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 16, 16, 64)   1024        activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 8, 8, 64)     256         add_84[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_82 (Add)                    (None, 16, 16, 64)   0           add_81[0][0]                     \n",
            "                                                                 conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 8, 8, 64)     0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 16, 16, 64)   256         add_82[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 8, 8, 16)     1024        activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 16, 16, 64)   0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 8, 8, 16)     64          conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 16, 16, 16)   1024        activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 8, 8, 16)     0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 16, 16, 16)   64          conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 8, 8, 16)     2320        activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 16, 16, 16)   0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 8, 8, 16)     64          conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 16, 16, 16)   2320        activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 8, 8, 16)     0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 16, 16, 16)   64          conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 8, 8, 64)     1024        activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 16, 16, 16)   0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_85 (Add)                    (None, 8, 8, 64)     0           add_84[0][0]                     \n",
            "                                                                 conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 16, 16, 64)   1024        activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_8 (UpSampling2D)  (None, 16, 16, 64)   0           add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_83 (Add)                    (None, 16, 16, 64)   0           add_82[0][0]                     \n",
            "                                                                 conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_86 (Add)                    (None, 16, 16, 64)   0           up_sampling2d_8[0][0]            \n",
            "                                                                 add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 16, 16, 64)   256         add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 16, 16, 64)   0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 16, 16, 64)   4096        activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 16, 16, 64)   256         conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 16, 16, 64)   0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 16, 16, 64)   4096        activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 16, 16, 64)   0           conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_7 (TensorFlow [(4,)]               0           activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_7 (TensorFlowO [(None, 16, 16, 64)] 0           tf_op_layer_Shape_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_87 (Add)                    (None, 16, 16, 64)   0           tf_op_layer_Fill_7[0][0]         \n",
            "                                                                 activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_7 (Multiply)           (None, 16, 16, 64)   0           add_87[0][0]                     \n",
            "                                                                 add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 16, 16, 64)   256         multiply_7[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 16, 16, 64)   0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 16, 16, 16)   1024        activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 16, 16, 16)   64          conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 16, 16, 16)   0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 16, 16, 16)   2320        activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 16, 16, 16)   64          conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 16, 16, 16)   0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 16, 16, 64)   1024        activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_88 (Add)                    (None, 16, 16, 64)   0           multiply_7[0][0]                 \n",
            "                                                                 conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 16, 16, 64)   256         add_88[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 16, 16, 64)   0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 16, 16, 32)   2048        activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 16, 16, 32)   128         conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 16, 16, 32)   0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 8, 8, 32)     9248        activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 8, 8, 32)     128         conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 8, 8, 32)     0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 8, 8, 128)    8192        activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 8, 8, 128)    4096        activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_89 (Add)                    (None, 8, 8, 128)    0           conv2d_238[0][0]                 \n",
            "                                                                 conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 8, 8, 128)    512         add_89[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 8, 8, 128)    0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 8, 8, 32)     4096        activation_241[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 8, 8, 32)     128         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 8, 8, 32)     0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 8, 8, 32)     9248        activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 8, 8, 32)     128         conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 8, 8, 32)     0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 8, 8, 128)    4096        activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_90 (Add)                    (None, 8, 8, 128)    0           add_89[0][0]                     \n",
            "                                                                 conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 8, 8, 128)    512         add_90[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 8, 8, 128)    0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 8, 8, 32)     4096        activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 8, 8, 32)     128         conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 8, 8, 32)     0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 8, 8, 32)     9248        activation_251[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 8, 8, 32)     128         conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 8, 8, 32)     0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 8, 8, 128)    4096        activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_93 (Add)                    (None, 8, 8, 128)    0           add_90[0][0]                     \n",
            "                                                                 conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 8, 8, 128)    512         add_93[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 8, 8, 128)    512         add_90[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 8, 8, 128)    0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 8, 8, 128)    0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 8, 8, 32)     4096        activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 8, 8, 32)     4096        activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 8, 8, 32)     128         conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 8, 8, 32)     128         conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 8, 8, 32)     0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 8, 8, 32)     0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 8, 8, 32)     9248        activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 8, 8, 32)     9248        activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 8, 8, 32)     128         conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 8, 8, 32)     128         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 8, 8, 32)     0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 8, 8, 32)     0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 8, 8, 128)    4096        activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 8, 8, 128)    4096        activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_94 (Add)                    (None, 8, 8, 128)    0           add_93[0][0]                     \n",
            "                                                                 conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_91 (Add)                    (None, 8, 8, 128)    0           add_90[0][0]                     \n",
            "                                                                 conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 8, 8, 128)    512         add_94[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 8, 8, 128)    512         add_91[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 8, 8, 128)    0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 8, 8, 128)    0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 8, 8, 128)    16384       activation_256[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 8, 8, 32)     4096        activation_247[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 8, 8, 128)    512         conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 8, 8, 32)     128         conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 8, 8, 128)    0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 8, 8, 32)     0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 8, 8, 128)    16384       activation_257[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 8, 8, 32)     9248        activation_248[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 8, 8, 128)    0           conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 8, 8, 32)     128         conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_8 (TensorFlow [(4,)]               0           activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 8, 8, 32)     0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_8 (TensorFlowO [(None, 8, 8, 128)]  0           tf_op_layer_Shape_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 8, 8, 128)    4096        activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_95 (Add)                    (None, 8, 8, 128)    0           tf_op_layer_Fill_8[0][0]         \n",
            "                                                                 activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_92 (Add)                    (None, 8, 8, 128)    0           add_91[0][0]                     \n",
            "                                                                 conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "multiply_8 (Multiply)           (None, 8, 8, 128)    0           add_95[0][0]                     \n",
            "                                                                 add_92[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 8, 8, 128)    512         multiply_8[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 8, 8, 128)    0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 8, 8, 32)     4096        activation_259[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 8, 8, 32)     128         conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 8, 8, 32)     0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 8, 8, 32)     9248        activation_260[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 8, 8, 32)     128         conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 8, 8, 32)     0           batch_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 8, 8, 128)    4096        activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_96 (Add)                    (None, 8, 8, 128)    0           multiply_8[0][0]                 \n",
            "                                                                 conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 8, 8, 128)    512         add_96[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 8, 8, 128)    0           batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 8, 8, 64)     8192        activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 8, 8, 64)     256         conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 8, 8, 64)     0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 8, 8, 64)     36928       activation_263[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 8, 8, 64)     256         conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 8, 8, 64)     0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 8, 8, 256)    32768       activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 8, 8, 256)    16384       activation_264[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_97 (Add)                    (None, 8, 8, 256)    0           conv2d_262[0][0]                 \n",
            "                                                                 conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 8, 8, 256)    1024        add_97[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 8, 8, 256)    0           batch_normalization_254[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 8, 8, 64)     16384       activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 8, 8, 64)     256         conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 8, 8, 64)     0           batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 8, 8, 64)     36928       activation_266[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_256 (BatchN (None, 8, 8, 64)     256         conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 8, 8, 64)     0           batch_normalization_256[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 8, 8, 256)    16384       activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_98 (Add)                    (None, 8, 8, 256)    0           add_97[0][0]                     \n",
            "                                                                 conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_257 (BatchN (None, 8, 8, 256)    1024        add_98[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 8, 8, 256)    0           batch_normalization_257[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 8, 8, 256)    0           activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 256)    0           zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 1, 1, 10)     2570        average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 10)           0           conv2d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 10)           0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 10)           0           dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 396,810\n",
            "Trainable params: 389,258\n",
            "Non-trainable params: 7,552\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHQ4WZnfYOzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "num_classes = 10 \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def get_cutout(sl=0.02, sh=0.4, rand_r=0.3):\n",
        "    def cutout(img):\n",
        "        if np.random.rand() > 0.5:\n",
        "            return img\n",
        "        while True:\n",
        "            s = np.random.uniform(sl, sh) * 32 * 32\n",
        "            r = np.random.uniform(rand_r, 1/rand_r)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, 32)\n",
        "            top = np.random.randint(0, 32)\n",
        "            if left + w <= 32 and top + h <= 32:\n",
        "                break\n",
        "        img[top:top + h, left:left + w, :] = np.random.uniform(0, 1)\n",
        "        return img\n",
        "    return cutout\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    preprocessing_function=get_cutout()\n",
        "    )\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqsHsPg9YVU4",
        "colab_type": "code",
        "outputId": "8eef14ea-d37b-4f53-abbd-97bab2d52115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_red = red_model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                                      epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                      validation_data=(x_test, y_test), callbacks=[cp_callback, warm_up_lr])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.2963 - accuracy: 0.1560\n",
            "Epoch 00001: saving model to /content\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 2.2963 - accuracy: 0.1560 - val_loss: 2.0136 - val_accuracy: 0.2735\n",
            "Epoch 2/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.9596 - accuracy: 0.2900\n",
            "Epoch 00002: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.9596 - accuracy: 0.2900 - val_loss: 1.7538 - val_accuracy: 0.3712\n",
            "Epoch 3/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 1.8107 - accuracy: 0.3454\n",
            "Epoch 00003: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.8107 - accuracy: 0.3455 - val_loss: 1.6159 - val_accuracy: 0.4064\n",
            "Epoch 4/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 1.7099 - accuracy: 0.3820\n",
            "Epoch 00004: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.7100 - accuracy: 0.3819 - val_loss: 1.5119 - val_accuracy: 0.4509\n",
            "Epoch 5/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 1.6402 - accuracy: 0.4085\n",
            "Epoch 00005: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.6401 - accuracy: 0.4085 - val_loss: 1.4707 - val_accuracy: 0.4615\n",
            "Epoch 6/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.5771 - accuracy: 0.4291\n",
            "Epoch 00006: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.5771 - accuracy: 0.4291 - val_loss: 1.3262 - val_accuracy: 0.5246\n",
            "Epoch 7/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 1.5088 - accuracy: 0.4564\n",
            "Epoch 00007: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.5089 - accuracy: 0.4564 - val_loss: 1.6362 - val_accuracy: 0.4463\n",
            "Epoch 8/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 1.4512 - accuracy: 0.4770\n",
            "Epoch 00008: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.4511 - accuracy: 0.4771 - val_loss: 1.4253 - val_accuracy: 0.4916\n",
            "Epoch 9/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3914 - accuracy: 0.4989\n",
            "Epoch 00009: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.3914 - accuracy: 0.4989 - val_loss: 1.3550 - val_accuracy: 0.5353\n",
            "Epoch 10/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.5186\n",
            "Epoch 00010: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.3392 - accuracy: 0.5186 - val_loss: 1.6838 - val_accuracy: 0.4600\n",
            "Epoch 11/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 1.2843 - accuracy: 0.5377\n",
            "Epoch 00011: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.2843 - accuracy: 0.5376 - val_loss: 1.3828 - val_accuracy: 0.5290\n",
            "Epoch 12/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 1.2373 - accuracy: 0.5546\n",
            "Epoch 00012: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.2375 - accuracy: 0.5545 - val_loss: 1.2202 - val_accuracy: 0.5828\n",
            "Epoch 13/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1935 - accuracy: 0.5699\n",
            "Epoch 00013: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.1935 - accuracy: 0.5699 - val_loss: 0.9872 - val_accuracy: 0.6590\n",
            "Epoch 14/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1480 - accuracy: 0.5871\n",
            "Epoch 00014: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.1480 - accuracy: 0.5871 - val_loss: 1.0525 - val_accuracy: 0.6420\n",
            "Epoch 15/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 1.1055 - accuracy: 0.6004\n",
            "Epoch 00015: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.1058 - accuracy: 0.6002 - val_loss: 1.4218 - val_accuracy: 0.5426\n",
            "Epoch 16/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0597 - accuracy: 0.6151\n",
            "Epoch 00016: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.0597 - accuracy: 0.6151 - val_loss: 0.8147 - val_accuracy: 0.7155\n",
            "Epoch 17/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0279 - accuracy: 0.6243\n",
            "Epoch 00017: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.0279 - accuracy: 0.6243 - val_loss: 0.9413 - val_accuracy: 0.6786\n",
            "Epoch 18/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9999 - accuracy: 0.6361\n",
            "Epoch 00018: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.9999 - accuracy: 0.6361 - val_loss: 0.8388 - val_accuracy: 0.7138\n",
            "Epoch 19/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9581 - accuracy: 0.6518\n",
            "Epoch 00019: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.9581 - accuracy: 0.6518 - val_loss: 0.8406 - val_accuracy: 0.7191\n",
            "Epoch 20/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9361 - accuracy: 0.6588\n",
            "Epoch 00020: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.9361 - accuracy: 0.6588 - val_loss: 0.8584 - val_accuracy: 0.7160\n",
            "Epoch 21/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9161 - accuracy: 0.6650\n",
            "Epoch 00021: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.9161 - accuracy: 0.6650 - val_loss: 0.9103 - val_accuracy: 0.6931\n",
            "Epoch 22/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8975 - accuracy: 0.6712\n",
            "Epoch 00022: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.8975 - accuracy: 0.6712 - val_loss: 1.1562 - val_accuracy: 0.6371\n",
            "Epoch 23/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8800 - accuracy: 0.6764\n",
            "Epoch 00023: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.8800 - accuracy: 0.6764 - val_loss: 0.7167 - val_accuracy: 0.7654\n",
            "Epoch 24/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.8615 - accuracy: 0.6839\n",
            "Epoch 00024: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.8616 - accuracy: 0.6839 - val_loss: 0.8580 - val_accuracy: 0.7178\n",
            "Epoch 25/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8484 - accuracy: 0.6853\n",
            "Epoch 00025: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.8484 - accuracy: 0.6853 - val_loss: 0.9021 - val_accuracy: 0.7149\n",
            "Epoch 26/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.8291 - accuracy: 0.6930\n",
            "Epoch 00026: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.8289 - accuracy: 0.6931 - val_loss: 0.8301 - val_accuracy: 0.7318\n",
            "Epoch 27/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8151 - accuracy: 0.6964\n",
            "Epoch 00027: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.8151 - accuracy: 0.6964 - val_loss: 0.7769 - val_accuracy: 0.7430\n",
            "Epoch 28/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7972 - accuracy: 0.7026\n",
            "Epoch 00028: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.7972 - accuracy: 0.7026 - val_loss: 0.6873 - val_accuracy: 0.7643\n",
            "Epoch 29/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7810 - accuracy: 0.7090\n",
            "Epoch 00029: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.7810 - accuracy: 0.7090 - val_loss: 0.6286 - val_accuracy: 0.7911\n",
            "Epoch 30/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7705 - accuracy: 0.7113\n",
            "Epoch 00030: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.7705 - accuracy: 0.7113 - val_loss: 0.7847 - val_accuracy: 0.7551\n",
            "Epoch 31/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7567 - accuracy: 0.7165\n",
            "Epoch 00031: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.7567 - accuracy: 0.7165 - val_loss: 0.8977 - val_accuracy: 0.7127\n",
            "Epoch 32/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7460 - accuracy: 0.7203\n",
            "Epoch 00032: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.7460 - accuracy: 0.7203 - val_loss: 0.7345 - val_accuracy: 0.7582\n",
            "Epoch 33/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7267 - accuracy: 0.7268\n",
            "Epoch 00033: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.7267 - accuracy: 0.7268 - val_loss: 0.5618 - val_accuracy: 0.8146\n",
            "Epoch 34/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.7235 - accuracy: 0.7251\n",
            "Epoch 00034: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.7236 - accuracy: 0.7250 - val_loss: 0.5516 - val_accuracy: 0.8171\n",
            "Epoch 35/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.7000 - accuracy: 0.7361\n",
            "Epoch 00035: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.7000 - accuracy: 0.7361 - val_loss: 0.6584 - val_accuracy: 0.7895\n",
            "Epoch 36/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.6967 - accuracy: 0.7365\n",
            "Epoch 00036: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6968 - accuracy: 0.7364 - val_loss: 0.5996 - val_accuracy: 0.8080\n",
            "Epoch 37/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6879 - accuracy: 0.7342\n",
            "Epoch 00037: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6879 - accuracy: 0.7342 - val_loss: 0.5658 - val_accuracy: 0.8198\n",
            "Epoch 38/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6823 - accuracy: 0.7404\n",
            "Epoch 00038: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6823 - accuracy: 0.7404 - val_loss: 0.5838 - val_accuracy: 0.8054\n",
            "Epoch 39/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6679 - accuracy: 0.7433\n",
            "Epoch 00039: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6679 - accuracy: 0.7433 - val_loss: 0.6219 - val_accuracy: 0.8070\n",
            "Epoch 40/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6616 - accuracy: 0.7473\n",
            "Epoch 00040: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.6616 - accuracy: 0.7473 - val_loss: 0.5813 - val_accuracy: 0.8096\n",
            "Epoch 41/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6569 - accuracy: 0.7463\n",
            "Epoch 00041: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.6569 - accuracy: 0.7463 - val_loss: 0.6067 - val_accuracy: 0.8021\n",
            "Epoch 42/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.6458 - accuracy: 0.7485\n",
            "Epoch 00042: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.6457 - accuracy: 0.7485 - val_loss: 0.4672 - val_accuracy: 0.8439\n",
            "Epoch 43/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6384 - accuracy: 0.7525\n",
            "Epoch 00043: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6384 - accuracy: 0.7525 - val_loss: 0.4986 - val_accuracy: 0.8324\n",
            "Epoch 44/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6338 - accuracy: 0.7550\n",
            "Epoch 00044: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6338 - accuracy: 0.7550 - val_loss: 0.5698 - val_accuracy: 0.8197\n",
            "Epoch 45/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.7555\n",
            "Epoch 00045: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6249 - accuracy: 0.7555 - val_loss: 0.5970 - val_accuracy: 0.8055\n",
            "Epoch 46/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6215 - accuracy: 0.7582\n",
            "Epoch 00046: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6215 - accuracy: 0.7582 - val_loss: 0.5706 - val_accuracy: 0.8161\n",
            "Epoch 47/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6109 - accuracy: 0.7614\n",
            "Epoch 00047: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6109 - accuracy: 0.7614 - val_loss: 0.4945 - val_accuracy: 0.8370\n",
            "Epoch 48/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.6010 - accuracy: 0.7642\n",
            "Epoch 00048: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6009 - accuracy: 0.7642 - val_loss: 0.4874 - val_accuracy: 0.8414\n",
            "Epoch 49/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5960 - accuracy: 0.7653\n",
            "Epoch 00049: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5960 - accuracy: 0.7653 - val_loss: 0.4331 - val_accuracy: 0.8549\n",
            "Epoch 50/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6041 - accuracy: 0.7632\n",
            "Epoch 00050: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6041 - accuracy: 0.7632 - val_loss: 0.6026 - val_accuracy: 0.8175\n",
            "Epoch 51/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5865 - accuracy: 0.7673\n",
            "Epoch 00051: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5865 - accuracy: 0.7673 - val_loss: 0.4557 - val_accuracy: 0.8525\n",
            "Epoch 52/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.7707\n",
            "Epoch 00052: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.5822 - accuracy: 0.7707 - val_loss: 0.6587 - val_accuracy: 0.7983\n",
            "Epoch 53/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5769 - accuracy: 0.7726\n",
            "Epoch 00053: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5769 - accuracy: 0.7726 - val_loss: 0.4604 - val_accuracy: 0.8499\n",
            "Epoch 54/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5751 - accuracy: 0.7727\n",
            "Epoch 00054: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5751 - accuracy: 0.7727 - val_loss: 0.5191 - val_accuracy: 0.8345\n",
            "Epoch 55/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.7754\n",
            "Epoch 00055: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5645 - accuracy: 0.7754 - val_loss: 0.4625 - val_accuracy: 0.8535\n",
            "Epoch 56/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7783\n",
            "Epoch 00056: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5648 - accuracy: 0.7782 - val_loss: 0.4926 - val_accuracy: 0.8440\n",
            "Epoch 57/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5570 - accuracy: 0.7780\n",
            "Epoch 00057: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5570 - accuracy: 0.7780 - val_loss: 0.4323 - val_accuracy: 0.8624\n",
            "Epoch 58/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.7798\n",
            "Epoch 00058: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5540 - accuracy: 0.7798 - val_loss: 0.6198 - val_accuracy: 0.8047\n",
            "Epoch 59/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.7803\n",
            "Epoch 00059: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5496 - accuracy: 0.7803 - val_loss: 0.4874 - val_accuracy: 0.8459\n",
            "Epoch 60/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.7811\n",
            "Epoch 00060: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5465 - accuracy: 0.7811 - val_loss: 0.4948 - val_accuracy: 0.8475\n",
            "Epoch 61/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.5473 - accuracy: 0.7796\n",
            "Epoch 00061: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.5471 - accuracy: 0.7797 - val_loss: 0.4663 - val_accuracy: 0.8509\n",
            "Epoch 62/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.5356 - accuracy: 0.7835\n",
            "Epoch 00062: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5358 - accuracy: 0.7835 - val_loss: 0.4680 - val_accuracy: 0.8550\n",
            "Epoch 63/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5387 - accuracy: 0.7820\n",
            "Epoch 00063: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5387 - accuracy: 0.7820 - val_loss: 0.4893 - val_accuracy: 0.8408\n",
            "Epoch 64/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.7854\n",
            "Epoch 00064: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5310 - accuracy: 0.7854 - val_loss: 0.4296 - val_accuracy: 0.8600\n",
            "Epoch 65/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.7858\n",
            "Epoch 00065: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5282 - accuracy: 0.7858 - val_loss: 0.4816 - val_accuracy: 0.8496\n",
            "Epoch 66/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5162 - accuracy: 0.7917\n",
            "Epoch 00066: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.5162 - accuracy: 0.7917 - val_loss: 0.5124 - val_accuracy: 0.8519\n",
            "Epoch 67/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.7926\n",
            "Epoch 00067: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5138 - accuracy: 0.7926 - val_loss: 0.5520 - val_accuracy: 0.8415\n",
            "Epoch 68/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.7897\n",
            "Epoch 00068: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5169 - accuracy: 0.7897 - val_loss: 0.4167 - val_accuracy: 0.8655\n",
            "Epoch 69/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.5057 - accuracy: 0.7936\n",
            "Epoch 00069: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5056 - accuracy: 0.7936 - val_loss: 0.3979 - val_accuracy: 0.8724\n",
            "Epoch 70/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5111 - accuracy: 0.7940\n",
            "Epoch 00070: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5111 - accuracy: 0.7940 - val_loss: 0.4395 - val_accuracy: 0.8604\n",
            "Epoch 71/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.5033 - accuracy: 0.7932\n",
            "Epoch 00071: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5033 - accuracy: 0.7931 - val_loss: 0.4104 - val_accuracy: 0.8727\n",
            "Epoch 72/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.7952\n",
            "Epoch 00072: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.4996 - accuracy: 0.7952 - val_loss: 0.4583 - val_accuracy: 0.8506\n",
            "Epoch 73/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4972 - accuracy: 0.7972\n",
            "Epoch 00073: saving model to /content\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.4973 - accuracy: 0.7971 - val_loss: 0.4210 - val_accuracy: 0.8714\n",
            "Epoch 74/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.7987\n",
            "Epoch 00074: saving model to /content\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.4922 - accuracy: 0.7987 - val_loss: 0.5364 - val_accuracy: 0.8459\n",
            "Epoch 75/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.7956\n",
            "Epoch 00075: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4938 - accuracy: 0.7956 - val_loss: 0.4294 - val_accuracy: 0.8638\n",
            "Epoch 76/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.7997\n",
            "Epoch 00076: saving model to /content\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.4846 - accuracy: 0.7997 - val_loss: 0.3971 - val_accuracy: 0.8790\n",
            "Epoch 77/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.8025\n",
            "Epoch 00077: saving model to /content\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.4801 - accuracy: 0.8025 - val_loss: 0.3773 - val_accuracy: 0.8835\n",
            "Epoch 78/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4822 - accuracy: 0.8000\n",
            "Epoch 00078: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4824 - accuracy: 0.7999 - val_loss: 0.4037 - val_accuracy: 0.8738\n",
            "Epoch 79/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4784 - accuracy: 0.8016\n",
            "Epoch 00079: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4784 - accuracy: 0.8016 - val_loss: 0.5124 - val_accuracy: 0.8461\n",
            "Epoch 80/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.8028\n",
            "Epoch 00080: saving model to /content\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.4747 - accuracy: 0.8028 - val_loss: 0.3767 - val_accuracy: 0.8843\n",
            "Epoch 81/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4798 - accuracy: 0.8025\n",
            "Epoch 00081: saving model to /content\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.4799 - accuracy: 0.8023 - val_loss: 0.4289 - val_accuracy: 0.8702\n",
            "Epoch 82/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4691 - accuracy: 0.8043\n",
            "Epoch 00082: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4691 - accuracy: 0.8043 - val_loss: 0.4600 - val_accuracy: 0.8608\n",
            "Epoch 83/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.8073\n",
            "Epoch 00083: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4662 - accuracy: 0.8073 - val_loss: 0.4074 - val_accuracy: 0.8766\n",
            "Epoch 84/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.8042\n",
            "Epoch 00084: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4682 - accuracy: 0.8042 - val_loss: 0.3897 - val_accuracy: 0.8861\n",
            "Epoch 85/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4650 - accuracy: 0.8071\n",
            "Epoch 00085: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4650 - accuracy: 0.8071 - val_loss: 0.3960 - val_accuracy: 0.8813\n",
            "Epoch 86/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4599 - accuracy: 0.8078\n",
            "Epoch 00086: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4601 - accuracy: 0.8078 - val_loss: 0.4551 - val_accuracy: 0.8673\n",
            "Epoch 87/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4644 - accuracy: 0.8056\n",
            "Epoch 00087: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4643 - accuracy: 0.8057 - val_loss: 0.3558 - val_accuracy: 0.8890\n",
            "Epoch 88/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4595 - accuracy: 0.8072\n",
            "Epoch 00088: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4595 - accuracy: 0.8072 - val_loss: 0.4242 - val_accuracy: 0.8705\n",
            "Epoch 89/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4495 - accuracy: 0.8102\n",
            "Epoch 00089: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4493 - accuracy: 0.8102 - val_loss: 0.4168 - val_accuracy: 0.8761\n",
            "Epoch 90/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.8104\n",
            "Epoch 00090: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4528 - accuracy: 0.8104 - val_loss: 0.3983 - val_accuracy: 0.8798\n",
            "Epoch 91/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4446 - accuracy: 0.8114\n",
            "Epoch 00091: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.4446 - accuracy: 0.8114 - val_loss: 0.3799 - val_accuracy: 0.8871\n",
            "Epoch 92/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.8113\n",
            "Epoch 00092: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4461 - accuracy: 0.8113 - val_loss: 0.3797 - val_accuracy: 0.8893\n",
            "Epoch 93/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.8159\n",
            "Epoch 00093: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4351 - accuracy: 0.8159 - val_loss: 0.4148 - val_accuracy: 0.8766\n",
            "Epoch 94/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.8150\n",
            "Epoch 00094: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4379 - accuracy: 0.8150 - val_loss: 0.4029 - val_accuracy: 0.8830\n",
            "Epoch 95/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4424 - accuracy: 0.8128\n",
            "Epoch 00095: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4424 - accuracy: 0.8128 - val_loss: 0.3693 - val_accuracy: 0.8916\n",
            "Epoch 96/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8186\n",
            "Epoch 00096: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.4287 - accuracy: 0.8186 - val_loss: 0.3976 - val_accuracy: 0.8810\n",
            "Epoch 97/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4359 - accuracy: 0.8132\n",
            "Epoch 00097: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4359 - accuracy: 0.8132 - val_loss: 0.4156 - val_accuracy: 0.8785\n",
            "Epoch 98/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.8149\n",
            "Epoch 00098: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.4367 - accuracy: 0.8149 - val_loss: 0.3444 - val_accuracy: 0.8960\n",
            "Epoch 99/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.8196\n",
            "Epoch 00099: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4264 - accuracy: 0.8196 - val_loss: 0.4052 - val_accuracy: 0.8850\n",
            "Epoch 100/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4304 - accuracy: 0.8154\n",
            "Epoch 00100: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4304 - accuracy: 0.8154 - val_loss: 0.4666 - val_accuracy: 0.8697\n",
            "Epoch 101/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4239 - accuracy: 0.8181\n",
            "Epoch 00101: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4239 - accuracy: 0.8181 - val_loss: 0.3959 - val_accuracy: 0.8901\n",
            "Epoch 102/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8205\n",
            "Epoch 00102: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4202 - accuracy: 0.8205 - val_loss: 0.3946 - val_accuracy: 0.8863\n",
            "Epoch 103/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.8193\n",
            "Epoch 00103: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4229 - accuracy: 0.8193 - val_loss: 0.3784 - val_accuracy: 0.8879\n",
            "Epoch 104/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8206\n",
            "Epoch 00104: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4205 - accuracy: 0.8205 - val_loss: 0.3655 - val_accuracy: 0.8956\n",
            "Epoch 105/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8211\n",
            "Epoch 00105: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4203 - accuracy: 0.8211 - val_loss: 0.3764 - val_accuracy: 0.8896\n",
            "Epoch 106/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8221\n",
            "Epoch 00106: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4149 - accuracy: 0.8221 - val_loss: 0.3708 - val_accuracy: 0.8901\n",
            "Epoch 107/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8216\n",
            "Epoch 00107: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4118 - accuracy: 0.8216 - val_loss: 0.3765 - val_accuracy: 0.8881\n",
            "Epoch 108/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8206\n",
            "Epoch 00108: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4118 - accuracy: 0.8206 - val_loss: 0.3771 - val_accuracy: 0.8938\n",
            "Epoch 109/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4116 - accuracy: 0.8228\n",
            "Epoch 00109: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4117 - accuracy: 0.8227 - val_loss: 0.3884 - val_accuracy: 0.8881\n",
            "Epoch 110/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4082 - accuracy: 0.8227\n",
            "Epoch 00110: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4082 - accuracy: 0.8227 - val_loss: 0.4085 - val_accuracy: 0.8879\n",
            "Epoch 111/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4035 - accuracy: 0.8261\n",
            "Epoch 00111: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4035 - accuracy: 0.8261 - val_loss: 0.4634 - val_accuracy: 0.8748\n",
            "Epoch 112/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4027 - accuracy: 0.8243\n",
            "Epoch 00112: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4028 - accuracy: 0.8243 - val_loss: 0.4824 - val_accuracy: 0.8681\n",
            "Epoch 113/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8250\n",
            "Epoch 00113: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4041 - accuracy: 0.8250 - val_loss: 0.3752 - val_accuracy: 0.8901\n",
            "Epoch 114/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3988 - accuracy: 0.8271\n",
            "Epoch 00114: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3988 - accuracy: 0.8271 - val_loss: 0.3610 - val_accuracy: 0.8956\n",
            "Epoch 115/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8262\n",
            "Epoch 00115: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4001 - accuracy: 0.8262 - val_loss: 0.3953 - val_accuracy: 0.8897\n",
            "Epoch 116/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8272\n",
            "Epoch 00116: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3940 - accuracy: 0.8273 - val_loss: 0.3835 - val_accuracy: 0.8947\n",
            "Epoch 117/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3991 - accuracy: 0.8247\n",
            "Epoch 00117: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3991 - accuracy: 0.8247 - val_loss: 0.3685 - val_accuracy: 0.8963\n",
            "Epoch 118/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8296\n",
            "Epoch 00118: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3907 - accuracy: 0.8297 - val_loss: 0.3620 - val_accuracy: 0.8959\n",
            "Epoch 119/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.8317\n",
            "Epoch 00119: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3883 - accuracy: 0.8317 - val_loss: 0.4246 - val_accuracy: 0.8845\n",
            "Epoch 120/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3799 - accuracy: 0.8310\n",
            "Epoch 00120: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3799 - accuracy: 0.8310 - val_loss: 0.3723 - val_accuracy: 0.8951\n",
            "Epoch 121/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.8315\n",
            "Epoch 00121: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3805 - accuracy: 0.8315 - val_loss: 0.4163 - val_accuracy: 0.8872\n",
            "Epoch 122/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.8322\n",
            "Epoch 00122: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.3824 - accuracy: 0.8322 - val_loss: 0.3532 - val_accuracy: 0.8991\n",
            "Epoch 123/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.8318\n",
            "Epoch 00123: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.3805 - accuracy: 0.8318 - val_loss: 0.3625 - val_accuracy: 0.8993\n",
            "Epoch 124/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.8332\n",
            "Epoch 00124: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3822 - accuracy: 0.8332 - val_loss: 0.3692 - val_accuracy: 0.8986\n",
            "Epoch 125/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3739 - accuracy: 0.8342\n",
            "Epoch 00125: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3739 - accuracy: 0.8342 - val_loss: 0.4101 - val_accuracy: 0.8893\n",
            "Epoch 126/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3758 - accuracy: 0.8324\n",
            "Epoch 00126: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.3756 - accuracy: 0.8325 - val_loss: 0.3874 - val_accuracy: 0.8939\n",
            "Epoch 127/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3701 - accuracy: 0.8369\n",
            "Epoch 00127: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3701 - accuracy: 0.8369 - val_loss: 0.3886 - val_accuracy: 0.9007\n",
            "Epoch 128/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3728 - accuracy: 0.8341\n",
            "Epoch 00128: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3728 - accuracy: 0.8341 - val_loss: 0.4147 - val_accuracy: 0.8848\n",
            "Epoch 129/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3705 - accuracy: 0.8337\n",
            "Epoch 00129: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3705 - accuracy: 0.8337 - val_loss: 0.4089 - val_accuracy: 0.8929\n",
            "Epoch 130/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3707 - accuracy: 0.8352\n",
            "Epoch 00130: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3707 - accuracy: 0.8352 - val_loss: 0.3999 - val_accuracy: 0.8949\n",
            "Epoch 131/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3703 - accuracy: 0.8347\n",
            "Epoch 00131: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3703 - accuracy: 0.8347 - val_loss: 0.3724 - val_accuracy: 0.8946\n",
            "Epoch 132/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3669 - accuracy: 0.8355\n",
            "Epoch 00132: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3669 - accuracy: 0.8355 - val_loss: 0.3702 - val_accuracy: 0.9017\n",
            "Epoch 133/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3664 - accuracy: 0.8353\n",
            "Epoch 00133: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3664 - accuracy: 0.8353 - val_loss: 0.3743 - val_accuracy: 0.9009\n",
            "Epoch 134/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8358\n",
            "Epoch 00134: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3648 - accuracy: 0.8358 - val_loss: 0.4027 - val_accuracy: 0.8945\n",
            "Epoch 135/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.8387\n",
            "Epoch 00135: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3611 - accuracy: 0.8387 - val_loss: 0.3697 - val_accuracy: 0.9030\n",
            "Epoch 136/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3552 - accuracy: 0.8383\n",
            "Epoch 00136: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3552 - accuracy: 0.8383 - val_loss: 0.3567 - val_accuracy: 0.9026\n",
            "Epoch 137/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3575 - accuracy: 0.8404\n",
            "Epoch 00137: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3575 - accuracy: 0.8404 - val_loss: 0.3537 - val_accuracy: 0.9055\n",
            "Epoch 138/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.8385\n",
            "Epoch 00138: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3536 - accuracy: 0.8385 - val_loss: 0.3620 - val_accuracy: 0.9022\n",
            "Epoch 139/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.8416\n",
            "Epoch 00139: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3549 - accuracy: 0.8416 - val_loss: 0.3717 - val_accuracy: 0.9030\n",
            "Epoch 140/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3561 - accuracy: 0.8381\n",
            "Epoch 00140: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3561 - accuracy: 0.8381 - val_loss: 0.3869 - val_accuracy: 0.8960\n",
            "Epoch 141/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3516 - accuracy: 0.8385\n",
            "Epoch 00141: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3516 - accuracy: 0.8385 - val_loss: 0.4183 - val_accuracy: 0.8918\n",
            "Epoch 142/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.8381\n",
            "Epoch 00142: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3577 - accuracy: 0.8381 - val_loss: 0.3780 - val_accuracy: 0.8979\n",
            "Epoch 143/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.8415\n",
            "Epoch 00143: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3540 - accuracy: 0.8415 - val_loss: 0.3987 - val_accuracy: 0.8994\n",
            "Epoch 144/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8422\n",
            "Epoch 00144: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3444 - accuracy: 0.8422 - val_loss: 0.4080 - val_accuracy: 0.8985\n",
            "Epoch 145/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3470 - accuracy: 0.8428\n",
            "Epoch 00145: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3472 - accuracy: 0.8428 - val_loss: 0.4387 - val_accuracy: 0.8906\n",
            "Epoch 146/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8429\n",
            "Epoch 00146: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3417 - accuracy: 0.8429 - val_loss: 0.3721 - val_accuracy: 0.9019\n",
            "Epoch 147/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8433\n",
            "Epoch 00147: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3417 - accuracy: 0.8433 - val_loss: 0.3799 - val_accuracy: 0.9029\n",
            "Epoch 148/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.8404\n",
            "Epoch 00148: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3504 - accuracy: 0.8404 - val_loss: 0.4049 - val_accuracy: 0.8990\n",
            "Epoch 149/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.8437\n",
            "Epoch 00149: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3425 - accuracy: 0.8437 - val_loss: 0.3938 - val_accuracy: 0.8990\n",
            "Epoch 150/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.8445\n",
            "Epoch 00150: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3413 - accuracy: 0.8445 - val_loss: 0.3451 - val_accuracy: 0.9089\n",
            "Epoch 151/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8445\n",
            "Epoch 00151: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3420 - accuracy: 0.8445 - val_loss: 0.3828 - val_accuracy: 0.9010\n",
            "Epoch 152/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3365 - accuracy: 0.8454\n",
            "Epoch 00152: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3364 - accuracy: 0.8454 - val_loss: 0.3768 - val_accuracy: 0.9036\n",
            "Epoch 153/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.8438\n",
            "Epoch 00153: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3409 - accuracy: 0.8438 - val_loss: 0.3717 - val_accuracy: 0.9035\n",
            "Epoch 154/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3350 - accuracy: 0.8457\n",
            "Epoch 00154: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3350 - accuracy: 0.8457 - val_loss: 0.3792 - val_accuracy: 0.9031\n",
            "Epoch 155/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.8467\n",
            "Epoch 00155: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3338 - accuracy: 0.8467 - val_loss: 0.3950 - val_accuracy: 0.9025\n",
            "Epoch 156/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.8467\n",
            "Epoch 00156: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3304 - accuracy: 0.8467 - val_loss: 0.3870 - val_accuracy: 0.9043\n",
            "Epoch 157/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3315 - accuracy: 0.8454\n",
            "Epoch 00157: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3315 - accuracy: 0.8454 - val_loss: 0.3674 - val_accuracy: 0.9068\n",
            "Epoch 158/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8470\n",
            "Epoch 00158: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3282 - accuracy: 0.8470 - val_loss: 0.3746 - val_accuracy: 0.9077\n",
            "Epoch 159/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8468\n",
            "Epoch 00159: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3296 - accuracy: 0.8467 - val_loss: 0.3680 - val_accuracy: 0.9053\n",
            "Epoch 160/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.8450\n",
            "Epoch 00160: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3310 - accuracy: 0.8450 - val_loss: 0.3884 - val_accuracy: 0.9004\n",
            "Epoch 161/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8495\n",
            "Epoch 00161: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3247 - accuracy: 0.8495 - val_loss: 0.3667 - val_accuracy: 0.9081\n",
            "Epoch 162/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3233 - accuracy: 0.8509\n",
            "Epoch 00162: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3233 - accuracy: 0.8509 - val_loss: 0.3649 - val_accuracy: 0.9076\n",
            "Epoch 163/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8482\n",
            "Epoch 00163: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3278 - accuracy: 0.8482 - val_loss: 0.4099 - val_accuracy: 0.9028\n",
            "Epoch 164/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8498\n",
            "Epoch 00164: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3206 - accuracy: 0.8498 - val_loss: 0.3710 - val_accuracy: 0.9085\n",
            "Epoch 165/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.8485\n",
            "Epoch 00165: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3257 - accuracy: 0.8485 - val_loss: 0.3669 - val_accuracy: 0.9057\n",
            "Epoch 166/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8491\n",
            "Epoch 00166: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3250 - accuracy: 0.8491 - val_loss: 0.3613 - val_accuracy: 0.9083\n",
            "Epoch 167/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3212 - accuracy: 0.8495\n",
            "Epoch 00167: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3212 - accuracy: 0.8495 - val_loss: 0.3572 - val_accuracy: 0.9097\n",
            "Epoch 168/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8499\n",
            "Epoch 00168: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3204 - accuracy: 0.8499 - val_loss: 0.3526 - val_accuracy: 0.9130\n",
            "Epoch 169/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8514\n",
            "Epoch 00169: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3195 - accuracy: 0.8514 - val_loss: 0.3635 - val_accuracy: 0.9078\n",
            "Epoch 170/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8509\n",
            "Epoch 00170: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3220 - accuracy: 0.8509 - val_loss: 0.3502 - val_accuracy: 0.9084\n",
            "Epoch 171/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.8487\n",
            "Epoch 00171: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.3210 - accuracy: 0.8487 - val_loss: 0.4018 - val_accuracy: 0.9049\n",
            "Epoch 172/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.8492\n",
            "Epoch 00172: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3160 - accuracy: 0.8492 - val_loss: 0.3538 - val_accuracy: 0.9108\n",
            "Epoch 173/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3149 - accuracy: 0.8524\n",
            "Epoch 00173: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.3149 - accuracy: 0.8524 - val_loss: 0.3683 - val_accuracy: 0.9084\n",
            "Epoch 174/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8511\n",
            "Epoch 00174: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3138 - accuracy: 0.8512 - val_loss: 0.3856 - val_accuracy: 0.9069\n",
            "Epoch 175/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.8524\n",
            "Epoch 00175: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3129 - accuracy: 0.8524 - val_loss: 0.4028 - val_accuracy: 0.9047\n",
            "Epoch 176/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.8518\n",
            "Epoch 00176: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3170 - accuracy: 0.8518 - val_loss: 0.3711 - val_accuracy: 0.9081\n",
            "Epoch 177/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8518\n",
            "Epoch 00177: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3088 - accuracy: 0.8518 - val_loss: 0.3831 - val_accuracy: 0.9099\n",
            "Epoch 178/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.8526\n",
            "Epoch 00178: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3149 - accuracy: 0.8527 - val_loss: 0.3606 - val_accuracy: 0.9129\n",
            "Epoch 179/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8550\n",
            "Epoch 00179: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.3062 - accuracy: 0.8551 - val_loss: 0.3501 - val_accuracy: 0.9142\n",
            "Epoch 180/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.8552\n",
            "Epoch 00180: saving model to /content\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.3081 - accuracy: 0.8552 - val_loss: 0.3743 - val_accuracy: 0.9110\n",
            "Epoch 181/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.8527\n",
            "Epoch 00181: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.3121 - accuracy: 0.8527 - val_loss: 0.3682 - val_accuracy: 0.9121\n",
            "Epoch 182/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3103 - accuracy: 0.8518\n",
            "Epoch 00182: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.3103 - accuracy: 0.8518 - val_loss: 0.3831 - val_accuracy: 0.9092\n",
            "Epoch 183/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8529\n",
            "Epoch 00183: saving model to /content\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.3067 - accuracy: 0.8528 - val_loss: 0.3642 - val_accuracy: 0.9115\n",
            "Epoch 184/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8571\n",
            "Epoch 00184: saving model to /content\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.2976 - accuracy: 0.8569 - val_loss: 0.3694 - val_accuracy: 0.9121\n",
            "Epoch 185/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8548\n",
            "Epoch 00185: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.3029 - accuracy: 0.8548 - val_loss: 0.3631 - val_accuracy: 0.9124\n",
            "Epoch 186/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.3093 - accuracy: 0.8543\n",
            "Epoch 00186: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.3092 - accuracy: 0.8543 - val_loss: 0.3456 - val_accuracy: 0.9142\n",
            "Epoch 187/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.8546\n",
            "Epoch 00187: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.3036 - accuracy: 0.8546 - val_loss: 0.3563 - val_accuracy: 0.9128\n",
            "Epoch 188/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8599\n",
            "Epoch 00188: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.2931 - accuracy: 0.8600 - val_loss: 0.3590 - val_accuracy: 0.9116\n",
            "Epoch 189/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.8569\n",
            "Epoch 00189: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.2997 - accuracy: 0.8569 - val_loss: 0.3553 - val_accuracy: 0.9127\n",
            "Epoch 190/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.8568\n",
            "Epoch 00190: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.2984 - accuracy: 0.8568 - val_loss: 0.3617 - val_accuracy: 0.9136\n",
            "Epoch 191/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8566\n",
            "Epoch 00191: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.2972 - accuracy: 0.8565 - val_loss: 0.3705 - val_accuracy: 0.9143\n",
            "Epoch 192/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.8564\n",
            "Epoch 00192: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.2971 - accuracy: 0.8564 - val_loss: 0.3586 - val_accuracy: 0.9168\n",
            "Epoch 193/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2931 - accuracy: 0.8587\n",
            "Epoch 00193: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.2931 - accuracy: 0.8587 - val_loss: 0.3658 - val_accuracy: 0.9124\n",
            "Epoch 194/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.8546\n",
            "Epoch 00194: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.3020 - accuracy: 0.8546 - val_loss: 0.3630 - val_accuracy: 0.9148\n",
            "Epoch 195/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2977 - accuracy: 0.8563\n",
            "Epoch 00195: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.2977 - accuracy: 0.8563 - val_loss: 0.3556 - val_accuracy: 0.9158\n",
            "Epoch 196/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.8596\n",
            "Epoch 00196: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.2948 - accuracy: 0.8596 - val_loss: 0.3617 - val_accuracy: 0.9166\n",
            "Epoch 197/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8576\n",
            "Epoch 00197: saving model to /content\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.2919 - accuracy: 0.8577 - val_loss: 0.3624 - val_accuracy: 0.9151\n",
            "Epoch 198/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.8573\n",
            "Epoch 00198: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2961 - accuracy: 0.8573 - val_loss: 0.3673 - val_accuracy: 0.9162\n",
            "Epoch 199/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.8584\n",
            "Epoch 00199: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.2934 - accuracy: 0.8584 - val_loss: 0.3793 - val_accuracy: 0.9122\n",
            "Epoch 200/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8587\n",
            "Epoch 00200: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2919 - accuracy: 0.8588 - val_loss: 0.3623 - val_accuracy: 0.9145\n",
            "Epoch 201/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.8598\n",
            "Epoch 00201: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2907 - accuracy: 0.8598 - val_loss: 0.3668 - val_accuracy: 0.9158\n",
            "Epoch 202/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8569\n",
            "Epoch 00202: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2949 - accuracy: 0.8570 - val_loss: 0.3571 - val_accuracy: 0.9171\n",
            "Epoch 203/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8585\n",
            "Epoch 00203: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2913 - accuracy: 0.8585 - val_loss: 0.3611 - val_accuracy: 0.9173\n",
            "Epoch 204/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.8590\n",
            "Epoch 00204: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.2926 - accuracy: 0.8590 - val_loss: 0.3655 - val_accuracy: 0.9165\n",
            "Epoch 205/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.8598\n",
            "Epoch 00205: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2863 - accuracy: 0.8598 - val_loss: 0.3612 - val_accuracy: 0.9155\n",
            "Epoch 206/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.8622\n",
            "Epoch 00206: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2858 - accuracy: 0.8622 - val_loss: 0.3597 - val_accuracy: 0.9172\n",
            "Epoch 207/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.8600\n",
            "Epoch 00207: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2898 - accuracy: 0.8600 - val_loss: 0.3591 - val_accuracy: 0.9182\n",
            "Epoch 208/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2870 - accuracy: 0.8625\n",
            "Epoch 00208: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2870 - accuracy: 0.8625 - val_loss: 0.3643 - val_accuracy: 0.9176\n",
            "Epoch 209/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.8600\n",
            "Epoch 00209: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2873 - accuracy: 0.8601 - val_loss: 0.3575 - val_accuracy: 0.9168\n",
            "Epoch 210/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.8596\n",
            "Epoch 00210: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2876 - accuracy: 0.8596 - val_loss: 0.3515 - val_accuracy: 0.9175\n",
            "Epoch 211/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.8630\n",
            "Epoch 00211: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2841 - accuracy: 0.8630 - val_loss: 0.3576 - val_accuracy: 0.9173\n",
            "Epoch 212/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.8613\n",
            "Epoch 00212: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2888 - accuracy: 0.8613 - val_loss: 0.3613 - val_accuracy: 0.9185\n",
            "Epoch 213/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8612\n",
            "Epoch 00213: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2839 - accuracy: 0.8611 - val_loss: 0.3518 - val_accuracy: 0.9182\n",
            "Epoch 214/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.8627\n",
            "Epoch 00214: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2830 - accuracy: 0.8627 - val_loss: 0.3584 - val_accuracy: 0.9163\n",
            "Epoch 215/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.8607\n",
            "Epoch 00215: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2861 - accuracy: 0.8607 - val_loss: 0.3510 - val_accuracy: 0.9197\n",
            "Epoch 216/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.8582\n",
            "Epoch 00216: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2880 - accuracy: 0.8582 - val_loss: 0.3631 - val_accuracy: 0.9170\n",
            "Epoch 217/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.8584\n",
            "Epoch 00217: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2888 - accuracy: 0.8584 - val_loss: 0.3583 - val_accuracy: 0.9175\n",
            "Epoch 218/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.8601\n",
            "Epoch 00218: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2858 - accuracy: 0.8601 - val_loss: 0.3567 - val_accuracy: 0.9165\n",
            "Epoch 219/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.8609\n",
            "Epoch 00219: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2817 - accuracy: 0.8609 - val_loss: 0.3557 - val_accuracy: 0.9191\n",
            "Epoch 220/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.8631\n",
            "Epoch 00220: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2848 - accuracy: 0.8631 - val_loss: 0.3553 - val_accuracy: 0.9201\n",
            "Epoch 221/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.8610\n",
            "Epoch 00221: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2864 - accuracy: 0.8609 - val_loss: 0.3590 - val_accuracy: 0.9192\n",
            "Epoch 222/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.8617\n",
            "Epoch 00222: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2832 - accuracy: 0.8617 - val_loss: 0.3562 - val_accuracy: 0.9186\n",
            "Epoch 223/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.8616\n",
            "Epoch 00223: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2850 - accuracy: 0.8617 - val_loss: 0.3567 - val_accuracy: 0.9183\n",
            "Epoch 224/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.8627\n",
            "Epoch 00224: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2834 - accuracy: 0.8626 - val_loss: 0.3503 - val_accuracy: 0.9192\n",
            "Epoch 225/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2840 - accuracy: 0.8620\n",
            "Epoch 00225: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2840 - accuracy: 0.8620 - val_loss: 0.3578 - val_accuracy: 0.9181\n",
            "Epoch 226/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8594\n",
            "Epoch 00226: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2856 - accuracy: 0.8594 - val_loss: 0.3520 - val_accuracy: 0.9192\n",
            "Epoch 227/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.8620\n",
            "Epoch 00227: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2798 - accuracy: 0.8620 - val_loss: 0.3538 - val_accuracy: 0.9179\n",
            "Epoch 228/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2804 - accuracy: 0.8623\n",
            "Epoch 00228: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2803 - accuracy: 0.8623 - val_loss: 0.3545 - val_accuracy: 0.9185\n",
            "Epoch 229/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.8626\n",
            "Epoch 00229: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2805 - accuracy: 0.8626 - val_loss: 0.3539 - val_accuracy: 0.9180\n",
            "Epoch 230/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.8647\n",
            "Epoch 00230: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2832 - accuracy: 0.8647 - val_loss: 0.3519 - val_accuracy: 0.9179\n",
            "Epoch 231/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.8616\n",
            "Epoch 00231: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2820 - accuracy: 0.8616 - val_loss: 0.3543 - val_accuracy: 0.9185\n",
            "Epoch 232/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.8613\n",
            "Epoch 00232: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2818 - accuracy: 0.8613 - val_loss: 0.3565 - val_accuracy: 0.9181\n",
            "Epoch 233/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.8637\n",
            "Epoch 00233: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2768 - accuracy: 0.8637 - val_loss: 0.3516 - val_accuracy: 0.9190\n",
            "Epoch 234/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8636\n",
            "Epoch 00234: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2809 - accuracy: 0.8636 - val_loss: 0.3509 - val_accuracy: 0.9189\n",
            "Epoch 235/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8655\n",
            "Epoch 00235: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2776 - accuracy: 0.8656 - val_loss: 0.3530 - val_accuracy: 0.9186\n",
            "Epoch 236/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.8628\n",
            "Epoch 00236: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2819 - accuracy: 0.8628 - val_loss: 0.3519 - val_accuracy: 0.9186\n",
            "Epoch 237/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2809 - accuracy: 0.8615\n",
            "Epoch 00237: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2809 - accuracy: 0.8615 - val_loss: 0.3546 - val_accuracy: 0.9185\n",
            "Epoch 238/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2772 - accuracy: 0.8635\n",
            "Epoch 00238: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2772 - accuracy: 0.8635 - val_loss: 0.3531 - val_accuracy: 0.9191\n",
            "Epoch 239/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.8611\n",
            "Epoch 00239: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2816 - accuracy: 0.8611 - val_loss: 0.3522 - val_accuracy: 0.9188\n",
            "Epoch 240/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.8622\n",
            "Epoch 00240: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2791 - accuracy: 0.8622 - val_loss: 0.3539 - val_accuracy: 0.9189\n",
            "Epoch 241/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2791 - accuracy: 0.8637\n",
            "Epoch 00241: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2791 - accuracy: 0.8637 - val_loss: 0.3531 - val_accuracy: 0.9189\n",
            "Epoch 242/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.8633\n",
            "Epoch 00242: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2783 - accuracy: 0.8633 - val_loss: 0.3537 - val_accuracy: 0.9190\n",
            "Epoch 243/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.8625\n",
            "Epoch 00243: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2810 - accuracy: 0.8624 - val_loss: 0.3530 - val_accuracy: 0.9191\n",
            "Epoch 244/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.8622\n",
            "Epoch 00244: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2808 - accuracy: 0.8622 - val_loss: 0.3516 - val_accuracy: 0.9192\n",
            "Epoch 245/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.8617\n",
            "Epoch 00245: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2806 - accuracy: 0.8617 - val_loss: 0.3526 - val_accuracy: 0.9186\n",
            "Epoch 246/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.8627\n",
            "Epoch 00246: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2765 - accuracy: 0.8627 - val_loss: 0.3544 - val_accuracy: 0.9185\n",
            "Epoch 247/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.8620\n",
            "Epoch 00247: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2838 - accuracy: 0.8620 - val_loss: 0.3534 - val_accuracy: 0.9191\n",
            "Epoch 248/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8656\n",
            "Epoch 00248: saving model to /content\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.2759 - accuracy: 0.8656 - val_loss: 0.3525 - val_accuracy: 0.9192\n",
            "Epoch 249/250\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.8642\n",
            "Epoch 00249: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2746 - accuracy: 0.8642 - val_loss: 0.3520 - val_accuracy: 0.9191\n",
            "Epoch 250/250\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2817 - accuracy: 0.8619\n",
            "Epoch 00250: saving model to /content\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2817 - accuracy: 0.8619 - val_loss: 0.3528 - val_accuracy: 0.9192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQb5_s_8KGE_",
        "colab_type": "code",
        "outputId": "58501e36-f50e-472c-ee53-b0e0b83767a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history_red.history['accuracy'], label='train')\n",
        "plt.plot(history_red.history['val_accuracy'], label='test')\n",
        "\n",
        "max_idx=np.argmax(history_red.history['val_accuracy'])\n",
        "plt.plot(max_idx, history_red.history['val_accuracy'][max_idx], 'ks')\n",
        "plt.annotate('  epoch:'+str(max_idx)+'\\n'+'  acc:'+str(history_red.history['val_accuracy'][max_idx]),\n",
        "             xytext=(max_idx,history_red.history['val_accuracy'][max_idx]),\n",
        "             xy=(max_idx,history_red.history['val_accuracy'][max_idx]))\n",
        "\n",
        "plt.title('Accuracy with 50000 training samples')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history_red.history['loss'], label='train')\n",
        "plt.plot(history_red.history['val_loss'], label='test')\n",
        "plt.title('Loss with 50000 training samples')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Multiclass Cross Entropy')\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f02ec22dd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VythWlDQTFpo",
        "colab_type": "code",
        "outputId": "693163e8-f6f6-4275-d520-982320e138b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "red_model.evaluate(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0139 - accuracy: 0.9959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.013866527006030083, 0.9958999752998352]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Yoc6AdTWSD",
        "colab_type": "code",
        "outputId": "40d41b8f-137f-45c6-d3c2-e014fff3008a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "red_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 8ms/step - loss: 0.3528 - accuracy: 0.9192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.35276347398757935, 0.9192000031471252]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyc0XjlpsLIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}